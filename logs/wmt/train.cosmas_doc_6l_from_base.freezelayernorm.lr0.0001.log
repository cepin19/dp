[2019-04-22 11:30:42] [marian] Marian v1.7.8 b6b55a5 2019-04-19 12:38:45 +0200
[2019-04-22 11:30:42] [marian] Running on cosmas.lingea.cz as process 75321 with command line:
[2019-04-22 11:30:42] [marian] /home/large/data/models/marian/marian-doc/marian-doc-laynorm-cosmas/doc-marian/build/marian --model model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 5 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9200 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 1 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
[2019-04-22 11:30:42] [config] after-batches: 0
[2019-04-22 11:30:42] [config] after-epochs: 5
[2019-04-22 11:30:42] [config] allow-unk: false
[2019-04-22 11:30:42] [config] beam-size: 6
[2019-04-22 11:30:42] [config] bert-class-symbol: "[CLS]"
[2019-04-22 11:30:42] [config] bert-mask-symbol: "[MASK]"
[2019-04-22 11:30:42] [config] bert-masking-fraction: 0.15
[2019-04-22 11:30:42] [config] bert-sep-symbol: "[SEP]"
[2019-04-22 11:30:42] [config] bert-train-type-embeddings: true
[2019-04-22 11:30:42] [config] bert-type-vocab-size: 2
[2019-04-22 11:30:42] [config] best-deep: false
[2019-04-22 11:30:42] [config] clip-gemm: 0
[2019-04-22 11:30:42] [config] clip-norm: 5
[2019-04-22 11:30:42] [config] context-enc-depth: 1
[2019-04-22 11:30:42] [config] cost-type: ce-mean-words
[2019-04-22 11:30:42] [config] cpu-threads: 0
[2019-04-22 11:30:42] [config] data-weighting: ""
[2019-04-22 11:30:42] [config] data-weighting-type: sentence
[2019-04-22 11:30:42] [config] dec-cell: gru
[2019-04-22 11:30:42] [config] dec-cell-base-depth: 2
[2019-04-22 11:30:42] [config] dec-cell-high-depth: 1
[2019-04-22 11:30:42] [config] dec-depth: 6
[2019-04-22 11:30:42] [config] devices:
[2019-04-22 11:30:42] [config]   - 2
[2019-04-22 11:30:42] [config]   - 3
[2019-04-22 11:30:42] [config] dim-emb: 512
[2019-04-22 11:30:42] [config] dim-rnn: 1024
[2019-04-22 11:30:42] [config] dim-vocabs:
[2019-04-22 11:30:42] [config]   - 0
[2019-04-22 11:30:42] [config]   - 0
[2019-04-22 11:30:42] [config] disp-first: 0
[2019-04-22 11:30:42] [config] disp-freq: 100
[2019-04-22 11:30:42] [config] disp-label-counts: false
[2019-04-22 11:30:42] [config] dropout-rnn: 0
[2019-04-22 11:30:42] [config] dropout-src: 0
[2019-04-22 11:30:42] [config] dropout-trg: 0
[2019-04-22 11:30:42] [config] dump-config: ""
[2019-04-22 11:30:42] [config] early-stopping: 15
[2019-04-22 11:30:42] [config] embedding-fix-src: true
[2019-04-22 11:30:42] [config] embedding-fix-trg: true
[2019-04-22 11:30:42] [config] embedding-normalization: false
[2019-04-22 11:30:42] [config] embedding-vectors:
[2019-04-22 11:30:42] [config]   []
[2019-04-22 11:30:42] [config] enc-cell: gru
[2019-04-22 11:30:42] [config] enc-cell-depth: 1
[2019-04-22 11:30:42] [config] enc-depth: 6
[2019-04-22 11:30:42] [config] enc-type: bidirectional
[2019-04-22 11:30:42] [config] exponential-smoothing: 0.0001
[2019-04-22 11:30:42] [config] freeze: true
[2019-04-22 11:30:42] [config] grad-dropping-momentum: 0
[2019-04-22 11:30:42] [config] grad-dropping-rate: 0
[2019-04-22 11:30:42] [config] grad-dropping-warmup: 100
[2019-04-22 11:30:42] [config] guided-alignment: none
[2019-04-22 11:30:42] [config] guided-alignment-cost: mse
[2019-04-22 11:30:42] [config] guided-alignment-weight: 0.1
[2019-04-22 11:30:42] [config] hier-att: false
[2019-04-22 11:30:42] [config] ignore-model-config: false
[2019-04-22 11:30:42] [config] input-types:
[2019-04-22 11:30:42] [config]   []
[2019-04-22 11:30:42] [config] interpolate-env-vars: false
[2019-04-22 11:30:42] [config] keep-best: true
[2019-04-22 11:30:42] [config] label-smoothing: 0.1
[2019-04-22 11:30:42] [config] layer-normalization: false
[2019-04-22 11:30:42] [config] learn-rate: 0.0001
[2019-04-22 11:30:42] [config] log: model/bt_encz.log
[2019-04-22 11:30:42] [config] log-level: info
[2019-04-22 11:30:42] [config] log-time-zone: ""
[2019-04-22 11:30:42] [config] lr-decay: 0
[2019-04-22 11:30:42] [config] lr-decay-freq: 50000
[2019-04-22 11:30:42] [config] lr-decay-inv-sqrt:
[2019-04-22 11:30:42] [config]   - 16000
[2019-04-22 11:30:42] [config] lr-decay-repeat-warmup: false
[2019-04-22 11:30:42] [config] lr-decay-reset-optimizer: false
[2019-04-22 11:30:42] [config] lr-decay-start:
[2019-04-22 11:30:42] [config]   - 10
[2019-04-22 11:30:42] [config]   - 1
[2019-04-22 11:30:42] [config] lr-decay-strategy: epoch+stalled
[2019-04-22 11:30:42] [config] lr-report: true
[2019-04-22 11:30:42] [config] lr-warmup: 16000
[2019-04-22 11:30:42] [config] lr-warmup-at-reload: false
[2019-04-22 11:30:42] [config] lr-warmup-cycle: false
[2019-04-22 11:30:42] [config] lr-warmup-start-rate: 0
[2019-04-22 11:30:42] [config] max-length: 95
[2019-04-22 11:30:42] [config] max-length-crop: false
[2019-04-22 11:30:42] [config] max-length-factor: 3
[2019-04-22 11:30:42] [config] maxi-batch: 1000
[2019-04-22 11:30:42] [config] maxi-batch-sort: trg
[2019-04-22 11:30:42] [config] mini-batch: 1000
[2019-04-22 11:30:42] [config] mini-batch-fit: true
[2019-04-22 11:30:42] [config] mini-batch-fit-step: 10
[2019-04-22 11:30:42] [config] mini-batch-overstuff: 1
[2019-04-22 11:30:42] [config] mini-batch-track-lr: false
[2019-04-22 11:30:42] [config] mini-batch-understuff: 1
[2019-04-22 11:30:42] [config] mini-batch-warmup: 0
[2019-04-22 11:30:42] [config] mini-batch-words: 0
[2019-04-22 11:30:42] [config] mini-batch-words-ref: 0
[2019-04-22 11:30:42] [config] model: model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz
[2019-04-22 11:30:42] [config] multi-loss-type: sum
[2019-04-22 11:30:42] [config] multi-node: false
[2019-04-22 11:30:42] [config] multi-node-overlap: true
[2019-04-22 11:30:42] [config] n-best: false
[2019-04-22 11:30:42] [config] no-nccl: false
[2019-04-22 11:30:42] [config] no-reload: false
[2019-04-22 11:30:42] [config] no-restore-corpus: false
[2019-04-22 11:30:42] [config] no-shuffle: false
[2019-04-22 11:30:42] [config] normalize: 0.6
[2019-04-22 11:30:42] [config] num-devices: 0
[2019-04-22 11:30:42] [config] optimizer: adam
[2019-04-22 11:30:42] [config] optimizer-delay: 4
[2019-04-22 11:30:42] [config] optimizer-params:
[2019-04-22 11:30:42] [config]   - 0.9
[2019-04-22 11:30:42] [config]   - 0.98
[2019-04-22 11:30:42] [config]   - 1e-09
[2019-04-22 11:30:42] [config] overwrite: true
[2019-04-22 11:30:42] [config] pretrained-model: model/model_base_encz2.npz
[2019-04-22 11:30:42] [config] quiet: false
[2019-04-22 11:30:42] [config] quiet-translation: true
[2019-04-22 11:30:42] [config] relative-paths: false
[2019-04-22 11:30:42] [config] right-left: false
[2019-04-22 11:30:42] [config] save-freq: 2000
[2019-04-22 11:30:42] [config] seed: 1111
[2019-04-22 11:30:42] [config] shuffle-in-ram: false
[2019-04-22 11:30:42] [config] skip: false
[2019-04-22 11:30:42] [config] sqlite: ""
[2019-04-22 11:30:42] [config] sqlite-drop: false
[2019-04-22 11:30:42] [config] sync-sgd: true
[2019-04-22 11:30:42] [config] tempdir: /tmp
[2019-04-22 11:30:42] [config] tied-embeddings: false
[2019-04-22 11:30:42] [config] tied-embeddings-all: true
[2019-04-22 11:30:42] [config] tied-embeddings-src: false
[2019-04-22 11:30:42] [config] train-sets:
[2019-04-22 11:30:42] [config]   - corpus.docs.en.bpe.src_prev
[2019-04-22 11:30:42] [config]   - corpus.docs.en.bpe.src
[2019-04-22 11:30:42] [config]   - corpus.docs.cs.bpe
[2019-04-22 11:30:42] [config] transformer-aan-activation: swish
[2019-04-22 11:30:42] [config] transformer-aan-depth: 2
[2019-04-22 11:30:42] [config] transformer-aan-nogate: false
[2019-04-22 11:30:42] [config] transformer-decoder-autoreg: self-attention
[2019-04-22 11:30:42] [config] transformer-dim-aan: 2048
[2019-04-22 11:30:42] [config] transformer-dim-ffn: 2048
[2019-04-22 11:30:42] [config] transformer-dropout: 0.1
[2019-04-22 11:30:42] [config] transformer-dropout-attention: 0
[2019-04-22 11:30:42] [config] transformer-dropout-ffn: 0
[2019-04-22 11:30:42] [config] transformer-ffn-activation: swish
[2019-04-22 11:30:42] [config] transformer-ffn-depth: 2
[2019-04-22 11:30:42] [config] transformer-guided-alignment-layer: last
[2019-04-22 11:30:42] [config] transformer-heads: 8
[2019-04-22 11:30:42] [config] transformer-no-projection: false
[2019-04-22 11:30:42] [config] transformer-postprocess: dan
[2019-04-22 11:30:42] [config] transformer-postprocess-emb: d
[2019-04-22 11:30:42] [config] transformer-preprocess: ""
[2019-04-22 11:30:42] [config] transformer-tied-layers:
[2019-04-22 11:30:42] [config]   []
[2019-04-22 11:30:42] [config] transformer-train-position-embeddings: false
[2019-04-22 11:30:42] [config] type: transformer-context
[2019-04-22 11:30:42] [config] ulr: false
[2019-04-22 11:30:42] [config] ulr-dim-emb: 0
[2019-04-22 11:30:42] [config] ulr-dropout: 0
[2019-04-22 11:30:42] [config] ulr-keys-vectors: ""
[2019-04-22 11:30:42] [config] ulr-query-vectors: ""
[2019-04-22 11:30:42] [config] ulr-softmax-temperature: 1
[2019-04-22 11:30:42] [config] ulr-trainable-transformation: false
[2019-04-22 11:30:42] [config] valid-freq: 2000
[2019-04-22 11:30:42] [config] valid-log: model/valid.log
[2019-04-22 11:30:42] [config] valid-max-length: 1000
[2019-04-22 11:30:42] [config] valid-metrics:
[2019-04-22 11:30:42] [config]   - ce-mean-words
[2019-04-22 11:30:42] [config]   - perplexity
[2019-04-22 11:30:42] [config]   - translation
[2019-04-22 11:30:42] [config] valid-mini-batch: 16
[2019-04-22 11:30:42] [config] valid-script-path: ./val.sh
[2019-04-22 11:30:42] [config] valid-sets:
[2019-04-22 11:30:42] [config]   - newstest2016.docs.src_prev
[2019-04-22 11:30:42] [config]   - newstest2016.docs.src
[2019-04-22 11:30:42] [config]   - newstest2016.docs.cs.bpe
[2019-04-22 11:30:42] [config] valid-translation-output: ""
[2019-04-22 11:30:42] [config] vocabs:
[2019-04-22 11:30:42] [config]   - corp/vocab.encs.yml
[2019-04-22 11:30:42] [config]   - corp/vocab.encs.yml
[2019-04-22 11:30:42] [config]   - corp/vocab.encs.yml
[2019-04-22 11:30:42] [config] word-penalty: 0
[2019-04-22 11:30:42] [config] workspace: 9200
[2019-04-22 11:30:42] [config] Model is being created with Marian v1.7.8 b6b55a5 2019-04-19 12:38:45 +0200
[2019-04-22 11:30:42] Using synchronous training
[2019-04-22 11:30:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-22 11:30:42] [data] Setting vocabulary size for input 0 to 34028
[2019-04-22 11:30:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-22 11:30:42] [data] Setting vocabulary size for input 1 to 34028
[2019-04-22 11:30:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-22 11:30:42] [data] Setting vocabulary size for input 2 to 34028
[2019-04-22 11:30:42] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-04-22 11:30:42] [batching] Collecting statistics for batch fitting with step size 10
[2019-04-22 11:30:45] [memory] Extending reserved space to 9216 MB (device gpu2)
[2019-04-22 11:30:45] [memory] Extending reserved space to 9216 MB (device gpu3)
[2019-04-22 11:30:45] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-22 11:30:46] [comm] NCCLCommunicator constructed successfully.
[2019-04-22 11:30:46] [training] Using 2 GPUs
[2019-04-22 11:30:46] [memory] Reserving 319 MB, device gpu2
[2019-04-22 11:30:46] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-04-22 11:30:46] [memory] Reserving 319 MB, device gpu2
[2019-04-22 11:30:58] [batching] Done. Typical MB size is 34880 target words
[2019-04-22 11:30:58] [memory] Extending reserved space to 9216 MB (device gpu2)
[2019-04-22 11:30:58] [memory] Extending reserved space to 9216 MB (device gpu3)
[2019-04-22 11:30:59] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-22 11:30:59] [comm] NCCLCommunicator constructed successfully.
[2019-04-22 11:30:59] [training] Using 2 GPUs
[2019-04-22 11:30:59] [training] Initializing model weights with the pre-trained model model/model_base_encz2.npz
[2019-04-22 11:30:59] Loading model from model/model_base_encz2.npz
[2019-04-22 11:31:00] Loading model from model/model_base_encz2.npz
[2019-04-22 11:31:00] Training started
[2019-04-22 11:31:00] [data] Shuffling data
tcmalloc: large alloc 1073741824 bytes == 0x1a9334000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f83dc600000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f835c600000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f7e10000000 @ 
[2019-04-22 11:31:37] [data] Done reading 57951104 sentences
[2019-04-22 11:35:15] [data] Done shuffling 57951104 sentences to temp files
[2019-04-22 11:35:43] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu2
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu3
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu3
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu2
[2019-04-22 11:35:43] [memory] Reserving 159 MB, device gpu2
[2019-04-22 11:35:43] [memory] Reserving 159 MB, device gpu3
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu3
[2019-04-22 11:35:43] [memory] Reserving 319 MB, device gpu2
[2019-04-22 11:37:00] Ep. 1 : Up. 100 : Sen. 85,152 : Cost 10.36674213 : Time 377.68s : 3424.96 words/s : L.r. 6.2500e-07
[2019-04-22 11:38:12] Ep. 1 : Up. 200 : Sen. 162,428 : Cost 9.83301258 : Time 72.20s : 15298.76 words/s : L.r. 1.2500e-06
[2019-04-22 11:39:28] Ep. 1 : Up. 300 : Sen. 242,995 : Cost 9.27541065 : Time 75.89s : 15315.55 words/s : L.r. 1.8750e-06
[2019-04-22 11:40:41] Ep. 1 : Up. 400 : Sen. 302,307 : Cost 9.30133247 : Time 72.84s : 15342.32 words/s : L.r. 2.5000e-06
[2019-04-22 11:41:55] Ep. 1 : Up. 500 : Sen. 376,694 : Cost 8.73346424 : Time 74.42s : 15015.39 words/s : L.r. 3.1250e-06
[2019-04-22 11:43:06] Ep. 1 : Up. 600 : Sen. 447,300 : Cost 8.61500645 : Time 70.49s : 14502.07 words/s : L.r. 3.7500e-06
[2019-04-22 11:44:23] Ep. 1 : Up. 700 : Sen. 519,745 : Cost 8.55736160 : Time 76.82s : 15098.42 words/s : L.r. 4.3750e-06
[2019-04-22 11:45:37] Ep. 1 : Up. 800 : Sen. 587,064 : Cost 8.53394413 : Time 74.12s : 15131.08 words/s : L.r. 5.0000e-06
[2019-04-22 11:46:57] Ep. 1 : Up. 900 : Sen. 669,106 : Cost 8.44476509 : Time 80.19s : 15609.75 words/s : L.r. 5.6250e-06
[2019-04-22 11:48:12] Ep. 1 : Up. 1000 : Sen. 762,875 : Cost 8.35512352 : Time 74.67s : 15328.80 words/s : L.r. 6.2500e-06
[2019-04-22 11:49:30] Ep. 1 : Up. 1100 : Sen. 822,720 : Cost 8.47605610 : Time 77.79s : 15202.13 words/s : L.r. 6.8750e-06
[2019-04-22 11:50:46] Ep. 1 : Up. 1200 : Sen. 910,392 : Cost 8.07220840 : Time 76.29s : 15313.21 words/s : L.r. 7.5000e-06
[2019-04-22 11:51:57] Ep. 1 : Up. 1300 : Sen. 987,815 : Cost 8.07171822 : Time 71.66s : 14688.61 words/s : L.r. 8.1250e-06
[2019-04-22 11:53:15] Ep. 1 : Up. 1400 : Sen. 1,063,061 : Cost 7.92588091 : Time 77.47s : 15148.39 words/s : L.r. 8.7500e-06
[2019-04-22 11:54:33] Ep. 1 : Up. 1500 : Sen. 1,132,234 : Cost 7.84657621 : Time 77.98s : 15627.89 words/s : L.r. 9.3750e-06
[2019-04-22 11:55:47] Ep. 1 : Up. 1600 : Sen. 1,210,999 : Cost 7.43054199 : Time 74.34s : 15065.89 words/s : L.r. 1.0000e-05
[2019-04-22 11:56:57] Ep. 1 : Up. 1700 : Sen. 1,280,931 : Cost 6.96787119 : Time 69.78s : 14615.53 words/s : L.r. 1.0625e-05
[2019-04-22 11:58:10] Ep. 1 : Up. 1800 : Sen. 1,353,157 : Cost 6.40849018 : Time 73.26s : 14842.77 words/s : L.r. 1.1250e-05
[2019-04-22 11:59:24] Ep. 1 : Up. 1900 : Sen. 1,423,667 : Cost 5.85417032 : Time 73.75s : 15146.43 words/s : L.r. 1.1875e-05
[2019-04-22 12:00:45] Ep. 1 : Up. 2000 : Sen. 1,511,157 : Cost 5.38098907 : Time 80.96s : 15849.99 words/s : L.r. 1.2500e-05
[2019-04-22 12:00:45] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.orig.npz
[2019-04-22 12:00:47] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz
[2019-04-22 12:00:49] Saving Adam parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.optimizer.npz
[2019-04-22 12:00:57] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-ce-mean-words.npz
[2019-04-22 12:00:58] [valid] Ep. 1 : Up. 2000 : ce-mean-words : 5.3534 : new best
[2019-04-22 12:01:01] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-perplexity.npz
[2019-04-22 12:01:02] [valid] Ep. 1 : Up. 2000 : perplexity : 211.325 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-22 12:02:47] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-translation.npz
[2019-04-22 12:02:48] [valid] Ep. 1 : Up. 2000 : translation : 0.4 : new best
[2019-04-22 12:04:04] Ep. 1 : Up. 2100 : Sen. 1,588,698 : Cost 4.62029314 : Time 199.12s : 5674.30 words/s : L.r. 1.3125e-05
[2019-04-22 12:05:22] Ep. 1 : Up. 2200 : Sen. 1,666,805 : Cost 3.87331486 : Time 77.84s : 15215.53 words/s : L.r. 1.3750e-05
[2019-04-22 12:06:36] Ep. 1 : Up. 2300 : Sen. 1,744,253 : Cost 3.52793860 : Time 74.08s : 14944.89 words/s : L.r. 1.4375e-05
[2019-04-22 12:07:56] Ep. 1 : Up. 2400 : Sen. 1,823,703 : Cost 3.30523920 : Time 79.53s : 15177.67 words/s : L.r. 1.5000e-05
[2019-04-22 12:09:09] Ep. 1 : Up. 2500 : Sen. 1,888,249 : Cost 3.20045543 : Time 73.14s : 14861.56 words/s : L.r. 1.5625e-05
[2019-04-22 12:10:26] Ep. 1 : Up. 2600 : Sen. 1,971,873 : Cost 3.05201101 : Time 77.45s : 15289.66 words/s : L.r. 1.6250e-05
[2019-04-22 12:11:44] Ep. 1 : Up. 2700 : Sen. 2,052,508 : Cost 3.12522459 : Time 77.75s : 15492.27 words/s : L.r. 1.6875e-05
[2019-04-22 12:13:04] Ep. 1 : Up. 2800 : Sen. 2,127,706 : Cost 2.93587685 : Time 79.65s : 15180.13 words/s : L.r. 1.7500e-05
[2019-04-22 12:14:26] Ep. 1 : Up. 2900 : Sen. 2,216,040 : Cost 3.04398632 : Time 81.95s : 15769.79 words/s : L.r. 1.8125e-05
[2019-04-22 12:15:43] Ep. 1 : Up. 3000 : Sen. 2,296,403 : Cost 2.96501684 : Time 77.65s : 15438.75 words/s : L.r. 1.8750e-05
[2019-04-22 12:16:59] Ep. 1 : Up. 3100 : Sen. 2,362,648 : Cost 2.86347580 : Time 75.33s : 15106.88 words/s : L.r. 1.9375e-05
[2019-04-22 12:18:17] Ep. 1 : Up. 3200 : Sen. 2,438,076 : Cost 2.88512611 : Time 78.95s : 15207.93 words/s : L.r. 2.0000e-05
[2019-04-22 12:19:29] Ep. 1 : Up. 3300 : Sen. 2,492,745 : Cost 2.82781196 : Time 71.92s : 14512.26 words/s : L.r. 2.0625e-05
[2019-04-22 12:20:41] Ep. 1 : Up. 3400 : Sen. 2,556,307 : Cost 2.87484860 : Time 71.79s : 14644.70 words/s : L.r. 2.1250e-05
[2019-04-22 12:21:57] Ep. 1 : Up. 3500 : Sen. 2,629,411 : Cost 2.86587524 : Time 75.66s : 15121.37 words/s : L.r. 2.1875e-05
[2019-04-22 12:23:12] Ep. 1 : Up. 3600 : Sen. 2,717,854 : Cost 2.82269073 : Time 75.59s : 14968.49 words/s : L.r. 2.2500e-05
[2019-04-22 12:24:31] Ep. 1 : Up. 3700 : Sen. 2,800,159 : Cost 2.85015535 : Time 78.97s : 15466.67 words/s : L.r. 2.3125e-05
[2019-04-22 12:25:47] Ep. 1 : Up. 3800 : Sen. 2,886,880 : Cost 2.87306094 : Time 75.31s : 15077.93 words/s : L.r. 2.3750e-05
[2019-04-22 12:27:04] Ep. 1 : Up. 3900 : Sen. 2,971,933 : Cost 2.84887481 : Time 77.67s : 14953.32 words/s : L.r. 2.4375e-05
[2019-04-22 12:28:18] Ep. 1 : Up. 4000 : Sen. 3,048,407 : Cost 2.88077211 : Time 73.95s : 14976.76 words/s : L.r. 2.5000e-05
[2019-04-22 12:28:18] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.orig.npz
[2019-04-22 12:28:21] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz
[2019-04-22 12:28:24] Saving Adam parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.optimizer.npz
[2019-04-22 12:28:32] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-ce-mean-words.npz
[2019-04-22 12:28:34] [valid] Ep. 1 : Up. 4000 : ce-mean-words : 1.62501 : new best
[2019-04-22 12:28:37] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-perplexity.npz
[2019-04-22 12:28:39] [valid] Ep. 1 : Up. 4000 : perplexity : 5.07849 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-22 12:29:23] Saving model weights and runtime parameters to model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz.best-translation.npz
[2019-04-22 12:29:25] [valid] Ep. 1 : Up. 4000 : translation : 25.38 : new best
[2019-04-22 12:30:39] Ep. 1 : Up. 4100 : Sen. 3,119,512 : Cost 2.77108335 : Time 141.13s : 7830.33 words/s : L.r. 2.5625e-05
[2019-04-22 12:31:51] Ep. 1 : Up. 4200 : Sen. 3,197,709 : Cost 2.83049464 : Time 71.86s : 14781.74 words/s : L.r. 2.6250e-05
[2019-04-22 12:33:03] Ep. 1 : Up. 4300 : Sen. 3,266,802 : Cost 2.78763223 : Time 72.11s : 14806.61 words/s : L.r. 2.6875e-05
[2019-04-22 12:34:19] Ep. 1 : Up. 4400 : Sen. 3,334,324 : Cost 2.87583828 : Time 75.89s : 15066.26 words/s : L.r. 2.7500e-05
[2019-04-22 12:35:35] Ep. 1 : Up. 4500 : Sen. 3,429,160 : Cost 2.80665660 : Time 75.30s : 14861.78 words/s : L.r. 2.8125e-05
[2019-04-22 12:36:55] Ep. 1 : Up. 4600 : Sen. 3,511,311 : Cost 2.82916045 : Time 80.28s : 15601.86 words/s : L.r. 2.8750e-05
[2019-04-22 12:38:10] Ep. 1 : Up. 4700 : Sen. 3,571,011 : Cost 2.72487569 : Time 74.87s : 14928.40 words/s : L.r. 2.9375e-05
[2019-04-22 12:39:31] Ep. 1 : Up. 4800 : Sen. 3,643,716 : Cost 2.84092236 : Time 81.26s : 15347.41 words/s : L.r. 3.0000e-05
[2019-04-22 12:40:52] Ep. 1 : Up. 4900 : Sen. 3,720,306 : Cost 2.80346799 : Time 80.65s : 15560.10 words/s : L.r. 3.0625e-05
[2019-04-22 12:42:06] Ep. 1 : Up. 5000 : Sen. 3,794,519 : Cost 2.82537031 : Time 74.59s : 15020.82 words/s : L.r. 3.1250e-05
[2019-04-22 12:43:26] Ep. 1 : Up. 5100 : Sen. 3,886,349 : Cost 2.79104853 : Time 79.95s : 15327.35 words/s : L.r. 3.1875e-05
[2019-04-22 12:44:43] Ep. 1 : Up. 5200 : Sen. 3,968,605 : Cost 2.79933310 : Time 76.35s : 15009.40 words/s : L.r. 3.2500e-05
[2019-04-22 12:45:59] Ep. 1 : Up. 5300 : Sen. 4,037,263 : Cost 2.77073693 : Time 76.26s : 15146.68 words/s : L.r. 3.3125e-05
train.cosmas_doc_6l_from_base.freezelayernorm.sh: line 29: 75321 Terminated              $marian/marian --model model/model_base_cosmas_doc_from_base_layernorm2.lr0.0001.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 5 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9200 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 1 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
