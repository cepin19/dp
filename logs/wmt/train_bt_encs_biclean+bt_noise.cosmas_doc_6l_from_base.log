[2019-04-17 11:46:57] [marian] Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 11:46:57] [marian] Running on cosmas.lingea.cz as process 112060 with command line:
[2019-04-17 11:46:57] [marian] /home/large/data/models/marian/marian-doc/doc-marian-cosmas/build/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 7000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
[2019-04-17 11:46:57] [config] after-batches: 0
[2019-04-17 11:46:57] [config] after-epochs: 1
[2019-04-17 11:46:57] [config] allow-unk: false
[2019-04-17 11:46:57] [config] beam-size: 6
[2019-04-17 11:46:57] [config] bert-class-symbol: "[CLS]"
[2019-04-17 11:46:57] [config] bert-mask-symbol: "[MASK]"
[2019-04-17 11:46:57] [config] bert-masking-fraction: 0.15
[2019-04-17 11:46:57] [config] bert-sep-symbol: "[SEP]"
[2019-04-17 11:46:57] [config] bert-train-type-embeddings: true
[2019-04-17 11:46:57] [config] bert-type-vocab-size: 2
[2019-04-17 11:46:57] [config] best-deep: false
[2019-04-17 11:46:57] [config] clip-gemm: 0
[2019-04-17 11:46:57] [config] clip-norm: 5
[2019-04-17 11:46:57] [config] context-enc-depth: 6
[2019-04-17 11:46:57] [config] cost-type: ce-mean-words
[2019-04-17 11:46:57] [config] cpu-threads: 0
[2019-04-17 11:46:57] [config] data-weighting: ""
[2019-04-17 11:46:57] [config] data-weighting-type: sentence
[2019-04-17 11:46:57] [config] dec-cell: gru
[2019-04-17 11:46:57] [config] dec-cell-base-depth: 2
[2019-04-17 11:46:57] [config] dec-cell-high-depth: 1
[2019-04-17 11:46:57] [config] dec-depth: 6
[2019-04-17 11:46:57] [config] devices:
[2019-04-17 11:46:57] [config]   - 2
[2019-04-17 11:46:57] [config]   - 3
[2019-04-17 11:46:57] [config] dim-emb: 512
[2019-04-17 11:46:57] [config] dim-rnn: 1024
[2019-04-17 11:46:57] [config] dim-vocabs:
[2019-04-17 11:46:57] [config]   - 34028
[2019-04-17 11:46:57] [config]   - 34028
[2019-04-17 11:46:57] [config]   - 34028
[2019-04-17 11:46:57] [config] disp-first: 0
[2019-04-17 11:46:57] [config] disp-freq: 100
[2019-04-17 11:46:57] [config] disp-label-counts: false
[2019-04-17 11:46:57] [config] dropout-rnn: 0
[2019-04-17 11:46:57] [config] dropout-src: 0
[2019-04-17 11:46:57] [config] dropout-trg: 0
[2019-04-17 11:46:57] [config] dump-config: ""
[2019-04-17 11:46:57] [config] early-stopping: 15
[2019-04-17 11:46:57] [config] embedding-fix-src: true
[2019-04-17 11:46:57] [config] embedding-fix-trg: true
[2019-04-17 11:46:57] [config] embedding-normalization: false
[2019-04-17 11:46:57] [config] embedding-vectors:
[2019-04-17 11:46:57] [config]   []
[2019-04-17 11:46:57] [config] enc-cell: gru
[2019-04-17 11:46:57] [config] enc-cell-depth: 1
[2019-04-17 11:46:57] [config] enc-depth: 6
[2019-04-17 11:46:57] [config] enc-type: bidirectional
[2019-04-17 11:46:57] [config] exponential-smoothing: 0.0001
[2019-04-17 11:46:57] [config] freeze: true
[2019-04-17 11:46:57] [config] grad-dropping-momentum: 0
[2019-04-17 11:46:57] [config] grad-dropping-rate: 0
[2019-04-17 11:46:57] [config] grad-dropping-warmup: 100
[2019-04-17 11:46:57] [config] guided-alignment: none
[2019-04-17 11:46:57] [config] guided-alignment-cost: mse
[2019-04-17 11:46:57] [config] guided-alignment-weight: 0.1
[2019-04-17 11:46:57] [config] hier-att: false
[2019-04-17 11:46:57] [config] ignore-model-config: false
[2019-04-17 11:46:57] [config] input-types:
[2019-04-17 11:46:57] [config]   []
[2019-04-17 11:46:57] [config] interpolate-env-vars: false
[2019-04-17 11:46:57] [config] keep-best: true
[2019-04-17 11:46:57] [config] label-smoothing: 0.1
[2019-04-17 11:46:57] [config] layer-normalization: false
[2019-04-17 11:46:57] [config] learn-rate: 0.0003
[2019-04-17 11:46:57] [config] log: model/bt_encz.log
[2019-04-17 11:46:57] [config] log-level: info
[2019-04-17 11:46:57] [config] log-time-zone: ""
[2019-04-17 11:46:57] [config] lr-decay: 0
[2019-04-17 11:46:57] [config] lr-decay-freq: 50000
[2019-04-17 11:46:57] [config] lr-decay-inv-sqrt:
[2019-04-17 11:46:57] [config]   - 16000
[2019-04-17 11:46:57] [config] lr-decay-repeat-warmup: false
[2019-04-17 11:46:57] [config] lr-decay-reset-optimizer: false
[2019-04-17 11:46:57] [config] lr-decay-start:
[2019-04-17 11:46:57] [config]   - 10
[2019-04-17 11:46:57] [config]   - 1
[2019-04-17 11:46:57] [config] lr-decay-strategy: epoch+stalled
[2019-04-17 11:46:57] [config] lr-report: true
[2019-04-17 11:46:57] [config] lr-warmup: 16000
[2019-04-17 11:46:57] [config] lr-warmup-at-reload: false
[2019-04-17 11:46:57] [config] lr-warmup-cycle: false
[2019-04-17 11:46:57] [config] lr-warmup-start-rate: 0
[2019-04-17 11:46:57] [config] max-length: 95
[2019-04-17 11:46:57] [config] max-length-crop: false
[2019-04-17 11:46:57] [config] max-length-factor: 3
[2019-04-17 11:46:57] [config] maxi-batch: 1000
[2019-04-17 11:46:57] [config] maxi-batch-sort: trg
[2019-04-17 11:46:57] [config] mini-batch: 1000
[2019-04-17 11:46:57] [config] mini-batch-fit: true
[2019-04-17 11:46:57] [config] mini-batch-fit-step: 10
[2019-04-17 11:46:57] [config] mini-batch-overstuff: 1
[2019-04-17 11:46:57] [config] mini-batch-track-lr: false
[2019-04-17 11:46:57] [config] mini-batch-understuff: 1
[2019-04-17 11:46:57] [config] mini-batch-warmup: 0
[2019-04-17 11:46:57] [config] mini-batch-words: 0
[2019-04-17 11:46:57] [config] mini-batch-words-ref: 0
[2019-04-17 11:46:57] [config] model: model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 11:46:57] [config] multi-loss-type: sum
[2019-04-17 11:46:57] [config] multi-node: false
[2019-04-17 11:46:57] [config] multi-node-overlap: true
[2019-04-17 11:46:57] [config] n-best: false
[2019-04-17 11:46:57] [config] no-nccl: false
[2019-04-17 11:46:57] [config] no-reload: false
[2019-04-17 11:46:57] [config] no-restore-corpus: false
[2019-04-17 11:46:57] [config] no-shuffle: false
[2019-04-17 11:46:57] [config] normalize: 0.6
[2019-04-17 11:46:57] [config] num-devices: 0
[2019-04-17 11:46:57] [config] optimizer: adam
[2019-04-17 11:46:57] [config] optimizer-delay: 4
[2019-04-17 11:46:57] [config] optimizer-params:
[2019-04-17 11:46:57] [config]   - 0.9
[2019-04-17 11:46:57] [config]   - 0.98
[2019-04-17 11:46:57] [config]   - 1e-09
[2019-04-17 11:46:57] [config] overwrite: true
[2019-04-17 11:46:57] [config] pretrained-model: model/model_base_encz2.npz
[2019-04-17 11:46:57] [config] quiet: false
[2019-04-17 11:46:57] [config] quiet-translation: true
[2019-04-17 11:46:57] [config] relative-paths: false
[2019-04-17 11:46:57] [config] right-left: false
[2019-04-17 11:46:57] [config] save-freq: 2000
[2019-04-17 11:46:57] [config] seed: 1111
[2019-04-17 11:46:57] [config] shuffle-in-ram: false
[2019-04-17 11:46:57] [config] skip: false
[2019-04-17 11:46:57] [config] sqlite: ""
[2019-04-17 11:46:57] [config] sqlite-drop: false
[2019-04-17 11:46:57] [config] sync-sgd: true
[2019-04-17 11:46:57] [config] tempdir: /tmp
[2019-04-17 11:46:57] [config] tied-embeddings: false
[2019-04-17 11:46:57] [config] tied-embeddings-all: true
[2019-04-17 11:46:57] [config] tied-embeddings-src: false
[2019-04-17 11:46:57] [config] train-sets:
[2019-04-17 11:46:57] [config]   - corpus.docs.en.bpe.src_prev
[2019-04-17 11:46:57] [config]   - corpus.docs.en.bpe.src
[2019-04-17 11:46:57] [config]   - corpus.docs.cs.bpe
[2019-04-17 11:46:57] [config] transformer-aan-activation: swish
[2019-04-17 11:46:57] [config] transformer-aan-depth: 2
[2019-04-17 11:46:57] [config] transformer-aan-nogate: false
[2019-04-17 11:46:57] [config] transformer-decoder-autoreg: self-attention
[2019-04-17 11:46:57] [config] transformer-dim-aan: 2048
[2019-04-17 11:46:57] [config] transformer-dim-ffn: 2048
[2019-04-17 11:46:57] [config] transformer-dropout: 0.1
[2019-04-17 11:46:57] [config] transformer-dropout-attention: 0
[2019-04-17 11:46:57] [config] transformer-dropout-ffn: 0
[2019-04-17 11:46:57] [config] transformer-ffn-activation: swish
[2019-04-17 11:46:57] [config] transformer-ffn-depth: 2
[2019-04-17 11:46:57] [config] transformer-guided-alignment-layer: last
[2019-04-17 11:46:57] [config] transformer-heads: 8
[2019-04-17 11:46:57] [config] transformer-no-projection: false
[2019-04-17 11:46:57] [config] transformer-postprocess: dan
[2019-04-17 11:46:57] [config] transformer-postprocess-emb: d
[2019-04-17 11:46:57] [config] transformer-preprocess: ""
[2019-04-17 11:46:57] [config] transformer-tied-layers:
[2019-04-17 11:46:57] [config]   []
[2019-04-17 11:46:57] [config] transformer-train-position-embeddings: false
[2019-04-17 11:46:57] [config] type: transformer-context
[2019-04-17 11:46:57] [config] ulr: false
[2019-04-17 11:46:57] [config] ulr-dim-emb: 0
[2019-04-17 11:46:57] [config] ulr-dropout: 0
[2019-04-17 11:46:57] [config] ulr-keys-vectors: ""
[2019-04-17 11:46:57] [config] ulr-query-vectors: ""
[2019-04-17 11:46:57] [config] ulr-softmax-temperature: 1
[2019-04-17 11:46:57] [config] ulr-trainable-transformation: false
[2019-04-17 11:46:57] [config] valid-freq: 2000
[2019-04-17 11:46:57] [config] valid-log: model/valid.log
[2019-04-17 11:46:57] [config] valid-max-length: 1000
[2019-04-17 11:46:57] [config] valid-metrics:
[2019-04-17 11:46:57] [config]   - ce-mean-words
[2019-04-17 11:46:57] [config]   - perplexity
[2019-04-17 11:46:57] [config]   - translation
[2019-04-17 11:46:57] [config] valid-mini-batch: 16
[2019-04-17 11:46:57] [config] valid-script-path: ./val.sh
[2019-04-17 11:46:57] [config] valid-sets:
[2019-04-17 11:46:57] [config]   - newstest2016.docs.src_prev
[2019-04-17 11:46:57] [config]   - newstest2016.docs.src
[2019-04-17 11:46:57] [config]   - newstest2016.docs.cs.bpe
[2019-04-17 11:46:57] [config] valid-translation-output: ""
[2019-04-17 11:46:57] [config] version: v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 11:46:57] [config] vocabs:
[2019-04-17 11:46:57] [config]   - corp/vocab.encs.yml
[2019-04-17 11:46:57] [config]   - corp/vocab.encs.yml
[2019-04-17 11:46:57] [config]   - corp/vocab.encs.yml
[2019-04-17 11:46:57] [config] word-penalty: 0
[2019-04-17 11:46:57] [config] workspace: 7000
[2019-04-17 11:46:57] [config] Loaded model has been created with Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 11:46:57] Using synchronous training
[2019-04-17 11:46:57] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 11:46:57] [data] Setting vocabulary size for input 0 to 34028
[2019-04-17 11:46:57] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 11:46:57] [data] Setting vocabulary size for input 1 to 34028
[2019-04-17 11:46:57] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 11:46:58] [data] Setting vocabulary size for input 2 to 34028
[2019-04-17 11:46:58] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-04-17 11:46:58] [batching] Collecting statistics for batch fitting with step size 10
[2019-04-17 11:46:59] [memory] Extending reserved space to 7040 MB (device gpu2)
[2019-04-17 11:47:00] [memory] Extending reserved space to 7040 MB (device gpu3)
[2019-04-17 11:47:00] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-17 11:47:00] [comm] NCCLCommunicator constructed successfully.
[2019-04-17 11:47:00] [training] Using 2 GPUs
[2019-04-17 11:47:00] [memory] Reserving 379 MB, device gpu2
[2019-04-17 11:47:00] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-04-17 11:47:00] [memory] Reserving 379 MB, device gpu2
[2019-04-17 11:47:09] [batching] Done. Typical MB size is 22104 target words
[2019-04-17 11:47:09] [memory] Extending reserved space to 7040 MB (device gpu2)
[2019-04-17 11:47:09] [memory] Extending reserved space to 7040 MB (device gpu3)
[2019-04-17 11:47:09] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-17 11:47:09] [comm] NCCLCommunicator constructed successfully.
[2019-04-17 11:47:09] [training] Using 2 GPUs
[2019-04-17 11:47:09] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 11:47:10] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 11:47:11] Loading Adam parameters from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.optimizer.npz
[2019-04-17 11:47:14] [memory] Reserving 379 MB, device gpu2
[2019-04-17 11:47:14] [memory] Reserving 379 MB, device gpu3
[2019-04-17 11:47:14] [training] Model reloaded from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 11:47:14] [data] Restoring the corpus state to epoch 1, batch 24000
[2019-04-17 11:47:14] [data] Shuffling data
tcmalloc: large alloc 1073741824 bytes == 0x1ba47a000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7fcee8500000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7fce58000000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7fcdd7800000 @ 
[2019-04-17 11:47:34] [data] Done reading 57951104 sentences
[2019-04-17 11:50:58] [data] Done shuffling 57951104 sentences to temp files
[2019-04-17 11:55:21] Training started
[2019-04-17 11:55:21] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-04-17 11:55:21] [memory] Reserving 379 MB, device gpu2
[2019-04-17 11:55:21] [memory] Reserving 379 MB, device gpu3
[2019-04-17 11:55:22] [memory] Reserving 379 MB, device gpu3
[2019-04-17 11:55:22] [memory] Reserving 379 MB, device gpu2
[2019-04-17 11:55:22] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 11:55:23] [memory] Reserving 379 MB, device cpu0
[2019-04-17 11:55:23] [memory] Reserving 189 MB, device gpu2
[2019-04-17 11:55:23] [memory] Reserving 189 MB, device gpu3
[2019-04-17 11:56:25] Ep. 1 : Up. 24100 : Sen. 12,876,350 : Cost 2.66635990 : Time 567.77s : 1391.97 words/s : L.r. 2.4444e-04
[2019-04-17 11:57:33] Ep. 1 : Up. 24200 : Sen. 12,936,822 : Cost 2.67478871 : Time 67.84s : 12048.14 words/s : L.r. 2.4393e-04
[2019-04-17 11:58:42] Ep. 1 : Up. 24300 : Sen. 12,991,362 : Cost 2.69291592 : Time 68.61s : 11875.51 words/s : L.r. 2.4343e-04
[2019-04-17 11:59:51] Ep. 1 : Up. 24400 : Sen. 13,044,708 : Cost 2.67222476 : Time 68.72s : 11893.73 words/s : L.r. 2.4293e-04
[2019-04-17 12:00:57] Ep. 1 : Up. 24500 : Sen. 13,091,622 : Cost 2.64788890 : Time 66.81s : 11767.13 words/s : L.r. 2.4244e-04
[2019-04-17 12:02:08] Ep. 1 : Up. 24600 : Sen. 13,149,366 : Cost 2.60609531 : Time 70.32s : 11982.42 words/s : L.r. 2.4194e-04
[2019-04-17 12:03:10] Ep. 1 : Up. 24700 : Sen. 13,190,671 : Cost 2.71295857 : Time 62.08s : 11117.58 words/s : L.r. 2.4145e-04
[2019-04-17 12:04:18] Ep. 1 : Up. 24800 : Sen. 13,245,673 : Cost 2.62149930 : Time 67.84s : 11640.78 words/s : L.r. 2.4097e-04
[2019-04-17 12:05:25] Ep. 1 : Up. 24900 : Sen. 13,297,004 : Cost 2.60869217 : Time 67.06s : 11974.36 words/s : L.r. 2.4048e-04
[2019-04-17 12:06:32] Ep. 1 : Up. 25000 : Sen. 13,343,320 : Cost 2.67833638 : Time 67.75s : 12258.06 words/s : L.r. 2.4000e-04
[2019-04-17 12:07:42] Ep. 1 : Up. 25100 : Sen. 13,400,337 : Cost 2.68169022 : Time 69.74s : 12569.38 words/s : L.r. 2.3952e-04
[2019-04-17 12:08:47] Ep. 1 : Up. 25200 : Sen. 13,445,488 : Cost 2.69641757 : Time 64.41s : 11293.91 words/s : L.r. 2.3905e-04
[2019-04-17 12:09:55] Ep. 1 : Up. 25300 : Sen. 13,496,143 : Cost 2.65010214 : Time 68.18s : 11717.01 words/s : L.r. 2.3857e-04
[2019-04-17 12:11:02] Ep. 1 : Up. 25400 : Sen. 13,551,817 : Cost 2.63644481 : Time 67.33s : 11661.02 words/s : L.r. 2.3810e-04
[2019-04-17 12:12:14] Ep. 1 : Up. 25500 : Sen. 13,615,567 : Cost 2.65847635 : Time 71.84s : 11906.26 words/s : L.r. 2.3764e-04
[2019-04-17 12:13:24] Ep. 1 : Up. 25600 : Sen. 13,678,115 : Cost 2.65243220 : Time 69.77s : 12279.70 words/s : L.r. 2.3717e-04
[2019-04-17 12:14:34] Ep. 1 : Up. 25700 : Sen. 13,738,998 : Cost 2.69265413 : Time 70.00s : 11915.62 words/s : L.r. 2.3671e-04
[2019-04-17 12:15:43] Ep. 1 : Up. 25800 : Sen. 13,793,576 : Cost 2.75849056 : Time 69.85s : 11682.51 words/s : L.r. 2.3625e-04
[2019-04-17 12:16:54] Ep. 1 : Up. 25900 : Sen. 13,849,939 : Cost 2.71305823 : Time 71.01s : 12279.78 words/s : L.r. 2.3579e-04
[2019-04-17 12:17:58] Ep. 1 : Up. 26000 : Sen. 13,898,258 : Cost 2.70649266 : Time 63.69s : 11435.18 words/s : L.r. 2.3534e-04
[2019-04-17 12:17:58] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 12:18:02] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 12:18:05] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.optimizer.npz
[2019-04-17 12:18:15] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-ce-mean-words.npz
[2019-04-17 12:18:18] [valid] Ep. 1 : Up. 26000 : ce-mean-words : 1.54231 : new best
[2019-04-17 12:18:21] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-perplexity.npz
[2019-04-17 12:18:23] [valid] Ep. 1 : Up. 26000 : perplexity : 4.6754 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 12:19:07] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-translation.npz
[2019-04-17 12:19:10] [valid] Ep. 1 : Up. 26000 : translation : 26.31 : new best
[2019-04-17 12:20:21] Ep. 1 : Up. 26100 : Sen. 13,952,616 : Cost 2.65598822 : Time 143.29s : 6249.81 words/s : L.r. 2.3489e-04
[2019-04-17 12:21:31] Ep. 1 : Up. 26200 : Sen. 14,008,928 : Cost 2.68001032 : Time 69.50s : 11900.71 words/s : L.r. 2.3444e-04
[2019-04-17 12:22:36] Ep. 1 : Up. 26300 : Sen. 14,064,049 : Cost 2.68999696 : Time 64.83s : 11686.03 words/s : L.r. 2.3399e-04
[2019-04-17 12:23:43] Ep. 1 : Up. 26400 : Sen. 14,107,085 : Cost 2.67431474 : Time 67.09s : 11833.68 words/s : L.r. 2.3355e-04
[2019-04-17 12:24:53] Ep. 1 : Up. 26500 : Sen. 14,164,224 : Cost 2.69689417 : Time 70.38s : 12216.34 words/s : L.r. 2.3311e-04
[2019-04-17 12:26:03] Ep. 1 : Up. 26600 : Sen. 14,220,223 : Cost 2.70116568 : Time 69.67s : 12157.04 words/s : L.r. 2.3267e-04
[2019-04-17 12:27:12] Ep. 1 : Up. 26700 : Sen. 14,269,569 : Cost 2.64734292 : Time 68.59s : 11534.75 words/s : L.r. 2.3223e-04
[2019-04-17 12:28:18] Ep. 1 : Up. 26800 : Sen. 14,311,766 : Cost 2.70296574 : Time 66.32s : 11937.67 words/s : L.r. 2.3180e-04
[2019-04-17 12:29:26] Ep. 1 : Up. 26900 : Sen. 14,361,293 : Cost 2.62726355 : Time 67.69s : 11908.93 words/s : L.r. 2.3137e-04
[2019-04-17 12:30:31] Ep. 1 : Up. 27000 : Sen. 14,407,928 : Cost 2.65746522 : Time 65.85s : 11492.69 words/s : L.r. 2.3094e-04
[2019-04-17 12:31:41] Ep. 1 : Up. 27100 : Sen. 14,460,609 : Cost 2.66547465 : Time 69.35s : 11743.27 words/s : L.r. 2.3051e-04
[2019-04-17 12:32:49] Ep. 1 : Up. 27200 : Sen. 14,509,048 : Cost 2.65884471 : Time 68.13s : 11877.93 words/s : L.r. 2.3009e-04
[2019-04-17 12:34:02] Ep. 1 : Up. 27300 : Sen. 14,575,422 : Cost 2.65337086 : Time 72.88s : 12178.42 words/s : L.r. 2.2967e-04
[2019-04-17 12:35:12] Ep. 1 : Up. 27400 : Sen. 14,638,747 : Cost 2.62449050 : Time 70.06s : 11957.30 words/s : L.r. 2.2925e-04
[2019-04-17 12:36:19] Ep. 1 : Up. 27500 : Sen. 14,694,100 : Cost 2.70115399 : Time 66.85s : 12348.42 words/s : L.r. 2.2883e-04
[2019-04-17 12:37:25] Ep. 1 : Up. 27600 : Sen. 14,742,646 : Cost 2.65315413 : Time 66.02s : 11660.83 words/s : L.r. 2.2842e-04
[2019-04-17 12:38:34] Ep. 1 : Up. 27700 : Sen. 14,807,179 : Cost 2.73604655 : Time 69.61s : 11629.78 words/s : L.r. 2.2800e-04
[2019-04-17 12:39:43] Ep. 1 : Up. 27800 : Sen. 14,863,756 : Cost 2.64144492 : Time 68.38s : 11786.85 words/s : L.r. 2.2759e-04
[2019-04-17 12:40:53] Ep. 1 : Up. 27900 : Sen. 14,914,953 : Cost 2.69801879 : Time 70.47s : 11652.85 words/s : L.r. 2.2718e-04
[2019-04-17 12:42:06] Ep. 1 : Up. 28000 : Sen. 14,970,281 : Cost 2.59593463 : Time 72.37s : 11091.88 words/s : L.r. 2.2678e-04
[2019-04-17 12:42:06] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 12:42:09] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 12:42:13] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.optimizer.npz
[2019-04-17 12:42:25] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-ce-mean-words.npz
[2019-04-17 12:42:27] [valid] Ep. 1 : Up. 28000 : ce-mean-words : 1.54171 : new best
[2019-04-17 12:42:31] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-perplexity.npz
[2019-04-17 12:42:34] [valid] Ep. 1 : Up. 28000 : perplexity : 4.67259 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 12:43:25] [valid] Ep. 1 : Up. 28000 : translation : 26.21 : stalled 1 times (last best: 26.31)
[2019-04-17 12:44:41] Ep. 1 : Up. 28100 : Sen. 15,033,835 : Cost 2.63855338 : Time 155.35s : 5325.47 words/s : L.r. 2.2637e-04
[2019-04-17 12:45:56] Ep. 1 : Up. 28200 : Sen. 15,093,137 : Cost 2.64627838 : Time 74.59s : 11435.38 words/s : L.r. 2.2597e-04
[2019-04-17 12:47:08] Ep. 1 : Up. 28300 : Sen. 15,140,291 : Cost 2.62738991 : Time 72.39s : 10963.13 words/s : L.r. 2.2557e-04
[2019-04-17 12:48:18] Ep. 1 : Up. 28400 : Sen. 15,192,268 : Cost 2.65429831 : Time 70.07s : 10541.76 words/s : L.r. 2.2518e-04
[2019-04-17 12:49:29] Ep. 1 : Up. 28500 : Sen. 15,239,303 : Cost 2.69884801 : Time 71.42s : 11239.79 words/s : L.r. 2.2478e-04
[2019-04-17 12:50:40] Ep. 1 : Up. 28600 : Sen. 15,292,935 : Cost 2.63919997 : Time 70.73s : 10791.69 words/s : L.r. 2.2439e-04
[2019-04-17 12:51:57] Ep. 1 : Up. 28700 : Sen. 15,357,405 : Cost 2.67811513 : Time 77.19s : 11891.15 words/s : L.r. 2.2400e-04
[2019-04-17 12:53:09] Ep. 1 : Up. 28800 : Sen. 15,411,607 : Cost 2.69529128 : Time 71.53s : 11168.15 words/s : L.r. 2.2361e-04
[2019-04-17 12:54:19] Ep. 1 : Up. 28900 : Sen. 15,468,616 : Cost 2.63085032 : Time 69.74s : 11299.98 words/s : L.r. 2.2322e-04
[2019-04-17 12:55:28] Ep. 1 : Up. 29000 : Sen. 15,518,270 : Cost 2.62860155 : Time 69.16s : 11393.65 words/s : L.r. 2.2283e-04
[2019-04-17 12:56:38] Ep. 1 : Up. 29100 : Sen. 15,576,214 : Cost 2.66654849 : Time 70.31s : 11128.82 words/s : L.r. 2.2245e-04
[2019-04-17 12:57:50] Ep. 1 : Up. 29200 : Sen. 15,628,895 : Cost 2.67993689 : Time 72.15s : 11578.08 words/s : L.r. 2.2207e-04
[2019-04-17 12:58:58] Ep. 1 : Up. 29300 : Sen. 15,675,146 : Cost 2.69788241 : Time 67.50s : 11191.20 words/s : L.r. 2.2169e-04
[2019-04-17 13:00:06] Ep. 1 : Up. 29400 : Sen. 15,721,179 : Cost 2.68424177 : Time 68.40s : 10989.97 words/s : L.r. 2.2131e-04
[2019-04-17 13:01:18] Ep. 1 : Up. 29500 : Sen. 15,782,283 : Cost 2.67628431 : Time 71.91s : 11365.81 words/s : L.r. 2.2094e-04
[2019-04-17 13:02:28] Ep. 1 : Up. 29600 : Sen. 15,830,626 : Cost 2.66993570 : Time 69.93s : 11204.89 words/s : L.r. 2.2056e-04
[2019-04-17 13:03:38] Ep. 1 : Up. 29700 : Sen. 15,885,069 : Cost 2.63928366 : Time 70.49s : 11220.89 words/s : L.r. 2.2019e-04
[2019-04-17 13:04:51] Ep. 1 : Up. 29800 : Sen. 15,931,040 : Cost 2.67267394 : Time 72.64s : 11518.29 words/s : L.r. 2.1982e-04
[2019-04-17 13:06:04] Ep. 1 : Up. 29900 : Sen. 15,987,691 : Cost 2.67379045 : Time 72.71s : 11700.43 words/s : L.r. 2.1946e-04
[2019-04-17 13:07:18] Ep. 1 : Up. 30000 : Sen. 16,037,840 : Cost 2.66404891 : Time 73.94s : 11249.59 words/s : L.r. 2.1909e-04
[2019-04-17 13:07:18] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 13:07:22] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 13:07:26] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.optimizer.npz
[2019-04-17 13:07:38] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-ce-mean-words.npz
[2019-04-17 13:07:40] [valid] Ep. 1 : Up. 30000 : ce-mean-words : 1.54149 : new best
[2019-04-17 13:07:44] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-perplexity.npz
[2019-04-17 13:07:47] [valid] Ep. 1 : Up. 30000 : perplexity : 4.67155 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 13:08:37] [valid] Ep. 1 : Up. 30000 : translation : 26.24 : stalled 2 times (last best: 26.31)
[2019-04-17 13:09:48] Ep. 1 : Up. 30100 : Sen. 16,087,784 : Cost 2.69540381 : Time 149.84s : 5078.43 words/s : L.r. 2.1872e-04
[2019-04-17 13:11:01] Ep. 1 : Up. 30200 : Sen. 16,143,710 : Cost 2.68579102 : Time 73.19s : 11372.92 words/s : L.r. 2.1836e-04
[2019-04-17 13:12:12] Ep. 1 : Up. 30300 : Sen. 16,194,496 : Cost 2.68257952 : Time 70.89s : 11340.83 words/s : L.r. 2.1800e-04
[2019-04-17 13:13:26] Ep. 1 : Up. 30400 : Sen. 16,241,757 : Cost 2.66732168 : Time 74.71s : 11674.40 words/s : L.r. 2.1764e-04
[2019-04-17 13:14:37] Ep. 1 : Up. 30500 : Sen. 16,293,055 : Cost 2.66035438 : Time 70.30s : 11671.68 words/s : L.r. 2.1729e-04
[2019-04-17 13:15:49] Ep. 1 : Up. 30600 : Sen. 16,347,150 : Cost 2.63624287 : Time 72.74s : 11790.19 words/s : L.r. 2.1693e-04
[2019-04-17 13:16:59] Ep. 1 : Up. 30700 : Sen. 16,398,265 : Cost 2.65073252 : Time 70.04s : 11256.59 words/s : L.r. 2.1658e-04
[2019-04-17 13:18:12] Ep. 1 : Up. 30800 : Sen. 16,451,235 : Cost 2.70068884 : Time 72.63s : 11426.00 words/s : L.r. 2.1623e-04
[2019-04-17 13:19:24] Ep. 1 : Up. 30900 : Sen. 16,514,019 : Cost 2.64261937 : Time 72.29s : 11432.85 words/s : L.r. 2.1587e-04
[2019-04-17 13:20:37] Ep. 1 : Up. 31000 : Sen. 16,564,320 : Cost 2.65927434 : Time 72.53s : 11102.78 words/s : L.r. 2.1553e-04
[2019-04-17 13:21:44] Ep. 1 : Up. 31100 : Sen. 16,618,849 : Cost 2.66169906 : Time 67.38s : 10685.50 words/s : L.r. 2.1518e-04
[2019-04-17 13:22:56] Ep. 1 : Up. 31200 : Sen. 16,670,271 : Cost 2.67849851 : Time 71.72s : 11365.45 words/s : L.r. 2.1483e-04
[2019-04-17 13:24:05] Ep. 1 : Up. 31300 : Sen. 16,721,210 : Cost 2.70569539 : Time 69.18s : 10915.64 words/s : L.r. 2.1449e-04
[2019-04-17 13:25:16] Ep. 1 : Up. 31400 : Sen. 16,772,005 : Cost 2.64992261 : Time 70.54s : 10999.35 words/s : L.r. 2.1415e-04
[2019-04-17 13:26:28] Ep. 1 : Up. 31500 : Sen. 16,820,417 : Cost 2.62463832 : Time 72.31s : 11275.38 words/s : L.r. 2.1381e-04
[2019-04-17 13:27:43] Ep. 1 : Up. 31600 : Sen. 16,884,457 : Cost 2.66190124 : Time 75.42s : 11419.01 words/s : L.r. 2.1347e-04
[2019-04-17 13:28:55] Ep. 1 : Up. 31700 : Sen. 16,946,448 : Cost 2.63431263 : Time 71.57s : 11028.50 words/s : L.r. 2.1313e-04
[2019-04-17 13:30:04] Ep. 1 : Up. 31800 : Sen. 16,999,144 : Cost 2.71144843 : Time 69.43s : 11041.60 words/s : L.r. 2.1280e-04
[2019-04-17 13:31:19] Ep. 1 : Up. 31900 : Sen. 17,062,128 : Cost 2.66875219 : Time 74.94s : 11406.50 words/s : L.r. 2.1246e-04
[2019-04-17 13:32:34] Ep. 1 : Up. 32000 : Sen. 17,122,352 : Cost 2.66521311 : Time 74.49s : 11328.37 words/s : L.r. 2.1213e-04
[2019-04-17 13:32:34] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.orig.npz
[2019-04-17 13:32:38] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz
[2019-04-17 13:32:42] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.optimizer.npz
[2019-04-17 13:32:53] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-ce-mean-words.npz
[2019-04-17 13:32:56] [valid] Ep. 1 : Up. 32000 : ce-mean-words : 1.54062 : new best
[2019-04-17 13:33:00] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-perplexity.npz
[2019-04-17 13:33:02] [valid] Ep. 1 : Up. 32000 : perplexity : 4.66749 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 13:33:54] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz.best-translation.npz
[2019-04-17 13:33:56] [valid] Ep. 1 : Up. 32000 : translation : 26.34 : new best
[2019-04-17 13:35:17] Ep. 1 : Up. 32100 : Sen. 17,188,438 : Cost 2.70109081 : Time 162.88s : 5892.25 words/s : L.r. 2.1180e-04
[2019-04-17 13:36:24] Ep. 1 : Up. 32200 : Sen. 17,226,989 : Cost 2.63250518 : Time 66.90s : 10779.54 words/s : L.r. 2.1147e-04
[2019-04-17 13:37:34] Ep. 1 : Up. 32300 : Sen. 17,275,871 : Cost 2.71255255 : Time 70.71s : 11395.85 words/s : L.r. 2.1114e-04
[2019-04-17 13:38:46] Ep. 1 : Up. 32400 : Sen. 17,333,835 : Cost 2.69546962 : Time 71.75s : 11125.07 words/s : L.r. 2.1082e-04
[2019-04-17 13:39:55] Ep. 1 : Up. 32500 : Sen. 17,384,886 : Cost 2.69607234 : Time 69.25s : 11092.88 words/s : L.r. 2.1049e-04
[2019-04-17 13:41:07] Ep. 1 : Up. 32600 : Sen. 17,436,006 : Cost 2.64079952 : Time 71.22s : 10911.07 words/s : L.r. 2.1017e-04
[2019-04-17 13:42:14] Ep. 1 : Up. 32700 : Sen. 17,484,465 : Cost 2.61802816 : Time 67.78s : 10914.36 words/s : L.r. 2.0985e-04
[2019-04-17 13:43:26] Ep. 1 : Up. 32800 : Sen. 17,534,744 : Cost 2.63108683 : Time 71.15s : 11230.50 words/s : L.r. 2.0953e-04
[2019-04-17 13:44:39] Ep. 1 : Up. 32900 : Sen. 17,589,874 : Cost 2.66421556 : Time 73.81s : 11465.64 words/s : L.r. 2.0921e-04
[2019-04-17 13:45:55] Ep. 1 : Up. 33000 : Sen. 17,650,538 : Cost 2.68685865 : Time 75.42s : 11593.91 words/s : L.r. 2.0889e-04
[2019-04-17 13:47:02] Ep. 1 : Up. 33100 : Sen. 17,701,049 : Cost 2.69722414 : Time 67.42s : 10882.86 words/s : L.r. 2.0858e-04
[2019-04-17 13:48:14] Ep. 1 : Up. 33200 : Sen. 17,758,334 : Cost 2.60959435 : Time 72.19s : 11602.50 words/s : L.r. 2.0826e-04
[2019-04-17 13:49:25] Ep. 1 : Up. 33300 : Sen. 17,803,982 : Cost 2.69002938 : Time 71.01s : 11160.76 words/s : L.r. 2.0795e-04
[2019-04-17 13:50:39] Ep. 1 : Up. 33400 : Sen. 17,861,496 : Cost 2.62012005 : Time 73.95s : 11339.10 words/s : L.r. 2.0764e-04
[2019-04-17 13:51:48] Ep. 1 : Up. 33500 : Sen. 17,917,157 : Cost 2.68810320 : Time 68.71s : 11249.87 words/s : L.r. 2.0733e-04
[2019-04-17 13:53:00] Ep. 1 : Up. 33600 : Sen. 17,972,876 : Cost 2.64780474 : Time 71.72s : 11275.52 words/s : L.r. 2.0702e-04
[2019-04-17 13:54:11] Ep. 1 : Up. 33700 : Sen. 18,018,192 : Cost 2.67873478 : Time 70.84s : 11561.82 words/s : L.r. 2.0671e-04
[2019-04-17 13:55:27] Ep. 1 : Up. 33800 : Sen. 18,090,616 : Cost 2.68850994 : Time 76.23s : 11667.28 words/s : L.r. 2.0641e-04
[2019-04-17 13:56:37] Ep. 1 : Up. 33900 : Sen. 18,144,852 : Cost 2.67567778 : Time 70.59s : 10931.32 words/s : L.r. 2.0610e-04
train_bt_encs_biclean+bt_noise.cosmas_doc_6l_from_base.sh: line 29: 112060 Terminated              $marian/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 7000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
