[2019-04-17 00:56:43] [marian] Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 00:56:43] [marian] Running on cosmas.lingea.cz as process 107970 with command line:
[2019-04-17 00:56:43] [marian] /home/large/data/models/marian/marian-doc/doc-marian-cosmas/build/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.cs.bpe corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.cs.bpe newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
[2019-04-17 00:56:44] [config] after-batches: 0
[2019-04-17 00:56:44] [config] after-epochs: 1
[2019-04-17 00:56:44] [config] allow-unk: false
[2019-04-17 00:56:44] [config] beam-size: 6
[2019-04-17 00:56:44] [config] bert-class-symbol: "[CLS]"
[2019-04-17 00:56:44] [config] bert-mask-symbol: "[MASK]"
[2019-04-17 00:56:44] [config] bert-masking-fraction: 0.15
[2019-04-17 00:56:44] [config] bert-sep-symbol: "[SEP]"
[2019-04-17 00:56:44] [config] bert-train-type-embeddings: true
[2019-04-17 00:56:44] [config] bert-type-vocab-size: 2
[2019-04-17 00:56:44] [config] best-deep: false
[2019-04-17 00:56:44] [config] clip-gemm: 0
[2019-04-17 00:56:44] [config] clip-norm: 5
[2019-04-17 00:56:44] [config] context-enc-depth: 6
[2019-04-17 00:56:44] [config] cost-type: ce-mean-words
[2019-04-17 00:56:44] [config] cpu-threads: 0
[2019-04-17 00:56:44] [config] data-weighting: ""
[2019-04-17 00:56:44] [config] data-weighting-type: sentence
[2019-04-17 00:56:44] [config] dec-cell: gru
[2019-04-17 00:56:44] [config] dec-cell-base-depth: 2
[2019-04-17 00:56:44] [config] dec-cell-high-depth: 1
[2019-04-17 00:56:44] [config] dec-depth: 6
[2019-04-17 00:56:44] [config] devices:
[2019-04-17 00:56:44] [config]   - 2
[2019-04-17 00:56:44] [config]   - 3
[2019-04-17 00:56:44] [config] dim-emb: 512
[2019-04-17 00:56:44] [config] dim-rnn: 1024
[2019-04-17 00:56:44] [config] dim-vocabs:
[2019-04-17 00:56:44] [config]   - 34028
[2019-04-17 00:56:44] [config]   - 34028
[2019-04-17 00:56:44] [config]   - 34028
[2019-04-17 00:56:44] [config] disp-first: 0
[2019-04-17 00:56:44] [config] disp-freq: 100
[2019-04-17 00:56:44] [config] disp-label-counts: false
[2019-04-17 00:56:44] [config] dropout-rnn: 0
[2019-04-17 00:56:44] [config] dropout-src: 0
[2019-04-17 00:56:44] [config] dropout-trg: 0
[2019-04-17 00:56:44] [config] dump-config: ""
[2019-04-17 00:56:44] [config] early-stopping: 15
[2019-04-17 00:56:44] [config] embedding-fix-src: true
[2019-04-17 00:56:44] [config] embedding-fix-trg: true
[2019-04-17 00:56:44] [config] embedding-normalization: false
[2019-04-17 00:56:44] [config] embedding-vectors:
[2019-04-17 00:56:44] [config]   []
[2019-04-17 00:56:44] [config] enc-cell: gru
[2019-04-17 00:56:44] [config] enc-cell-depth: 1
[2019-04-17 00:56:44] [config] enc-depth: 6
[2019-04-17 00:56:44] [config] enc-type: bidirectional
[2019-04-17 00:56:44] [config] exponential-smoothing: 0.0001
[2019-04-17 00:56:44] [config] freeze: true
[2019-04-17 00:56:44] [config] grad-dropping-momentum: 0
[2019-04-17 00:56:44] [config] grad-dropping-rate: 0
[2019-04-17 00:56:44] [config] grad-dropping-warmup: 100
[2019-04-17 00:56:44] [config] guided-alignment: none
[2019-04-17 00:56:44] [config] guided-alignment-cost: mse
[2019-04-17 00:56:44] [config] guided-alignment-weight: 0.1
[2019-04-17 00:56:44] [config] hier-att: false
[2019-04-17 00:56:44] [config] ignore-model-config: false
[2019-04-17 00:56:44] [config] input-types:
[2019-04-17 00:56:44] [config]   []
[2019-04-17 00:56:44] [config] interpolate-env-vars: false
[2019-04-17 00:56:44] [config] keep-best: true
[2019-04-17 00:56:44] [config] label-smoothing: 0.1
[2019-04-17 00:56:44] [config] layer-normalization: false
[2019-04-17 00:56:44] [config] learn-rate: 0.0003
[2019-04-17 00:56:44] [config] log: model/bt_encz.log
[2019-04-17 00:56:44] [config] log-level: info
[2019-04-17 00:56:44] [config] log-time-zone: ""
[2019-04-17 00:56:44] [config] lr-decay: 0
[2019-04-17 00:56:44] [config] lr-decay-freq: 50000
[2019-04-17 00:56:44] [config] lr-decay-inv-sqrt:
[2019-04-17 00:56:44] [config]   - 16000
[2019-04-17 00:56:44] [config] lr-decay-repeat-warmup: false
[2019-04-17 00:56:44] [config] lr-decay-reset-optimizer: false
[2019-04-17 00:56:44] [config] lr-decay-start:
[2019-04-17 00:56:44] [config]   - 10
[2019-04-17 00:56:44] [config]   - 1
[2019-04-17 00:56:44] [config] lr-decay-strategy: epoch+stalled
[2019-04-17 00:56:44] [config] lr-report: true
[2019-04-17 00:56:44] [config] lr-warmup: 16000
[2019-04-17 00:56:44] [config] lr-warmup-at-reload: false
[2019-04-17 00:56:44] [config] lr-warmup-cycle: false
[2019-04-17 00:56:44] [config] lr-warmup-start-rate: 0
[2019-04-17 00:56:44] [config] max-length: 95
[2019-04-17 00:56:44] [config] max-length-crop: false
[2019-04-17 00:56:44] [config] max-length-factor: 3
[2019-04-17 00:56:44] [config] maxi-batch: 1000
[2019-04-17 00:56:44] [config] maxi-batch-sort: trg
[2019-04-17 00:56:44] [config] mini-batch: 1000
[2019-04-17 00:56:44] [config] mini-batch-fit: true
[2019-04-17 00:56:44] [config] mini-batch-fit-step: 10
[2019-04-17 00:56:44] [config] mini-batch-overstuff: 1
[2019-04-17 00:56:44] [config] mini-batch-track-lr: false
[2019-04-17 00:56:44] [config] mini-batch-understuff: 1
[2019-04-17 00:56:44] [config] mini-batch-warmup: 0
[2019-04-17 00:56:44] [config] mini-batch-words: 0
[2019-04-17 00:56:44] [config] mini-batch-words-ref: 0
[2019-04-17 00:56:44] [config] model: model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 00:56:44] [config] multi-loss-type: sum
[2019-04-17 00:56:44] [config] multi-node: false
[2019-04-17 00:56:44] [config] multi-node-overlap: true
[2019-04-17 00:56:44] [config] n-best: false
[2019-04-17 00:56:44] [config] no-nccl: false
[2019-04-17 00:56:44] [config] no-reload: false
[2019-04-17 00:56:44] [config] no-restore-corpus: false
[2019-04-17 00:56:44] [config] no-shuffle: false
[2019-04-17 00:56:44] [config] normalize: 0.6
[2019-04-17 00:56:44] [config] num-devices: 0
[2019-04-17 00:56:44] [config] optimizer: adam
[2019-04-17 00:56:44] [config] optimizer-delay: 4
[2019-04-17 00:56:44] [config] optimizer-params:
[2019-04-17 00:56:44] [config]   - 0.9
[2019-04-17 00:56:44] [config]   - 0.98
[2019-04-17 00:56:44] [config]   - 1e-09
[2019-04-17 00:56:44] [config] overwrite: true
[2019-04-17 00:56:44] [config] pretrained-model: model/model_base_encz2.npz
[2019-04-17 00:56:44] [config] quiet: false
[2019-04-17 00:56:44] [config] quiet-translation: true
[2019-04-17 00:56:44] [config] relative-paths: false
[2019-04-17 00:56:44] [config] right-left: false
[2019-04-17 00:56:44] [config] save-freq: 2000
[2019-04-17 00:56:44] [config] seed: 1111
[2019-04-17 00:56:44] [config] shuffle-in-ram: false
[2019-04-17 00:56:44] [config] skip: false
[2019-04-17 00:56:44] [config] sqlite: ""
[2019-04-17 00:56:44] [config] sqlite-drop: false
[2019-04-17 00:56:44] [config] sync-sgd: true
[2019-04-17 00:56:44] [config] tempdir: /tmp
[2019-04-17 00:56:44] [config] tied-embeddings: false
[2019-04-17 00:56:44] [config] tied-embeddings-all: true
[2019-04-17 00:56:44] [config] tied-embeddings-src: false
[2019-04-17 00:56:44] [config] train-sets:
[2019-04-17 00:56:44] [config]   - corpus.docs.cs.bpe
[2019-04-17 00:56:44] [config]   - corpus.docs.en.bpe.src
[2019-04-17 00:56:44] [config]   - corpus.docs.cs.bpe
[2019-04-17 00:56:44] [config] transformer-aan-activation: swish
[2019-04-17 00:56:44] [config] transformer-aan-depth: 2
[2019-04-17 00:56:44] [config] transformer-aan-nogate: false
[2019-04-17 00:56:44] [config] transformer-decoder-autoreg: self-attention
[2019-04-17 00:56:44] [config] transformer-dim-aan: 2048
[2019-04-17 00:56:44] [config] transformer-dim-ffn: 2048
[2019-04-17 00:56:44] [config] transformer-dropout: 0.1
[2019-04-17 00:56:44] [config] transformer-dropout-attention: 0
[2019-04-17 00:56:44] [config] transformer-dropout-ffn: 0
[2019-04-17 00:56:44] [config] transformer-ffn-activation: swish
[2019-04-17 00:56:44] [config] transformer-ffn-depth: 2
[2019-04-17 00:56:44] [config] transformer-guided-alignment-layer: last
[2019-04-17 00:56:44] [config] transformer-heads: 8
[2019-04-17 00:56:44] [config] transformer-no-projection: false
[2019-04-17 00:56:44] [config] transformer-postprocess: dan
[2019-04-17 00:56:44] [config] transformer-postprocess-emb: d
[2019-04-17 00:56:44] [config] transformer-preprocess: ""
[2019-04-17 00:56:44] [config] transformer-tied-layers:
[2019-04-17 00:56:44] [config]   []
[2019-04-17 00:56:44] [config] transformer-train-position-embeddings: false
[2019-04-17 00:56:44] [config] type: transformer-context
[2019-04-17 00:56:44] [config] ulr: false
[2019-04-17 00:56:44] [config] ulr-dim-emb: 0
[2019-04-17 00:56:44] [config] ulr-dropout: 0
[2019-04-17 00:56:44] [config] ulr-keys-vectors: ""
[2019-04-17 00:56:44] [config] ulr-query-vectors: ""
[2019-04-17 00:56:44] [config] ulr-softmax-temperature: 1
[2019-04-17 00:56:44] [config] ulr-trainable-transformation: false
[2019-04-17 00:56:44] [config] valid-freq: 2000
[2019-04-17 00:56:44] [config] valid-log: model/valid.log
[2019-04-17 00:56:44] [config] valid-max-length: 1000
[2019-04-17 00:56:44] [config] valid-metrics:
[2019-04-17 00:56:44] [config]   - ce-mean-words
[2019-04-17 00:56:44] [config]   - perplexity
[2019-04-17 00:56:44] [config]   - translation
[2019-04-17 00:56:44] [config] valid-mini-batch: 16
[2019-04-17 00:56:44] [config] valid-script-path: ./val.sh
[2019-04-17 00:56:44] [config] valid-sets:
[2019-04-17 00:56:44] [config]   - newstest2016.docs.cs.bpe
[2019-04-17 00:56:44] [config]   - newstest2016.docs.src
[2019-04-17 00:56:44] [config]   - newstest2016.docs.cs.bpe
[2019-04-17 00:56:44] [config] valid-translation-output: ""
[2019-04-17 00:56:44] [config] version: v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 00:56:44] [config] vocabs:
[2019-04-17 00:56:44] [config]   - corp/vocab.encs.yml
[2019-04-17 00:56:44] [config]   - corp/vocab.encs.yml
[2019-04-17 00:56:44] [config]   - corp/vocab.encs.yml
[2019-04-17 00:56:44] [config] word-penalty: 0
[2019-04-17 00:56:44] [config] workspace: 9000
[2019-04-17 00:56:44] [config] Loaded model has been created with Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-17 00:56:44] Using synchronous training
[2019-04-17 00:56:44] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 00:56:45] [data] Setting vocabulary size for input 0 to 34028
[2019-04-17 00:56:45] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 00:56:45] [data] Setting vocabulary size for input 1 to 34028
[2019-04-17 00:56:45] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-17 00:56:45] [data] Setting vocabulary size for input 2 to 34028
[2019-04-17 00:56:45] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-04-17 00:56:45] [batching] Collecting statistics for batch fitting with step size 10
[2019-04-17 00:56:47] [memory] Extending reserved space to 9088 MB (device gpu2)
[2019-04-17 00:56:47] [memory] Extending reserved space to 9088 MB (device gpu3)
[2019-04-17 00:56:47] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-17 00:56:47] [comm] NCCLCommunicator constructed successfully.
[2019-04-17 00:56:47] [training] Using 2 GPUs
[2019-04-17 00:56:47] [memory] Reserving 379 MB, device gpu2
[2019-04-17 00:56:47] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-04-17 00:56:48] [memory] Reserving 379 MB, device gpu2
[2019-04-17 00:57:00] [batching] Done. Typical MB size is 29080 target words
[2019-04-17 00:57:00] [memory] Extending reserved space to 9088 MB (device gpu2)
[2019-04-17 00:57:00] [memory] Extending reserved space to 9088 MB (device gpu3)
[2019-04-17 00:57:00] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-17 00:57:00] [comm] NCCLCommunicator constructed successfully.
[2019-04-17 00:57:00] [training] Using 2 GPUs
[2019-04-17 00:57:00] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 00:57:02] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 00:57:03] Loading Adam parameters from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 00:57:06] [memory] Reserving 379 MB, device gpu2
[2019-04-17 00:57:06] [memory] Reserving 379 MB, device gpu3
[2019-04-17 00:57:06] [training] Model reloaded from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 00:57:06] [data] Restoring the corpus state to epoch 1, batch 4000
[2019-04-17 00:57:06] [data] Shuffling data
tcmalloc: large alloc 1073741824 bytes == 0x7f1e5e000000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f1cc4e00000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f1798000000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f1e1e000000 @ 
[2019-04-17 00:57:26] [data] Done reading 57951104 sentences
[2019-04-17 01:00:48] [data] Done shuffling 57951104 sentences to temp files
[2019-04-17 01:02:50] Training started
[2019-04-17 01:02:50] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-04-17 01:02:50] [memory] Reserving 379 MB, device gpu2
[2019-04-17 01:02:50] [memory] Reserving 379 MB, device gpu3
[2019-04-17 01:02:50] [memory] Reserving 379 MB, device gpu3
[2019-04-17 01:02:50] [memory] Reserving 379 MB, device gpu2
[2019-04-17 01:02:50] Loading model from model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 01:02:51] [memory] Reserving 379 MB, device cpu0
[2019-04-17 01:02:52] [memory] Reserving 189 MB, device gpu2
[2019-04-17 01:02:52] [memory] Reserving 189 MB, device gpu3
[2019-04-17 01:05:06] Ep. 1 : Up. 4100 : Sen. 5,683,929 : Cost 2.66714168 : Time 500.75s : 4113.67 words/s : L.r. 7.6875e-05
[2019-04-17 01:07:26] Ep. 1 : Up. 4200 : Sen. 5,826,461 : Cost 2.66309929 : Time 140.56s : 14695.81 words/s : L.r. 7.8750e-05
[2019-04-17 01:09:47] Ep. 1 : Up. 4300 : Sen. 5,973,104 : Cost 2.61438704 : Time 140.89s : 14697.25 words/s : L.r. 8.0625e-05
[2019-04-17 01:12:10] Ep. 1 : Up. 4400 : Sen. 6,129,153 : Cost 2.53903198 : Time 142.51s : 14706.09 words/s : L.r. 8.2500e-05
[2019-04-17 01:14:36] Ep. 1 : Up. 4500 : Sen. 6,262,133 : Cost 2.60312486 : Time 145.89s : 14738.55 words/s : L.r. 8.4375e-05
[2019-04-17 01:17:04] Ep. 1 : Up. 4600 : Sen. 6,396,928 : Cost 2.59723067 : Time 148.13s : 14929.33 words/s : L.r. 8.6250e-05
[2019-04-17 01:19:21] Ep. 1 : Up. 4700 : Sen. 6,529,247 : Cost 2.62595534 : Time 137.22s : 14581.04 words/s : L.r. 8.8125e-05
[2019-04-17 01:21:43] Ep. 1 : Up. 4800 : Sen. 6,651,330 : Cost 2.61365080 : Time 142.22s : 14852.80 words/s : L.r. 9.0000e-05
[2019-04-17 01:24:08] Ep. 1 : Up. 4900 : Sen. 6,795,866 : Cost 2.56201029 : Time 144.83s : 14748.50 words/s : L.r. 9.1875e-05
[2019-04-17 01:26:33] Ep. 1 : Up. 5000 : Sen. 6,959,625 : Cost 2.57557511 : Time 145.03s : 14734.31 words/s : L.r. 9.3750e-05
[2019-04-17 01:28:53] Ep. 1 : Up. 5100 : Sen. 7,075,870 : Cost 2.60639548 : Time 139.82s : 14768.01 words/s : L.r. 9.5625e-05
[2019-04-17 01:31:14] Ep. 1 : Up. 5200 : Sen. 7,218,943 : Cost 2.55454421 : Time 141.13s : 14797.84 words/s : L.r. 9.7500e-05
[2019-04-17 01:33:39] Ep. 1 : Up. 5300 : Sen. 7,362,864 : Cost 2.53336024 : Time 145.23s : 14764.51 words/s : L.r. 9.9375e-05
[2019-04-17 01:36:03] Ep. 1 : Up. 5400 : Sen. 7,502,833 : Cost 2.57584476 : Time 144.28s : 14853.29 words/s : L.r. 1.0125e-04
[2019-04-17 01:38:26] Ep. 1 : Up. 5500 : Sen. 7,649,564 : Cost 2.57062387 : Time 142.23s : 14776.23 words/s : L.r. 1.0313e-04
[2019-04-17 01:40:46] Ep. 1 : Up. 5600 : Sen. 7,768,863 : Cost 2.51352906 : Time 140.66s : 14573.19 words/s : L.r. 1.0500e-04
[2019-04-17 01:43:11] Ep. 1 : Up. 5700 : Sen. 7,922,942 : Cost 2.53854537 : Time 144.55s : 14757.84 words/s : L.r. 1.0687e-04
[2019-04-17 01:45:39] Ep. 1 : Up. 5800 : Sen. 8,068,488 : Cost 2.53151155 : Time 147.93s : 14737.61 words/s : L.r. 1.0875e-04
[2019-04-17 01:47:58] Ep. 1 : Up. 5900 : Sen. 8,198,412 : Cost 2.55314612 : Time 138.86s : 14520.13 words/s : L.r. 1.1063e-04
[2019-04-17 01:50:14] Ep. 1 : Up. 6000 : Sen. 8,351,676 : Cost 2.47864962 : Time 136.43s : 14398.89 words/s : L.r. 1.1250e-04
[2019-04-17 01:50:14] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 01:50:17] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 01:50:21] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 01:50:30] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-17 01:50:32] [valid] Ep. 1 : Up. 6000 : ce-mean-words : 1.45089 : new best
[2019-04-17 01:50:35] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-17 01:50:37] [valid] Ep. 1 : Up. 6000 : perplexity : 4.2669 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 01:51:18] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-17 01:51:20] [valid] Ep. 1 : Up. 6000 : translation : 28.13 : new best
[2019-04-17 01:53:50] Ep. 1 : Up. 6100 : Sen. 8,467,841 : Cost 2.51100802 : Time 215.88s : 10198.30 words/s : L.r. 1.1438e-04
[2019-04-17 01:56:17] Ep. 1 : Up. 6200 : Sen. 8,616,249 : Cost 2.52018094 : Time 147.20s : 14819.67 words/s : L.r. 1.1625e-04
[2019-04-17 01:58:39] Ep. 1 : Up. 6300 : Sen. 8,748,053 : Cost 2.50853992 : Time 142.27s : 14715.26 words/s : L.r. 1.1813e-04
[2019-04-17 02:01:03] Ep. 1 : Up. 6400 : Sen. 8,889,746 : Cost 2.49022055 : Time 143.56s : 14579.25 words/s : L.r. 1.2000e-04
[2019-04-17 02:03:26] Ep. 1 : Up. 6500 : Sen. 9,027,468 : Cost 2.52471972 : Time 142.51s : 14803.28 words/s : L.r. 1.2188e-04
[2019-04-17 02:05:56] Ep. 1 : Up. 6600 : Sen. 9,176,064 : Cost 2.45063663 : Time 150.70s : 14609.91 words/s : L.r. 1.2375e-04
[2019-04-17 02:08:13] Ep. 1 : Up. 6700 : Sen. 9,311,119 : Cost 2.51396370 : Time 136.68s : 14478.07 words/s : L.r. 1.2562e-04
[2019-04-17 02:10:42] Ep. 1 : Up. 6800 : Sen. 9,449,408 : Cost 2.47435522 : Time 148.68s : 14804.36 words/s : L.r. 1.2750e-04
[2019-04-17 02:13:03] Ep. 1 : Up. 6900 : Sen. 9,583,052 : Cost 2.43849850 : Time 141.67s : 14728.26 words/s : L.r. 1.2938e-04
[2019-04-17 02:15:28] Ep. 1 : Up. 7000 : Sen. 9,722,178 : Cost 2.48966432 : Time 144.45s : 14839.78 words/s : L.r. 1.3125e-04
[2019-04-17 02:17:51] Ep. 1 : Up. 7100 : Sen. 9,861,039 : Cost 2.47712922 : Time 143.52s : 14731.23 words/s : L.r. 1.3313e-04
[2019-04-17 02:20:14] Ep. 1 : Up. 7200 : Sen. 10,010,688 : Cost 2.49996424 : Time 142.44s : 14731.44 words/s : L.r. 1.3500e-04
[2019-04-17 02:22:41] Ep. 1 : Up. 7300 : Sen. 10,145,093 : Cost 2.47312045 : Time 147.46s : 14721.40 words/s : L.r. 1.3688e-04
[2019-04-17 02:25:04] Ep. 1 : Up. 7400 : Sen. 10,268,730 : Cost 2.44206905 : Time 142.42s : 14715.65 words/s : L.r. 1.3875e-04
[2019-04-17 02:27:27] Ep. 1 : Up. 7500 : Sen. 10,422,037 : Cost 2.43478727 : Time 143.89s : 14671.26 words/s : L.r. 1.4063e-04
[2019-04-17 02:29:53] Ep. 1 : Up. 7600 : Sen. 10,570,193 : Cost 2.43633986 : Time 145.34s : 14680.12 words/s : L.r. 1.4250e-04
[2019-04-17 02:32:16] Ep. 1 : Up. 7700 : Sen. 10,721,432 : Cost 2.44171071 : Time 143.03s : 14644.48 words/s : L.r. 1.4438e-04
[2019-04-17 02:34:37] Ep. 1 : Up. 7800 : Sen. 10,858,305 : Cost 2.39441442 : Time 140.84s : 14800.16 words/s : L.r. 1.4625e-04
[2019-04-17 02:36:59] Ep. 1 : Up. 7900 : Sen. 10,985,460 : Cost 2.41171718 : Time 142.00s : 14641.77 words/s : L.r. 1.4813e-04
[2019-04-17 02:39:26] Ep. 1 : Up. 8000 : Sen. 11,126,799 : Cost 2.34774351 : Time 147.26s : 14702.58 words/s : L.r. 1.5000e-04
[2019-04-17 02:39:26] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 02:39:29] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 02:39:32] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 02:39:41] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-17 02:39:43] [valid] Ep. 1 : Up. 8000 : ce-mean-words : 1.39085 : new best
[2019-04-17 02:39:46] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-17 02:39:48] [valid] Ep. 1 : Up. 8000 : perplexity : 4.01826 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 02:40:29] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-17 02:40:31] [valid] Ep. 1 : Up. 8000 : translation : 30.03 : new best
[2019-04-17 02:42:57] Ep. 1 : Up. 8100 : Sen. 11,256,265 : Cost 2.40961242 : Time 210.68s : 10218.02 words/s : L.r. 1.5188e-04
[2019-04-17 02:45:24] Ep. 1 : Up. 8200 : Sen. 11,381,872 : Cost 2.35496759 : Time 147.61s : 14722.48 words/s : L.r. 1.5375e-04
[2019-04-17 02:47:50] Ep. 1 : Up. 8300 : Sen. 11,528,711 : Cost 2.23212004 : Time 145.32s : 14623.20 words/s : L.r. 1.5563e-04
[2019-04-17 02:50:08] Ep. 1 : Up. 8400 : Sen. 11,664,537 : Cost 2.26702404 : Time 138.69s : 14417.51 words/s : L.r. 1.5750e-04
[2019-04-17 02:52:28] Ep. 1 : Up. 8500 : Sen. 11,801,314 : Cost 2.22134137 : Time 139.86s : 14634.06 words/s : L.r. 1.5938e-04
[2019-04-17 02:54:53] Ep. 1 : Up. 8600 : Sen. 11,960,610 : Cost 2.16217995 : Time 145.17s : 14761.38 words/s : L.r. 1.6125e-04
[2019-04-17 02:57:17] Ep. 1 : Up. 8700 : Sen. 12,093,505 : Cost 2.16787553 : Time 143.71s : 14854.64 words/s : L.r. 1.6313e-04
[2019-04-17 02:59:41] Ep. 1 : Up. 8800 : Sen. 12,238,268 : Cost 2.12884259 : Time 144.19s : 14947.18 words/s : L.r. 1.6500e-04
[2019-04-17 03:02:04] Ep. 1 : Up. 8900 : Sen. 12,363,455 : Cost 2.12859941 : Time 142.91s : 14638.50 words/s : L.r. 1.6688e-04
[2019-04-17 03:04:30] Ep. 1 : Up. 9000 : Sen. 12,496,609 : Cost 2.05968213 : Time 145.62s : 14746.12 words/s : L.r. 1.6875e-04
[2019-04-17 03:06:50] Ep. 1 : Up. 9100 : Sen. 12,616,320 : Cost 2.11230540 : Time 140.85s : 14569.08 words/s : L.r. 1.7063e-04
[2019-04-17 03:09:15] Ep. 1 : Up. 9200 : Sen. 12,788,229 : Cost 1.91322362 : Time 144.41s : 14660.98 words/s : L.r. 1.7250e-04
[2019-04-17 03:11:42] Ep. 1 : Up. 9300 : Sen. 12,931,355 : Cost 2.09155226 : Time 147.34s : 14725.68 words/s : L.r. 1.7438e-04
[2019-04-17 03:14:05] Ep. 1 : Up. 9400 : Sen. 13,071,077 : Cost 1.99823821 : Time 142.66s : 14656.94 words/s : L.r. 1.7625e-04
[2019-04-17 03:16:26] Ep. 1 : Up. 9500 : Sen. 13,197,851 : Cost 2.03319073 : Time 140.60s : 14505.04 words/s : L.r. 1.7813e-04
[2019-04-17 03:18:49] Ep. 1 : Up. 9600 : Sen. 13,325,805 : Cost 2.00775504 : Time 143.92s : 14827.22 words/s : L.r. 1.8000e-04
[2019-04-17 03:21:18] Ep. 1 : Up. 9700 : Sen. 13,456,604 : Cost 1.97749221 : Time 148.21s : 14816.34 words/s : L.r. 1.8188e-04
[2019-04-17 03:23:40] Ep. 1 : Up. 9800 : Sen. 13,598,089 : Cost 1.87867904 : Time 142.04s : 14732.74 words/s : L.r. 1.8375e-04
[2019-04-17 03:26:00] Ep. 1 : Up. 9900 : Sen. 13,744,605 : Cost 1.94108462 : Time 140.37s : 14693.56 words/s : L.r. 1.8563e-04
[2019-04-17 03:28:27] Ep. 1 : Up. 10000 : Sen. 13,914,613 : Cost 1.86233497 : Time 146.75s : 14701.93 words/s : L.r. 1.8750e-04
[2019-04-17 03:28:27] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 03:28:30] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 03:28:33] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 03:28:42] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-17 03:28:44] [valid] Ep. 1 : Up. 10000 : ce-mean-words : 0.780917 : new best
[2019-04-17 03:28:47] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-17 03:28:49] [valid] Ep. 1 : Up. 10000 : perplexity : 2.18347 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 03:29:29] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-17 03:29:31] [valid] Ep. 1 : Up. 10000 : translation : 56.38 : new best
[2019-04-17 03:31:56] Ep. 1 : Up. 10100 : Sen. 14,045,687 : Cost 1.92761683 : Time 208.70s : 10204.28 words/s : L.r. 1.8938e-04
[2019-04-17 03:34:21] Ep. 1 : Up. 10200 : Sen. 14,191,445 : Cost 1.87633300 : Time 145.33s : 14717.35 words/s : L.r. 1.9125e-04
[2019-04-17 03:36:41] Ep. 1 : Up. 10300 : Sen. 14,334,844 : Cost 1.81871450 : Time 140.55s : 14492.87 words/s : L.r. 1.9313e-04
[2019-04-17 03:39:10] Ep. 1 : Up. 10400 : Sen. 14,474,578 : Cost 1.86768627 : Time 148.82s : 14846.63 words/s : L.r. 1.9500e-04
[2019-04-17 03:41:32] Ep. 1 : Up. 10500 : Sen. 14,611,980 : Cost 1.81810117 : Time 142.21s : 14542.43 words/s : L.r. 1.9688e-04
[2019-04-17 03:44:01] Ep. 1 : Up. 10600 : Sen. 14,753,604 : Cost 1.79974926 : Time 148.86s : 14729.26 words/s : L.r. 1.9875e-04
[2019-04-17 03:46:22] Ep. 1 : Up. 10700 : Sen. 14,888,292 : Cost 1.79051757 : Time 140.57s : 14668.67 words/s : L.r. 2.0062e-04
[2019-04-17 03:48:47] Ep. 1 : Up. 10800 : Sen. 15,026,936 : Cost 1.75710344 : Time 145.08s : 14961.72 words/s : L.r. 2.0250e-04
[2019-04-17 03:51:07] Ep. 1 : Up. 10900 : Sen. 15,172,391 : Cost 1.75003970 : Time 140.03s : 14656.99 words/s : L.r. 2.0437e-04
[2019-04-17 03:53:34] Ep. 1 : Up. 11000 : Sen. 15,310,533 : Cost 1.78857708 : Time 147.24s : 14804.21 words/s : L.r. 2.0625e-04
[2019-04-17 03:55:54] Ep. 1 : Up. 11100 : Sen. 15,453,897 : Cost 1.70432425 : Time 139.39s : 14720.85 words/s : L.r. 2.0813e-04
[2019-04-17 03:58:13] Ep. 1 : Up. 11200 : Sen. 15,594,674 : Cost 1.72589386 : Time 139.31s : 14591.71 words/s : L.r. 2.1000e-04
[2019-04-17 04:00:40] Ep. 1 : Up. 11300 : Sen. 15,732,218 : Cost 1.71890128 : Time 146.64s : 14899.04 words/s : L.r. 2.1188e-04
[2019-04-17 04:03:06] Ep. 1 : Up. 11400 : Sen. 15,841,557 : Cost 1.72291028 : Time 146.59s : 14734.27 words/s : L.r. 2.1375e-04
[2019-04-17 04:05:32] Ep. 1 : Up. 11500 : Sen. 16,001,648 : Cost 1.65408754 : Time 145.49s : 14809.43 words/s : L.r. 2.1563e-04
[2019-04-17 04:07:54] Ep. 1 : Up. 11600 : Sen. 16,146,083 : Cost 1.66285169 : Time 141.88s : 14759.11 words/s : L.r. 2.1750e-04
[2019-04-17 04:10:21] Ep. 1 : Up. 11700 : Sen. 16,282,572 : Cost 1.63496280 : Time 147.34s : 14693.53 words/s : L.r. 2.1938e-04
[2019-04-17 04:12:42] Ep. 1 : Up. 11800 : Sen. 16,403,868 : Cost 1.64703882 : Time 141.16s : 14428.65 words/s : L.r. 2.2125e-04
[2019-04-17 04:15:07] Ep. 1 : Up. 11900 : Sen. 16,564,723 : Cost 1.62415397 : Time 145.50s : 14854.29 words/s : L.r. 2.2312e-04
[2019-04-17 04:17:37] Ep. 1 : Up. 12000 : Sen. 16,715,779 : Cost 1.61850941 : Time 149.99s : 14940.99 words/s : L.r. 2.2500e-04
[2019-04-17 04:17:37] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 04:17:40] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 04:17:44] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 04:17:53] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-17 04:17:55] [valid] Ep. 1 : Up. 12000 : ce-mean-words : 0.278809 : new best
[2019-04-17 04:17:58] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-17 04:18:00] [valid] Ep. 1 : Up. 12000 : perplexity : 1.32155 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 04:18:38] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-17 04:18:40] [valid] Ep. 1 : Up. 12000 : translation : 84.68 : new best
[2019-04-17 04:21:02] Ep. 1 : Up. 12100 : Sen. 16,845,883 : Cost 1.64467788 : Time 204.91s : 10097.31 words/s : L.r. 2.2688e-04
[2019-04-17 04:23:22] Ep. 1 : Up. 12200 : Sen. 16,973,669 : Cost 1.60989022 : Time 139.20s : 14572.00 words/s : L.r. 2.2875e-04
[2019-04-17 04:25:51] Ep. 1 : Up. 12300 : Sen. 17,129,311 : Cost 1.57847714 : Time 149.75s : 14907.65 words/s : L.r. 2.3063e-04
[2019-04-17 04:28:12] Ep. 1 : Up. 12400 : Sen. 17,264,661 : Cost 1.56663704 : Time 140.36s : 14631.15 words/s : L.r. 2.3250e-04
[2019-04-17 04:30:35] Ep. 1 : Up. 12500 : Sen. 17,406,981 : Cost 1.59589124 : Time 142.94s : 14607.45 words/s : L.r. 2.3438e-04
[2019-04-17 04:32:59] Ep. 1 : Up. 12600 : Sen. 17,523,020 : Cost 1.58889186 : Time 144.09s : 14814.66 words/s : L.r. 2.3625e-04
[2019-04-17 04:35:19] Ep. 1 : Up. 12700 : Sen. 17,663,909 : Cost 1.55864406 : Time 139.83s : 14698.12 words/s : L.r. 2.3813e-04
[2019-04-17 04:37:39] Ep. 1 : Up. 12800 : Sen. 17,811,498 : Cost 1.54898834 : Time 140.42s : 14465.18 words/s : L.r. 2.4000e-04
[2019-04-17 04:40:04] Ep. 1 : Up. 12900 : Sen. 17,946,617 : Cost 1.54955149 : Time 145.47s : 14784.87 words/s : L.r. 2.4188e-04
[2019-04-17 04:42:33] Ep. 1 : Up. 13000 : Sen. 18,103,882 : Cost 1.51606727 : Time 148.39s : 15012.60 words/s : L.r. 2.4375e-04
[2019-04-17 04:44:56] Ep. 1 : Up. 13100 : Sen. 18,251,504 : Cost 1.53525424 : Time 143.64s : 14543.49 words/s : L.r. 2.4563e-04
[2019-04-17 04:47:21] Ep. 1 : Up. 13200 : Sen. 18,373,791 : Cost 1.53163481 : Time 144.10s : 14835.99 words/s : L.r. 2.4750e-04
[2019-04-17 04:49:45] Ep. 1 : Up. 13300 : Sen. 18,502,355 : Cost 1.52717853 : Time 144.03s : 14867.05 words/s : L.r. 2.4938e-04
[2019-04-17 04:52:03] Ep. 1 : Up. 13400 : Sen. 18,626,859 : Cost 1.51817966 : Time 138.57s : 14421.49 words/s : L.r. 2.5125e-04
[2019-04-17 04:54:28] Ep. 1 : Up. 13500 : Sen. 18,765,546 : Cost 1.51513743 : Time 144.42s : 14813.76 words/s : L.r. 2.5313e-04
[2019-04-17 04:56:52] Ep. 1 : Up. 13600 : Sen. 18,913,768 : Cost 1.52408600 : Time 143.97s : 14619.22 words/s : L.r. 2.5500e-04
[2019-04-17 04:59:08] Ep. 1 : Up. 13700 : Sen. 19,055,516 : Cost 1.50859940 : Time 136.77s : 14749.33 words/s : L.r. 2.5688e-04
[2019-04-17 05:01:31] Ep. 1 : Up. 13800 : Sen. 19,192,860 : Cost 1.51015103 : Time 142.26s : 14583.61 words/s : L.r. 2.5875e-04
[2019-04-17 05:03:55] Ep. 1 : Up. 13900 : Sen. 19,322,965 : Cost 1.51268351 : Time 144.35s : 14549.25 words/s : L.r. 2.6063e-04
[2019-04-17 05:06:21] Ep. 1 : Up. 14000 : Sen. 19,485,123 : Cost 1.49092865 : Time 146.14s : 14726.42 words/s : L.r. 2.6250e-04
[2019-04-17 05:06:21] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-17 05:06:24] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-17 05:06:27] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-17 05:06:36] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-17 05:06:38] [valid] Ep. 1 : Up. 14000 : ce-mean-words : 0.0980695 : new best
[2019-04-17 05:06:41] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-17 05:06:43] [valid] Ep. 1 : Up. 14000 : perplexity : 1.10304 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-17 05:07:21] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-17 05:07:23] [valid] Ep. 1 : Up. 14000 : translation : 95.34 : new best
[2019-04-17 05:09:45] Ep. 1 : Up. 14100 : Sen. 19,617,602 : Cost 1.50584817 : Time 203.43s : 10126.96 words/s : L.r. 2.6438e-04
[2019-04-17 05:12:08] Ep. 1 : Up. 14200 : Sen. 19,771,454 : Cost 1.50011897 : Time 143.94s : 14693.93 words/s : L.r. 2.6625e-04
[2019-04-17 05:14:39] Ep. 1 : Up. 14300 : Sen. 19,894,666 : Cost 1.48529410 : Time 150.24s : 14782.76 words/s : L.r. 2.6813e-04
[2019-04-17 05:17:08] Ep. 1 : Up. 14400 : Sen. 20,032,945 : Cost 1.48826385 : Time 149.36s : 14686.84 words/s : L.r. 2.7000e-04
[2019-04-17 05:19:27] Ep. 1 : Up. 14500 : Sen. 20,176,457 : Cost 1.48579860 : Time 139.33s : 14565.87 words/s : L.r. 2.7188e-04
[2019-04-17 05:21:53] Ep. 1 : Up. 14600 : Sen. 20,318,101 : Cost 1.47805941 : Time 145.36s : 14728.11 words/s : L.r. 2.7375e-04
[2019-04-17 05:24:23] Ep. 1 : Up. 14700 : Sen. 20,465,841 : Cost 1.47662008 : Time 149.77s : 14950.12 words/s : L.r. 2.7563e-04
[2019-04-17 05:26:50] Ep. 1 : Up. 14800 : Sen. 20,599,871 : Cost 1.48073792 : Time 147.16s : 14893.56 words/s : L.r. 2.7750e-04
[2019-04-17 05:29:13] Ep. 1 : Up. 14900 : Sen. 20,720,717 : Cost 1.48051703 : Time 143.58s : 14668.84 words/s : L.r. 2.7938e-04
train_bt_encs_biclean+bt_noise.cosmas_doc_6l_from_base_refcontext.sh: line 29: 107970 Terminated              $marian/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.cs.bpe corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.cs.bpe newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
