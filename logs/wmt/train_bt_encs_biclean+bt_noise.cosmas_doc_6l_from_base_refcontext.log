[2019-04-16 17:01:39] [marian] Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-16 17:01:39] [marian] Running on cosmas.lingea.cz as process 105891 with command line:
[2019-04-16 17:01:39] [marian] /home/large/data/models/marian/marian-doc/doc-marian-cosmas/build/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.cs.bpe corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.cs.bpe newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
[2019-04-16 17:01:39] [config] after-batches: 0
[2019-04-16 17:01:39] [config] after-epochs: 1
[2019-04-16 17:01:39] [config] allow-unk: false
[2019-04-16 17:01:39] [config] beam-size: 6
[2019-04-16 17:01:39] [config] bert-class-symbol: "[CLS]"
[2019-04-16 17:01:39] [config] bert-mask-symbol: "[MASK]"
[2019-04-16 17:01:39] [config] bert-masking-fraction: 0.15
[2019-04-16 17:01:39] [config] bert-sep-symbol: "[SEP]"
[2019-04-16 17:01:39] [config] bert-train-type-embeddings: true
[2019-04-16 17:01:39] [config] bert-type-vocab-size: 2
[2019-04-16 17:01:39] [config] best-deep: false
[2019-04-16 17:01:39] [config] clip-gemm: 0
[2019-04-16 17:01:39] [config] clip-norm: 5
[2019-04-16 17:01:39] [config] context-enc-depth: 6
[2019-04-16 17:01:39] [config] cost-type: ce-mean-words
[2019-04-16 17:01:39] [config] cpu-threads: 0
[2019-04-16 17:01:39] [config] data-weighting: ""
[2019-04-16 17:01:39] [config] data-weighting-type: sentence
[2019-04-16 17:01:39] [config] dec-cell: gru
[2019-04-16 17:01:39] [config] dec-cell-base-depth: 2
[2019-04-16 17:01:39] [config] dec-cell-high-depth: 1
[2019-04-16 17:01:39] [config] dec-depth: 6
[2019-04-16 17:01:39] [config] devices:
[2019-04-16 17:01:39] [config]   - 2
[2019-04-16 17:01:39] [config]   - 3
[2019-04-16 17:01:39] [config] dim-emb: 512
[2019-04-16 17:01:39] [config] dim-rnn: 1024
[2019-04-16 17:01:39] [config] dim-vocabs:
[2019-04-16 17:01:39] [config]   - 0
[2019-04-16 17:01:39] [config]   - 0
[2019-04-16 17:01:39] [config] disp-first: 0
[2019-04-16 17:01:39] [config] disp-freq: 100
[2019-04-16 17:01:39] [config] disp-label-counts: false
[2019-04-16 17:01:39] [config] dropout-rnn: 0
[2019-04-16 17:01:39] [config] dropout-src: 0
[2019-04-16 17:01:39] [config] dropout-trg: 0
[2019-04-16 17:01:39] [config] dump-config: ""
[2019-04-16 17:01:39] [config] early-stopping: 15
[2019-04-16 17:01:39] [config] embedding-fix-src: true
[2019-04-16 17:01:39] [config] embedding-fix-trg: true
[2019-04-16 17:01:39] [config] embedding-normalization: false
[2019-04-16 17:01:39] [config] embedding-vectors:
[2019-04-16 17:01:39] [config]   []
[2019-04-16 17:01:39] [config] enc-cell: gru
[2019-04-16 17:01:39] [config] enc-cell-depth: 1
[2019-04-16 17:01:39] [config] enc-depth: 6
[2019-04-16 17:01:39] [config] enc-type: bidirectional
[2019-04-16 17:01:39] [config] exponential-smoothing: 0.0001
[2019-04-16 17:01:39] [config] freeze: true
[2019-04-16 17:01:39] [config] grad-dropping-momentum: 0
[2019-04-16 17:01:39] [config] grad-dropping-rate: 0
[2019-04-16 17:01:39] [config] grad-dropping-warmup: 100
[2019-04-16 17:01:39] [config] guided-alignment: none
[2019-04-16 17:01:39] [config] guided-alignment-cost: mse
[2019-04-16 17:01:39] [config] guided-alignment-weight: 0.1
[2019-04-16 17:01:39] [config] hier-att: false
[2019-04-16 17:01:39] [config] ignore-model-config: false
[2019-04-16 17:01:39] [config] input-types:
[2019-04-16 17:01:39] [config]   []
[2019-04-16 17:01:39] [config] interpolate-env-vars: false
[2019-04-16 17:01:39] [config] keep-best: true
[2019-04-16 17:01:39] [config] label-smoothing: 0.1
[2019-04-16 17:01:39] [config] layer-normalization: false
[2019-04-16 17:01:39] [config] learn-rate: 0.0003
[2019-04-16 17:01:39] [config] log: model/bt_encz.log
[2019-04-16 17:01:39] [config] log-level: info
[2019-04-16 17:01:39] [config] log-time-zone: ""
[2019-04-16 17:01:39] [config] lr-decay: 0
[2019-04-16 17:01:39] [config] lr-decay-freq: 50000
[2019-04-16 17:01:39] [config] lr-decay-inv-sqrt:
[2019-04-16 17:01:39] [config]   - 16000
[2019-04-16 17:01:39] [config] lr-decay-repeat-warmup: false
[2019-04-16 17:01:39] [config] lr-decay-reset-optimizer: false
[2019-04-16 17:01:39] [config] lr-decay-start:
[2019-04-16 17:01:39] [config]   - 10
[2019-04-16 17:01:39] [config]   - 1
[2019-04-16 17:01:39] [config] lr-decay-strategy: epoch+stalled
[2019-04-16 17:01:39] [config] lr-report: true
[2019-04-16 17:01:39] [config] lr-warmup: 16000
[2019-04-16 17:01:39] [config] lr-warmup-at-reload: false
[2019-04-16 17:01:39] [config] lr-warmup-cycle: false
[2019-04-16 17:01:39] [config] lr-warmup-start-rate: 0
[2019-04-16 17:01:39] [config] max-length: 95
[2019-04-16 17:01:39] [config] max-length-crop: false
[2019-04-16 17:01:39] [config] max-length-factor: 3
[2019-04-16 17:01:39] [config] maxi-batch: 1000
[2019-04-16 17:01:39] [config] maxi-batch-sort: trg
[2019-04-16 17:01:39] [config] mini-batch: 1000
[2019-04-16 17:01:39] [config] mini-batch-fit: true
[2019-04-16 17:01:39] [config] mini-batch-fit-step: 10
[2019-04-16 17:01:39] [config] mini-batch-overstuff: 1
[2019-04-16 17:01:39] [config] mini-batch-track-lr: false
[2019-04-16 17:01:39] [config] mini-batch-understuff: 1
[2019-04-16 17:01:39] [config] mini-batch-warmup: 0
[2019-04-16 17:01:39] [config] mini-batch-words: 0
[2019-04-16 17:01:39] [config] mini-batch-words-ref: 0
[2019-04-16 17:01:39] [config] model: model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-16 17:01:39] [config] multi-loss-type: sum
[2019-04-16 17:01:39] [config] multi-node: false
[2019-04-16 17:01:39] [config] multi-node-overlap: true
[2019-04-16 17:01:39] [config] n-best: false
[2019-04-16 17:01:39] [config] no-nccl: false
[2019-04-16 17:01:39] [config] no-reload: false
[2019-04-16 17:01:39] [config] no-restore-corpus: false
[2019-04-16 17:01:39] [config] no-shuffle: false
[2019-04-16 17:01:39] [config] normalize: 0.6
[2019-04-16 17:01:39] [config] num-devices: 0
[2019-04-16 17:01:39] [config] optimizer: adam
[2019-04-16 17:01:39] [config] optimizer-delay: 4
[2019-04-16 17:01:39] [config] optimizer-params:
[2019-04-16 17:01:39] [config]   - 0.9
[2019-04-16 17:01:39] [config]   - 0.98
[2019-04-16 17:01:39] [config]   - 1e-09
[2019-04-16 17:01:39] [config] overwrite: true
[2019-04-16 17:01:39] [config] pretrained-model: model/model_base_encz2.npz
[2019-04-16 17:01:39] [config] quiet: false
[2019-04-16 17:01:39] [config] quiet-translation: true
[2019-04-16 17:01:39] [config] relative-paths: false
[2019-04-16 17:01:39] [config] right-left: false
[2019-04-16 17:01:39] [config] save-freq: 2000
[2019-04-16 17:01:39] [config] seed: 1111
[2019-04-16 17:01:39] [config] shuffle-in-ram: false
[2019-04-16 17:01:39] [config] skip: false
[2019-04-16 17:01:39] [config] sqlite: ""
[2019-04-16 17:01:39] [config] sqlite-drop: false
[2019-04-16 17:01:39] [config] sync-sgd: true
[2019-04-16 17:01:39] [config] tempdir: /tmp
[2019-04-16 17:01:39] [config] tied-embeddings: false
[2019-04-16 17:01:39] [config] tied-embeddings-all: true
[2019-04-16 17:01:39] [config] tied-embeddings-src: false
[2019-04-16 17:01:39] [config] train-sets:
[2019-04-16 17:01:39] [config]   - corpus.docs.cs.bpe
[2019-04-16 17:01:39] [config]   - corpus.docs.en.bpe.src
[2019-04-16 17:01:39] [config]   - corpus.docs.cs.bpe
[2019-04-16 17:01:39] [config] transformer-aan-activation: swish
[2019-04-16 17:01:39] [config] transformer-aan-depth: 2
[2019-04-16 17:01:39] [config] transformer-aan-nogate: false
[2019-04-16 17:01:39] [config] transformer-decoder-autoreg: self-attention
[2019-04-16 17:01:39] [config] transformer-dim-aan: 2048
[2019-04-16 17:01:39] [config] transformer-dim-ffn: 2048
[2019-04-16 17:01:39] [config] transformer-dropout: 0.1
[2019-04-16 17:01:39] [config] transformer-dropout-attention: 0
[2019-04-16 17:01:39] [config] transformer-dropout-ffn: 0
[2019-04-16 17:01:39] [config] transformer-ffn-activation: swish
[2019-04-16 17:01:39] [config] transformer-ffn-depth: 2
[2019-04-16 17:01:39] [config] transformer-guided-alignment-layer: last
[2019-04-16 17:01:39] [config] transformer-heads: 8
[2019-04-16 17:01:39] [config] transformer-no-projection: false
[2019-04-16 17:01:39] [config] transformer-postprocess: dan
[2019-04-16 17:01:39] [config] transformer-postprocess-emb: d
[2019-04-16 17:01:39] [config] transformer-preprocess: ""
[2019-04-16 17:01:39] [config] transformer-tied-layers:
[2019-04-16 17:01:39] [config]   []
[2019-04-16 17:01:39] [config] transformer-train-position-embeddings: false
[2019-04-16 17:01:39] [config] type: transformer-context
[2019-04-16 17:01:39] [config] ulr: false
[2019-04-16 17:01:39] [config] ulr-dim-emb: 0
[2019-04-16 17:01:39] [config] ulr-dropout: 0
[2019-04-16 17:01:39] [config] ulr-keys-vectors: ""
[2019-04-16 17:01:39] [config] ulr-query-vectors: ""
[2019-04-16 17:01:39] [config] ulr-softmax-temperature: 1
[2019-04-16 17:01:39] [config] ulr-trainable-transformation: false
[2019-04-16 17:01:39] [config] valid-freq: 2000
[2019-04-16 17:01:39] [config] valid-log: model/valid.log
[2019-04-16 17:01:39] [config] valid-max-length: 1000
[2019-04-16 17:01:39] [config] valid-metrics:
[2019-04-16 17:01:39] [config]   - ce-mean-words
[2019-04-16 17:01:39] [config]   - perplexity
[2019-04-16 17:01:39] [config]   - translation
[2019-04-16 17:01:39] [config] valid-mini-batch: 16
[2019-04-16 17:01:39] [config] valid-script-path: ./val.sh
[2019-04-16 17:01:39] [config] valid-sets:
[2019-04-16 17:01:39] [config]   - newstest2016.docs.cs.bpe
[2019-04-16 17:01:39] [config]   - newstest2016.docs.src
[2019-04-16 17:01:39] [config]   - newstest2016.docs.cs.bpe
[2019-04-16 17:01:39] [config] valid-translation-output: ""
[2019-04-16 17:01:39] [config] vocabs:
[2019-04-16 17:01:39] [config]   - corp/vocab.encs.yml
[2019-04-16 17:01:39] [config]   - corp/vocab.encs.yml
[2019-04-16 17:01:39] [config]   - corp/vocab.encs.yml
[2019-04-16 17:01:39] [config] word-penalty: 0
[2019-04-16 17:01:39] [config] workspace: 9000
[2019-04-16 17:01:39] [config] Model is being created with Marian v1.7.8 1e91cce 2019-04-04 17:46:39 +0200
[2019-04-16 17:01:39] Using synchronous training
[2019-04-16 17:01:39] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-16 17:01:40] [data] Setting vocabulary size for input 0 to 34028
[2019-04-16 17:01:40] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-16 17:01:40] [data] Setting vocabulary size for input 1 to 34028
[2019-04-16 17:01:40] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-04-16 17:01:40] [data] Setting vocabulary size for input 2 to 34028
[2019-04-16 17:01:40] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-04-16 17:01:40] [batching] Collecting statistics for batch fitting with step size 10
[2019-04-16 17:01:42] [memory] Extending reserved space to 9088 MB (device gpu2)
[2019-04-16 17:01:42] [memory] Extending reserved space to 9088 MB (device gpu3)
[2019-04-16 17:01:42] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-16 17:01:43] [comm] NCCLCommunicator constructed successfully.
[2019-04-16 17:01:43] [training] Using 2 GPUs
[2019-04-16 17:01:43] [memory] Reserving 379 MB, device gpu2
[2019-04-16 17:01:43] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-04-16 17:01:43] [memory] Reserving 379 MB, device gpu2
[2019-04-16 17:01:55] [batching] Done. Typical MB size is 29080 target words
[2019-04-16 17:01:55] [memory] Extending reserved space to 9088 MB (device gpu2)
[2019-04-16 17:01:55] [memory] Extending reserved space to 9088 MB (device gpu3)
[2019-04-16 17:01:55] [comm] Using NCCL 2.4.2 for GPU communication
[2019-04-16 17:01:55] [comm] NCCLCommunicator constructed successfully.
[2019-04-16 17:01:55] [training] Using 2 GPUs
[2019-04-16 17:01:55] [training] Initializing model weights with the pre-trained model model/model_base_encz2.npz
[2019-04-16 17:01:55] Loading model from model/model_base_encz2.npz
[2019-04-16 17:01:56] Loading model from model/model_base_encz2.npz
[2019-04-16 17:01:57] Training started
[2019-04-16 17:01:57] [data] Shuffling data
tcmalloc: large alloc 1073741824 bytes == 0x1bde7e000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f3e10afa000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f3d90afa000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7f3ce6000000 @ 
[2019-04-16 17:02:17] [data] Done reading 57951104 sentences
[2019-04-16 17:06:09] [data] Done shuffling 57951104 sentences to temp files
[2019-04-16 17:06:35] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-04-16 17:06:35] [memory] Reserving 379 MB, device gpu2
[2019-04-16 17:06:35] [memory] Reserving 379 MB, device gpu3
[2019-04-16 17:06:35] [memory] Reserving 379 MB, device gpu3
[2019-04-16 17:06:35] [memory] Reserving 379 MB, device gpu2
[2019-04-16 17:06:35] [memory] Reserving 189 MB, device gpu2
[2019-04-16 17:06:35] [memory] Reserving 189 MB, device gpu3
[2019-04-16 17:06:36] [memory] Reserving 379 MB, device gpu2
[2019-04-16 17:06:36] [memory] Reserving 379 MB, device gpu3
[2019-04-16 17:08:53] Ep. 1 : Up. 100 : Sen. 138,106 : Cost 9.74977207 : Time 432.63s : 4892.15 words/s : L.r. 1.8750e-06
[2019-04-16 17:11:20] Ep. 1 : Up. 200 : Sen. 281,773 : Cost 8.84782887 : Time 147.33s : 14687.04 words/s : L.r. 3.7500e-06
[2019-04-16 17:13:44] Ep. 1 : Up. 300 : Sen. 424,144 : Cost 8.53456306 : Time 143.85s : 14584.69 words/s : L.r. 5.6250e-06
[2019-04-16 17:16:05] Ep. 1 : Up. 400 : Sen. 568,312 : Cost 8.15886688 : Time 141.52s : 14608.08 words/s : L.r. 7.5000e-06
[2019-04-16 17:18:33] Ep. 1 : Up. 500 : Sen. 706,967 : Cost 7.91780376 : Time 147.93s : 14744.28 words/s : L.r. 9.3750e-06
[2019-04-16 17:20:57] Ep. 1 : Up. 600 : Sen. 850,308 : Cost 7.18146515 : Time 143.92s : 14600.15 words/s : L.r. 1.1250e-05
[2019-04-16 17:23:18] Ep. 1 : Up. 700 : Sen. 976,325 : Cost 6.26027918 : Time 140.92s : 14635.68 words/s : L.r. 1.3125e-05
[2019-04-16 17:25:42] Ep. 1 : Up. 800 : Sen. 1,114,493 : Cost 5.68438101 : Time 143.54s : 14606.48 words/s : L.r. 1.5000e-05
[2019-04-16 17:28:03] Ep. 1 : Up. 900 : Sen. 1,254,737 : Cost 5.38789129 : Time 141.60s : 14730.78 words/s : L.r. 1.6875e-05
[2019-04-16 17:30:28] Ep. 1 : Up. 1000 : Sen. 1,390,130 : Cost 4.77180529 : Time 144.88s : 14555.32 words/s : L.r. 1.8750e-05
[2019-04-16 17:32:49] Ep. 1 : Up. 1100 : Sen. 1,529,372 : Cost 3.92579198 : Time 141.27s : 14669.86 words/s : L.r. 2.0625e-05
[2019-04-16 17:35:13] Ep. 1 : Up. 1200 : Sen. 1,665,563 : Cost 3.37002516 : Time 143.50s : 14641.71 words/s : L.r. 2.2500e-05
[2019-04-16 17:37:45] Ep. 1 : Up. 1300 : Sen. 1,811,704 : Cost 3.17166591 : Time 152.31s : 14748.01 words/s : L.r. 2.4375e-05
[2019-04-16 17:40:06] Ep. 1 : Up. 1400 : Sen. 1,949,919 : Cost 3.06052566 : Time 140.89s : 14582.55 words/s : L.r. 2.6250e-05
[2019-04-16 17:42:31] Ep. 1 : Up. 1500 : Sen. 2,071,832 : Cost 2.96898532 : Time 144.46s : 14472.82 words/s : L.r. 2.8125e-05
[2019-04-16 17:44:51] Ep. 1 : Up. 1600 : Sen. 2,204,214 : Cost 2.95612311 : Time 140.41s : 14484.59 words/s : L.r. 3.0000e-05
[2019-04-16 17:47:17] Ep. 1 : Up. 1700 : Sen. 2,340,786 : Cost 2.91405582 : Time 145.68s : 14567.87 words/s : L.r. 3.1875e-05
[2019-04-16 17:49:45] Ep. 1 : Up. 1800 : Sen. 2,489,736 : Cost 2.87019396 : Time 147.84s : 14845.85 words/s : L.r. 3.3750e-05
[2019-04-16 17:52:10] Ep. 1 : Up. 1900 : Sen. 2,623,976 : Cost 2.86279416 : Time 145.03s : 14746.62 words/s : L.r. 3.5625e-05
[2019-04-16 17:54:35] Ep. 1 : Up. 2000 : Sen. 2,764,907 : Cost 2.84200239 : Time 145.49s : 14687.84 words/s : L.r. 3.7500e-05
[2019-04-16 17:54:35] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-16 17:54:38] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-16 17:54:40] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-16 17:54:47] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-16 17:54:49] [valid] Ep. 1 : Up. 2000 : ce-mean-words : 1.63753 : new best
[2019-04-16 17:54:52] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-16 17:54:53] [valid] Ep. 1 : Up. 2000 : perplexity : 5.14245 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-16 17:55:36] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-16 17:55:37] [valid] Ep. 1 : Up. 2000 : translation : 25.65 : new best
[2019-04-16 17:58:02] Ep. 1 : Up. 2100 : Sen. 2,912,278 : Cost 2.80366254 : Time 206.80s : 10197.65 words/s : L.r. 3.9375e-05
[2019-04-16 18:00:26] Ep. 1 : Up. 2200 : Sen. 3,055,421 : Cost 2.82264662 : Time 143.82s : 14488.89 words/s : L.r. 4.1250e-05
[2019-04-16 18:02:50] Ep. 1 : Up. 2300 : Sen. 3,199,380 : Cost 2.81787109 : Time 144.08s : 14603.99 words/s : L.r. 4.3125e-05
[2019-04-16 18:05:17] Ep. 1 : Up. 2400 : Sen. 3,334,883 : Cost 2.75208926 : Time 147.51s : 14676.03 words/s : L.r. 4.5000e-05
[2019-04-16 18:07:36] Ep. 1 : Up. 2500 : Sen. 3,485,622 : Cost 2.83291602 : Time 138.99s : 14545.22 words/s : L.r. 4.6875e-05
[2019-04-16 18:10:06] Ep. 1 : Up. 2600 : Sen. 3,629,860 : Cost 2.71492505 : Time 149.59s : 14638.40 words/s : L.r. 4.8750e-05
[2019-04-16 18:12:35] Ep. 1 : Up. 2700 : Sen. 3,780,056 : Cost 2.72244501 : Time 148.73s : 14622.42 words/s : L.r. 5.0625e-05
[2019-04-16 18:14:55] Ep. 1 : Up. 2800 : Sen. 3,903,741 : Cost 2.73827243 : Time 140.01s : 14393.02 words/s : L.r. 5.2500e-05
[2019-04-16 18:17:19] Ep. 1 : Up. 2900 : Sen. 4,038,659 : Cost 2.78119326 : Time 144.09s : 14777.58 words/s : L.r. 5.4375e-05
[2019-04-16 18:19:42] Ep. 1 : Up. 3000 : Sen. 4,166,200 : Cost 2.74508190 : Time 143.67s : 14539.88 words/s : L.r. 5.6250e-05
[2019-04-16 18:22:05] Ep. 1 : Up. 3100 : Sen. 4,279,286 : Cost 2.65906286 : Time 142.83s : 14532.56 words/s : L.r. 5.8125e-05
[2019-04-16 18:24:34] Ep. 1 : Up. 3200 : Sen. 4,425,529 : Cost 2.71708226 : Time 148.81s : 14801.50 words/s : L.r. 6.0000e-05
[2019-04-16 18:26:59] Ep. 1 : Up. 3300 : Sen. 4,568,484 : Cost 2.68917537 : Time 144.70s : 14582.62 words/s : L.r. 6.1875e-05
[2019-04-16 18:29:29] Ep. 1 : Up. 3400 : Sen. 4,712,759 : Cost 2.68937016 : Time 150.49s : 14712.90 words/s : L.r. 6.3750e-05
[2019-04-16 18:31:49] Ep. 1 : Up. 3500 : Sen. 4,842,315 : Cost 2.66049051 : Time 139.49s : 14322.26 words/s : L.r. 6.5625e-05
[2019-04-16 18:34:13] Ep. 1 : Up. 3600 : Sen. 5,004,048 : Cost 2.68946409 : Time 144.54s : 14590.50 words/s : L.r. 6.7500e-05
[2019-04-16 18:36:41] Ep. 1 : Up. 3700 : Sen. 5,143,564 : Cost 2.66865444 : Time 148.22s : 14791.05 words/s : L.r. 6.9375e-05
[2019-04-16 18:39:10] Ep. 1 : Up. 3800 : Sen. 5,278,783 : Cost 2.66423035 : Time 148.79s : 14780.45 words/s : L.r. 7.1250e-05
[2019-04-16 18:41:32] Ep. 1 : Up. 3900 : Sen. 5,410,282 : Cost 2.67232323 : Time 142.21s : 14421.22 words/s : L.r. 7.3125e-05
[2019-04-16 18:44:01] Ep. 1 : Up. 4000 : Sen. 5,551,008 : Cost 2.60705352 : Time 149.09s : 14616.01 words/s : L.r. 7.5000e-05
[2019-04-16 18:44:01] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.orig.npz
[2019-04-16 18:44:05] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz
[2019-04-16 18:44:09] Saving Adam parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.optimizer.npz
[2019-04-16 18:44:18] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-ce-mean-words.npz
[2019-04-16 18:44:20] [valid] Ep. 1 : Up. 4000 : ce-mean-words : 1.5282 : new best
[2019-04-16 18:44:23] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-perplexity.npz
[2019-04-16 18:44:25] [valid] Ep. 1 : Up. 4000 : perplexity : 4.60986 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-04-16 18:45:08] Saving model weights and runtime parameters to model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz.best-translation.npz
[2019-04-16 18:45:10] [valid] Ep. 1 : Up. 4000 : translation : 26.66 : new best
train_bt_encs_biclean+bt_noise.cosmas_doc_6l_from_base_refcontext.sh: line 29: 105891 Terminated              $marian/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc_from_base_refcontext.npz --pretrained-model model/model_base_encz2.npz --type transformer-context --train-sets corpus.docs.cs.bpe corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.cs.bpe newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --transformer-heads 8 --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 2 3 --sync-sgd --seed 1111 --exponential-smoothing
