[2019-05-13 00:21:27] [marian] Marian v1.7.8 1fb870a 2019-05-10 13:28:45 +0200
[2019-05-13 00:21:27] [marian] Running on cosmas.lingea.cz as process 46084 with command line:
[2019-05-13 00:21:27] [marian] /home/large/data/models/marian/marian-doc/doc-marian3_cosmas/build/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc.new.npz --pretrained-model model/model_bt_noise_encz_bicleaner.npz.best-translation.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 6000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --optimizer-delay 8 --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-preprocess n --transformer-postprocess da --tied-embeddings-all --dim-emb 1024 --transformer-dim-ffn 4096 --transformer-heads 16 --transformer-dropout 0.1 --transformer-dropout-attention 0.1 --transformer-dropout-ffn 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 8000 --lr-decay-inv-sqrt 8000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --devices 0 --sync-sgd --seed 0 --exponential-smoothing --sqlite -T /tmp
[2019-05-13 00:21:27] [config] after-batches: 0
[2019-05-13 00:21:27] [config] after-epochs: 1
[2019-05-13 00:21:27] [config] allow-unk: false
[2019-05-13 00:21:27] [config] beam-size: 6
[2019-05-13 00:21:27] [config] bert-class-symbol: "[CLS]"
[2019-05-13 00:21:27] [config] bert-mask-symbol: "[MASK]"
[2019-05-13 00:21:27] [config] bert-masking-fraction: 0.15
[2019-05-13 00:21:27] [config] bert-sep-symbol: "[SEP]"
[2019-05-13 00:21:27] [config] bert-train-type-embeddings: true
[2019-05-13 00:21:27] [config] bert-type-vocab-size: 2
[2019-05-13 00:21:27] [config] best-deep: false
[2019-05-13 00:21:27] [config] clip-gemm: 0
[2019-05-13 00:21:27] [config] clip-norm: 5
[2019-05-13 00:21:27] [config] context-enc-depth: 6
[2019-05-13 00:21:27] [config] context-gate: false
[2019-05-13 00:21:27] [config] cost-type: ce-mean-words
[2019-05-13 00:21:27] [config] cpu-threads: 0
[2019-05-13 00:21:27] [config] data-weighting: ""
[2019-05-13 00:21:27] [config] data-weighting-type: sentence
[2019-05-13 00:21:27] [config] dec-cell: gru
[2019-05-13 00:21:27] [config] dec-cell-base-depth: 2
[2019-05-13 00:21:27] [config] dec-cell-high-depth: 1
[2019-05-13 00:21:27] [config] dec-depth: 6
[2019-05-13 00:21:27] [config] devices:
[2019-05-13 00:21:27] [config]   - 0
[2019-05-13 00:21:27] [config] dim-emb: 1024
[2019-05-13 00:21:27] [config] dim-rnn: 1024
[2019-05-13 00:21:27] [config] dim-vocabs:
[2019-05-13 00:21:27] [config]   - 0
[2019-05-13 00:21:27] [config]   - 0
[2019-05-13 00:21:27] [config] disp-first: 0
[2019-05-13 00:21:27] [config] disp-freq: 100
[2019-05-13 00:21:27] [config] disp-label-counts: false
[2019-05-13 00:21:27] [config] dropout-rnn: 0
[2019-05-13 00:21:27] [config] dropout-src: 0
[2019-05-13 00:21:27] [config] dropout-trg: 0
[2019-05-13 00:21:27] [config] dump-config: ""
[2019-05-13 00:21:27] [config] early-stopping: 15
[2019-05-13 00:21:27] [config] embedding-fix-src: true
[2019-05-13 00:21:27] [config] embedding-fix-trg: true
[2019-05-13 00:21:27] [config] embedding-normalization: false
[2019-05-13 00:21:27] [config] embedding-vectors:
[2019-05-13 00:21:27] [config]   []
[2019-05-13 00:21:27] [config] enc-cell: gru
[2019-05-13 00:21:27] [config] enc-cell-depth: 1
[2019-05-13 00:21:27] [config] enc-depth: 6
[2019-05-13 00:21:27] [config] enc-type: bidirectional
[2019-05-13 00:21:27] [config] exponential-smoothing: 0.0001
[2019-05-13 00:21:27] [config] freeze: true
[2019-05-13 00:21:27] [config] grad-dropping-momentum: 0
[2019-05-13 00:21:27] [config] grad-dropping-rate: 0
[2019-05-13 00:21:27] [config] grad-dropping-warmup: 100
[2019-05-13 00:21:27] [config] guided-alignment: none
[2019-05-13 00:21:27] [config] guided-alignment-cost: mse
[2019-05-13 00:21:27] [config] guided-alignment-weight: 0.1
[2019-05-13 00:21:27] [config] ignore-model-config: false
[2019-05-13 00:21:27] [config] input-types:
[2019-05-13 00:21:27] [config]   []
[2019-05-13 00:21:27] [config] interpolate-env-vars: false
[2019-05-13 00:21:27] [config] keep-best: true
[2019-05-13 00:21:27] [config] label-smoothing: 0.1
[2019-05-13 00:21:27] [config] layer-normalization: false
[2019-05-13 00:21:27] [config] learn-rate: 0.0002
[2019-05-13 00:21:27] [config] log: model/bt_encz.log
[2019-05-13 00:21:27] [config] log-level: info
[2019-05-13 00:21:27] [config] log-time-zone: ""
[2019-05-13 00:21:27] [config] lr-decay: 0
[2019-05-13 00:21:27] [config] lr-decay-freq: 50000
[2019-05-13 00:21:27] [config] lr-decay-inv-sqrt:
[2019-05-13 00:21:27] [config]   - 8000
[2019-05-13 00:21:27] [config] lr-decay-repeat-warmup: false
[2019-05-13 00:21:27] [config] lr-decay-reset-optimizer: false
[2019-05-13 00:21:27] [config] lr-decay-start:
[2019-05-13 00:21:27] [config]   - 10
[2019-05-13 00:21:27] [config]   - 1
[2019-05-13 00:21:27] [config] lr-decay-strategy: epoch+stalled
[2019-05-13 00:21:27] [config] lr-report: true
[2019-05-13 00:21:27] [config] lr-warmup: 8000
[2019-05-13 00:21:27] [config] lr-warmup-at-reload: false
[2019-05-13 00:21:27] [config] lr-warmup-cycle: false
[2019-05-13 00:21:27] [config] lr-warmup-start-rate: 0
[2019-05-13 00:21:27] [config] max-length: 95
[2019-05-13 00:21:27] [config] max-length-crop: false
[2019-05-13 00:21:27] [config] max-length-factor: 3
[2019-05-13 00:21:27] [config] maxi-batch: 1000
[2019-05-13 00:21:27] [config] maxi-batch-sort: trg
[2019-05-13 00:21:27] [config] mini-batch: 1000
[2019-05-13 00:21:27] [config] mini-batch-fit: true
[2019-05-13 00:21:27] [config] mini-batch-fit-step: 10
[2019-05-13 00:21:27] [config] mini-batch-overstuff: 1
[2019-05-13 00:21:27] [config] mini-batch-track-lr: false
[2019-05-13 00:21:27] [config] mini-batch-understuff: 1
[2019-05-13 00:21:27] [config] mini-batch-warmup: 0
[2019-05-13 00:21:27] [config] mini-batch-words: 0
[2019-05-13 00:21:27] [config] mini-batch-words-ref: 0
[2019-05-13 00:21:27] [config] model: model/model_bt_noise_encz_bicleaner_cosmas_doc.new.npz
[2019-05-13 00:21:27] [config] multi-loss-type: sum
[2019-05-13 00:21:27] [config] multi-node: false
[2019-05-13 00:21:27] [config] multi-node-overlap: true
[2019-05-13 00:21:27] [config] n-best: false
[2019-05-13 00:21:27] [config] no-nccl: false
[2019-05-13 00:21:27] [config] no-reload: false
[2019-05-13 00:21:27] [config] no-restore-corpus: false
[2019-05-13 00:21:27] [config] no-shuffle: false
[2019-05-13 00:21:27] [config] normalize: 0.6
[2019-05-13 00:21:27] [config] num-devices: 0
[2019-05-13 00:21:27] [config] optimizer: adam
[2019-05-13 00:21:27] [config] optimizer-delay: 8
[2019-05-13 00:21:27] [config] optimizer-params:
[2019-05-13 00:21:27] [config]   - 0.9
[2019-05-13 00:21:27] [config]   - 0.98
[2019-05-13 00:21:27] [config]   - 1e-09
[2019-05-13 00:21:27] [config] overwrite: true
[2019-05-13 00:21:27] [config] pretrained-model: model/model_bt_noise_encz_bicleaner.npz.best-translation.npz
[2019-05-13 00:21:27] [config] quiet: false
[2019-05-13 00:21:27] [config] quiet-translation: true
[2019-05-13 00:21:27] [config] relative-paths: false
[2019-05-13 00:21:27] [config] right-left: false
[2019-05-13 00:21:27] [config] save-freq: 2000
[2019-05-13 00:21:27] [config] seed: 0
[2019-05-13 00:21:27] [config] shuffle-in-ram: false
[2019-05-13 00:21:27] [config] skip: false
[2019-05-13 00:21:27] [config] sqlite: temporary
[2019-05-13 00:21:27] [config] sqlite-drop: false
[2019-05-13 00:21:27] [config] sync-sgd: true
[2019-05-13 00:21:27] [config] tempdir: /tmp
[2019-05-13 00:21:27] [config] tied-embeddings: false
[2019-05-13 00:21:27] [config] tied-embeddings-all: true
[2019-05-13 00:21:27] [config] tied-embeddings-src: false
[2019-05-13 00:21:27] [config] train-sets:
[2019-05-13 00:21:27] [config]   - corpus.docs.en.bpe.src_prev
[2019-05-13 00:21:27] [config]   - corpus.docs.en.bpe.src
[2019-05-13 00:21:27] [config]   - corpus.docs.cs.bpe
[2019-05-13 00:21:27] [config] transformer-aan-activation: swish
[2019-05-13 00:21:27] [config] transformer-aan-depth: 2
[2019-05-13 00:21:27] [config] transformer-aan-nogate: false
[2019-05-13 00:21:27] [config] transformer-decoder-autoreg: self-attention
[2019-05-13 00:21:27] [config] transformer-dim-aan: 2048
[2019-05-13 00:21:27] [config] transformer-dim-ffn: 4096
[2019-05-13 00:21:27] [config] transformer-dropout: 0.1
[2019-05-13 00:21:27] [config] transformer-dropout-attention: 0.1
[2019-05-13 00:21:27] [config] transformer-dropout-ffn: 0.1
[2019-05-13 00:21:27] [config] transformer-ffn-activation: swish
[2019-05-13 00:21:27] [config] transformer-ffn-depth: 2
[2019-05-13 00:21:27] [config] transformer-guided-alignment-layer: last
[2019-05-13 00:21:27] [config] transformer-heads: 16
[2019-05-13 00:21:27] [config] transformer-no-projection: false
[2019-05-13 00:21:27] [config] transformer-postprocess: da
[2019-05-13 00:21:27] [config] transformer-postprocess-emb: d
[2019-05-13 00:21:27] [config] transformer-preprocess: n
[2019-05-13 00:21:27] [config] transformer-tied-layers:
[2019-05-13 00:21:27] [config]   []
[2019-05-13 00:21:27] [config] transformer-train-position-embeddings: false
[2019-05-13 00:21:27] [config] type: transformer-context
[2019-05-13 00:21:27] [config] ulr: false
[2019-05-13 00:21:27] [config] ulr-dim-emb: 0
[2019-05-13 00:21:27] [config] ulr-dropout: 0
[2019-05-13 00:21:27] [config] ulr-keys-vectors: ""
[2019-05-13 00:21:27] [config] ulr-query-vectors: ""
[2019-05-13 00:21:27] [config] ulr-softmax-temperature: 1
[2019-05-13 00:21:27] [config] ulr-trainable-transformation: false
[2019-05-13 00:21:27] [config] valid-freq: 2000
[2019-05-13 00:21:27] [config] valid-log: model/valid.log
[2019-05-13 00:21:27] [config] valid-max-length: 1000
[2019-05-13 00:21:27] [config] valid-metrics:
[2019-05-13 00:21:27] [config]   - ce-mean-words
[2019-05-13 00:21:27] [config]   - perplexity
[2019-05-13 00:21:27] [config]   - translation
[2019-05-13 00:21:27] [config] valid-mini-batch: 16
[2019-05-13 00:21:27] [config] valid-script-path: ./val.sh
[2019-05-13 00:21:27] [config] valid-sets:
[2019-05-13 00:21:27] [config]   - newstest2016.docs.src_prev
[2019-05-13 00:21:27] [config]   - newstest2016.docs.src
[2019-05-13 00:21:27] [config]   - newstest2016.docs.cs.bpe
[2019-05-13 00:21:27] [config] valid-translation-output: ""
[2019-05-13 00:21:27] [config] vocabs:
[2019-05-13 00:21:27] [config]   - corp/vocab.encs.yml
[2019-05-13 00:21:27] [config]   - corp/vocab.encs.yml
[2019-05-13 00:21:27] [config]   - corp/vocab.encs.yml
[2019-05-13 00:21:27] [config] word-penalty: 0
[2019-05-13 00:21:27] [config] workspace: 6000
[2019-05-13 00:21:27] [config] Model is being created with Marian v1.7.8 1fb870a 2019-05-10 13:28:45 +0200
[2019-05-13 00:21:27] Using synchronous training
[2019-05-13 00:21:27] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-05-13 00:21:27] [data] Setting vocabulary size for input 0 to 34028
[2019-05-13 00:21:27] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-05-13 00:21:27] [data] Setting vocabulary size for input 1 to 34028
[2019-05-13 00:21:27] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.yml
[2019-05-13 00:21:27] [data] Setting vocabulary size for input 2 to 34028
[2019-05-13 00:21:28] [sqlite] Creating temporary database in /tmp
[2019-05-13 00:21:30] [sqlite] Inserted 1000000 lines
[2019-05-13 00:21:33] [sqlite] Inserted 2000000 lines
[2019-05-13 00:21:38] [sqlite] Inserted 4000000 lines
[2019-05-13 00:21:49] [sqlite] Inserted 8000000 lines
[2019-05-13 00:22:15] [sqlite] Inserted 16000000 lines
[2019-05-13 00:23:08] [sqlite] Inserted 32000000 lines
[2019-05-13 00:24:30] [sqlite] Inserted 57951104 lines
[2019-05-13 00:24:30] [sqlite] Creating primary index
[2019-05-13 00:30:09] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-13 00:30:09] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-13 00:30:10] [memory] Extending reserved space to 6016 MB (device gpu0)
[2019-05-13 00:30:10] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-13 00:30:10] [comm] NCCLCommunicator constructed successfully.
[2019-05-13 00:30:10] [training] Using 1 GPUs
[2019-05-13 00:30:10] [memory] Reserving 1286 MB, device gpu0
[2019-05-13 00:30:10] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-05-13 00:30:11] [memory] Reserving 1286 MB, device gpu0
[2019-05-13 00:30:16] [batching] Done. Typical MB size is 7456 target words
[2019-05-13 00:30:16] [memory] Extending reserved space to 6016 MB (device gpu0)
[2019-05-13 00:30:16] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-13 00:30:16] [comm] NCCLCommunicator constructed successfully.
[2019-05-13 00:30:16] [training] Using 1 GPUs
[2019-05-13 00:30:16] [training] Initializing model weights with the pre-trained model model/model_bt_noise_encz_bicleaner.npz.best-translation.npz
[2019-05-13 00:30:16] Loading model from model/model_bt_noise_encz_bicleaner.npz.best-translation.npz
[2019-05-13 00:30:19] Training started
[2019-05-13 00:30:19] [sqlite] Selecting shuffled data
[2019-05-13 00:32:30] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-05-13 00:32:30] [memory] Reserving 1286 MB, device gpu0
[2019-05-13 00:32:30] [memory] Reserving 1286 MB, device gpu0
[2019-05-13 00:32:30] [memory] Reserving 1286 MB, device gpu0
[2019-05-13 00:32:31] [memory] Reserving 2572 MB, device gpu0
[2019-05-13 00:32:31] Error: CUDA error 2 'out of memory' - /home/large/data/models/marian/marian-doc/doc-marian3_cosmas/src/tensors/gpu/device.cu:38: cudaMalloc(&data_, size)
[2019-05-13 00:32:31] Error: Aborted from virtual void marian::gpu::Device::reserve(size_t) in /home/large/data/models/marian/marian-doc/doc-marian3_cosmas/src/tensors/gpu/device.cu:38

[CALL STACK]
[0x1989511]                                                           
[0x6268c8]                                                            
[0x71bc85]                                                            
[0x814e0c]                                                            
[0x83e76d]                                                            
[0x8b3843]                                                            
[0x845432]                                                            
[0x84701c]                                                            
[0x5117da]                                                            
[0x43f6f9]                                                            
[0x41ab59]                                                            
[0x7f943a3b9830]    __libc_start_main                                  + 0xf0
[0x43bc79]                                                            

train_bt_encs_biclean+bt_noise.doc.sh: line 29: 46084 Aborted                 (core dumped) $marian/marian --model model/model_bt_noise_encz_bicleaner_cosmas_doc.new.npz --pretrained-model model/model_bt_noise_encz_bicleaner.npz.best-translation.npz --type transformer-context --train-sets corpus.docs.en.bpe.src_prev corpus.docs.en.bpe.src corpus.docs.cs.bpe -e 1 --max-length 95 --vocabs corp/vocab.encs.yml corp/vocab.encs.yml corp/vocab.encs.yml --mini-batch-fit -w 6000 --mini-batch 1000 --maxi-batch 1000 --freeze --valid-freq 2000 --save-freq 2000 --disp-freq 100 --embedding-fix-src --embedding-fix-trg --valid-metrics ce-mean-words perplexity translation --valid-sets newstest2016.docs.src_prev newstest2016.docs.src newstest2016.docs.cs.bpe --valid-script-path ./val.sh --quiet-translation --beam-size 6 --normalize=0.6 --valid-mini-batch 16 --overwrite --keep-best --optimizer-delay 8 --early-stopping 15 --cost-type=ce-mean-words --log model/bt_encz.log --valid-log model/valid.log --enc-depth 6 --dec-depth 6 --context-enc-depth 6 --transformer-preprocess n --transformer-postprocess da --tied-embeddings-all --dim-emb 1024 --transformer-dim-ffn 4096 --transformer-heads 16 --transformer-dropout 0.1 --transformer-dropout-attention 0.1 --transformer-dropout-ffn 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 8000 --lr-decay-inv-sqrt 8000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --devices 0 --sync-sgd --seed 0 --exponential-smoothing --sqlite -T /tmp
