[2019-05-10 22:37:22] [marian] Marian v1.7.6 9cc5b17 2018-12-14 15:11:34 -0800
[2019-05-10 22:37:22] [marian] Running on spider3.lingea.cz as process 16665 with command line:
[2019-05-10 22:37:22] [marian] /home/big_maggie/usr/marian_spider/marian_1.7.6/marian-dev/build/marian --model model/model.src1tgt0.concat.npz --type transformer --train-sets corp/opensub.cs-en.docs.train.en.bpe corp/opensub.cs-en.docs.train.cs.bpe --max-length 110 --vocabs corp/vocab.encz.opensub.yml corp/vocab.encz.opensub.yml --mini-batch-fit -w 9000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.cs-en.docs.dev.en.bpe corp/opensub.cs-en.docs.dev.cs.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans2.log --valid-log model/valid_trans2.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 1 --sync-sgd --seed 1111 --no-nccl --exponential-smoothing --no-restore-corpus
[2019-05-10 22:37:22] [config] after-batches: 0
[2019-05-10 22:37:22] [config] after-epochs: 0
[2019-05-10 22:37:22] [config] allow-unk: false
[2019-05-10 22:37:22] [config] beam-size: 6
[2019-05-10 22:37:22] [config] best-deep: false
[2019-05-10 22:37:22] [config] clip-gemm: 0
[2019-05-10 22:37:22] [config] clip-norm: 5
[2019-05-10 22:37:22] [config] cost-type: ce-mean
[2019-05-10 22:37:22] [config] cpu-threads: 0
[2019-05-10 22:37:22] [config] data-weighting-type: sentence
[2019-05-10 22:37:22] [config] dec-cell: gru
[2019-05-10 22:37:22] [config] dec-cell-base-depth: 2
[2019-05-10 22:37:22] [config] dec-cell-high-depth: 1
[2019-05-10 22:37:22] [config] dec-depth: 6
[2019-05-10 22:37:22] [config] devices:
[2019-05-10 22:37:22] [config]   - 0
[2019-05-10 22:37:22] [config]   - 1
[2019-05-10 22:37:22] [config] dim-emb: 512
[2019-05-10 22:37:22] [config] dim-rnn: 1024
[2019-05-10 22:37:22] [config] dim-vocabs:
[2019-05-10 22:37:22] [config]   - 0
[2019-05-10 22:37:22] [config]   - 0
[2019-05-10 22:37:22] [config] disp-first: 0
[2019-05-10 22:37:22] [config] disp-freq: 500
[2019-05-10 22:37:22] [config] disp-label-counts: false
[2019-05-10 22:37:22] [config] dropout-rnn: 0
[2019-05-10 22:37:22] [config] dropout-src: 0
[2019-05-10 22:37:22] [config] dropout-trg: 0
[2019-05-10 22:37:22] [config] early-stopping: 10
[2019-05-10 22:37:22] [config] embedding-fix-src: false
[2019-05-10 22:37:22] [config] embedding-fix-trg: false
[2019-05-10 22:37:22] [config] embedding-normalization: false
[2019-05-10 22:37:22] [config] enc-cell: gru
[2019-05-10 22:37:22] [config] enc-cell-depth: 1
[2019-05-10 22:37:22] [config] enc-depth: 6
[2019-05-10 22:37:22] [config] enc-type: bidirectional
[2019-05-10 22:37:22] [config] exponential-smoothing: 0.0001
[2019-05-10 22:37:22] [config] grad-dropping-momentum: 0
[2019-05-10 22:37:22] [config] grad-dropping-rate: 0
[2019-05-10 22:37:22] [config] grad-dropping-warmup: 100
[2019-05-10 22:37:22] [config] guided-alignment: none
[2019-05-10 22:37:22] [config] guided-alignment-cost: mse
[2019-05-10 22:37:22] [config] guided-alignment-weight: 0.1
[2019-05-10 22:37:22] [config] ignore-model-config: false
[2019-05-10 22:37:22] [config] interpolate-env-vars: false
[2019-05-10 22:37:22] [config] keep-best: false
[2019-05-10 22:37:22] [config] label-smoothing: 0.1
[2019-05-10 22:37:22] [config] layer-normalization: false
[2019-05-10 22:37:22] [config] learn-rate: 0.0003
[2019-05-10 22:37:22] [config] log: model/train_trans2.log
[2019-05-10 22:37:22] [config] log-level: info
[2019-05-10 22:37:22] [config] lr-decay: 0
[2019-05-10 22:37:22] [config] lr-decay-freq: 50000
[2019-05-10 22:37:22] [config] lr-decay-inv-sqrt: 16000
[2019-05-10 22:37:22] [config] lr-decay-repeat-warmup: false
[2019-05-10 22:37:22] [config] lr-decay-reset-optimizer: false
[2019-05-10 22:37:22] [config] lr-decay-start:
[2019-05-10 22:37:22] [config]   - 10
[2019-05-10 22:37:22] [config]   - 1
[2019-05-10 22:37:22] [config] lr-decay-strategy: epoch+stalled
[2019-05-10 22:37:22] [config] lr-report: true
[2019-05-10 22:37:22] [config] lr-warmup: 16000
[2019-05-10 22:37:22] [config] lr-warmup-at-reload: false
[2019-05-10 22:37:22] [config] lr-warmup-cycle: false
[2019-05-10 22:37:22] [config] lr-warmup-start-rate: 0
[2019-05-10 22:37:22] [config] max-length: 110
[2019-05-10 22:37:22] [config] max-length-crop: false
[2019-05-10 22:37:22] [config] max-length-factor: 3
[2019-05-10 22:37:22] [config] maxi-batch: 1000
[2019-05-10 22:37:22] [config] maxi-batch-sort: trg
[2019-05-10 22:37:22] [config] mini-batch: 64
[2019-05-10 22:37:22] [config] mini-batch-fit: true
[2019-05-10 22:37:22] [config] mini-batch-fit-step: 10
[2019-05-10 22:37:22] [config] mini-batch-words: 0
[2019-05-10 22:37:22] [config] model: model/model.src1tgt0.concat.npz
[2019-05-10 22:37:22] [config] multi-node: false
[2019-05-10 22:37:22] [config] multi-node-overlap: true
[2019-05-10 22:37:22] [config] n-best: false
[2019-05-10 22:37:22] [config] no-nccl: true
[2019-05-10 22:37:22] [config] no-reload: false
[2019-05-10 22:37:22] [config] no-restore-corpus: true
[2019-05-10 22:37:22] [config] no-shuffle: false
[2019-05-10 22:37:22] [config] normalize: 0.6
[2019-05-10 22:37:22] [config] optimizer: adam
[2019-05-10 22:37:22] [config] optimizer-delay: 4
[2019-05-10 22:37:22] [config] optimizer-params:
[2019-05-10 22:37:22] [config]   - 0.9
[2019-05-10 22:37:22] [config]   - 0.98
[2019-05-10 22:37:22] [config]   - 1e-09
[2019-05-10 22:37:22] [config] overwrite: false
[2019-05-10 22:37:22] [config] quiet: false
[2019-05-10 22:37:22] [config] quiet-translation: true
[2019-05-10 22:37:22] [config] relative-paths: false
[2019-05-10 22:37:22] [config] right-left: false
[2019-05-10 22:37:22] [config] save-freq: 5000
[2019-05-10 22:37:22] [config] seed: 1111
[2019-05-10 22:37:22] [config] sentencepiece-alphas:
[2019-05-10 22:37:22] [config]   []
[2019-05-10 22:37:22] [config] sentencepiece-max-lines: 10000000
[2019-05-10 22:37:22] [config] sentencepiece-options: ""
[2019-05-10 22:37:22] [config] shuffle-in-ram: false
[2019-05-10 22:37:22] [config] skip: false
[2019-05-10 22:37:22] [config] sqlite: ""
[2019-05-10 22:37:22] [config] sqlite-drop: false
[2019-05-10 22:37:22] [config] sync-sgd: true
[2019-05-10 22:37:22] [config] tempdir: /tmp
[2019-05-10 22:37:22] [config] tied-embeddings: false
[2019-05-10 22:37:22] [config] tied-embeddings-all: true
[2019-05-10 22:37:22] [config] tied-embeddings-src: false
[2019-05-10 22:37:22] [config] train-sets:
[2019-05-10 22:37:22] [config]   - corp/opensub.cs-en.docs.train.en.bpe
[2019-05-10 22:37:22] [config]   - corp/opensub.cs-en.docs.train.cs.bpe
[2019-05-10 22:37:22] [config] transformer-aan-activation: swish
[2019-05-10 22:37:22] [config] transformer-aan-depth: 2
[2019-05-10 22:37:22] [config] transformer-aan-nogate: false
[2019-05-10 22:37:22] [config] transformer-decoder-autoreg: self-attention
[2019-05-10 22:37:22] [config] transformer-dim-aan: 2048
[2019-05-10 22:37:22] [config] transformer-dim-ffn: 2048
[2019-05-10 22:37:22] [config] transformer-dropout: 0.1
[2019-05-10 22:37:22] [config] transformer-dropout-attention: 0
[2019-05-10 22:37:22] [config] transformer-dropout-ffn: 0
[2019-05-10 22:37:22] [config] transformer-ffn-activation: swish
[2019-05-10 22:37:22] [config] transformer-ffn-depth: 2
[2019-05-10 22:37:22] [config] transformer-guided-alignment-layer: last
[2019-05-10 22:37:22] [config] transformer-heads: 8
[2019-05-10 22:37:22] [config] transformer-no-projection: false
[2019-05-10 22:37:22] [config] transformer-postprocess: dan
[2019-05-10 22:37:22] [config] transformer-postprocess-emb: d
[2019-05-10 22:37:22] [config] transformer-preprocess: ""
[2019-05-10 22:37:22] [config] transformer-tied-layers:
[2019-05-10 22:37:22] [config]   []
[2019-05-10 22:37:22] [config] type: transformer
[2019-05-10 22:37:22] [config] ulr: false
[2019-05-10 22:37:22] [config] ulr-dim-emb: 0
[2019-05-10 22:37:22] [config] ulr-dropout: 0
[2019-05-10 22:37:22] [config] ulr-keys-vectors: ""
[2019-05-10 22:37:22] [config] ulr-query-vectors: ""
[2019-05-10 22:37:22] [config] ulr-softmax-temperature: 1
[2019-05-10 22:37:22] [config] ulr-trainable-transformation: false
[2019-05-10 22:37:22] [config] valid-freq: 5000
[2019-05-10 22:37:22] [config] valid-log: model/valid_trans2.log
[2019-05-10 22:37:22] [config] valid-max-length: 1000
[2019-05-10 22:37:22] [config] valid-metrics:
[2019-05-10 22:37:22] [config]   - cross-entropy
[2019-05-10 22:37:22] [config]   - perplexity
[2019-05-10 22:37:22] [config]   - translation
[2019-05-10 22:37:22] [config] valid-mini-batch: 64
[2019-05-10 22:37:22] [config] valid-script-path: ./val.sh
[2019-05-10 22:37:22] [config] valid-sets:
[2019-05-10 22:37:22] [config]   - corp/opensub.cs-en.docs.dev.en.bpe
[2019-05-10 22:37:22] [config]   - corp/opensub.cs-en.docs.dev.cs.bpe
[2019-05-10 22:37:22] [config] valid-translation-output: data/valid.bpe.en.output
[2019-05-10 22:37:22] [config] vocabs:
[2019-05-10 22:37:22] [config]   - corp/vocab.encz.opensub.yml
[2019-05-10 22:37:22] [config]   - corp/vocab.encz.opensub.yml
[2019-05-10 22:37:22] [config] word-penalty: 0
[2019-05-10 22:37:22] [config] workspace: 9000
[2019-05-10 22:37:22] [config] Model is being created with Marian v1.7.6 9cc5b17 2018-12-14 15:11:34 -0800
[2019-05-10 22:37:22] Using synchronous training
[2019-05-10 22:37:22] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.yml
[2019-05-10 22:37:22] [data] Setting vocabulary size for input 0 to 33486
[2019-05-10 22:37:22] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.yml
[2019-05-10 22:37:23] [data] Setting vocabulary size for input 1 to 33486
[2019-05-10 22:37:23] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-10 22:37:23] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-10 22:37:25] [memory] Extending reserved space to 9088 MB (device gpu0)
[2019-05-10 22:37:26] [memory] Extending reserved space to 9088 MB (device gpu1)
[2019-05-10 22:37:26] [comm] NCCL communicator overridden
[2019-05-10 22:37:26] [memory] Reserving 233 MB, device gpu0
[2019-05-10 22:37:26] [memory] Reserving 233 MB, device gpu0
[2019-05-10 22:37:41] [batching] Done
[2019-05-10 22:37:41] [memory] Extending reserved space to 9088 MB (device gpu0)
[2019-05-10 22:37:41] [memory] Extending reserved space to 9088 MB (device gpu1)
[2019-05-10 22:37:41] [comm] NCCL communicator overridden
[2019-05-10 22:37:41] Training started
[2019-05-10 22:37:41] [data] Shuffling files
[2019-05-10 22:38:47] [data] Done reading 42314920 sentences
[2019-05-10 22:42:34] [data] Done shuffling 42314920 sentences to temp files
[2019-05-10 22:42:46] [memory] Reserving 233 MB, device gpu0
[2019-05-10 22:42:46] [memory] Reserving 233 MB, device gpu1
[2019-05-10 22:42:46] [memory] Reserving 116 MB, device gpu0
[2019-05-10 22:42:46] [memory] Reserving 116 MB, device gpu1
[2019-05-10 22:42:46] [memory] Reserving 233 MB, device gpu0
[2019-05-10 22:42:46] [memory] Reserving 233 MB, device gpu1
[2019-05-10 22:42:47] [memory] Reserving 116 MB, device gpu0
[2019-05-10 22:42:47] [memory] Reserving 116 MB, device gpu1
[2019-05-10 22:42:47] [memory] Reserving 233 MB, device gpu1
[2019-05-10 22:42:47] [memory] Reserving 233 MB, device gpu0
[2019-05-10 22:49:32] Ep. 1 : Up. 500 : Sen. 613,714 : Cost 86.55764008 : Time 710.89s : 7715.72 words/s : L.r. 9.3750e-06
[2019-05-10 22:56:29] Ep. 1 : Up. 1000 : Sen. 1,231,840 : Cost 70.89383698 : Time 416.79s : 13215.90 words/s : L.r. 1.8750e-05
[2019-05-10 23:03:26] Ep. 1 : Up. 1500 : Sen. 1,851,608 : Cost 61.54000854 : Time 417.66s : 13162.86 words/s : L.r. 2.8125e-05
[2019-05-10 23:10:18] Ep. 1 : Up. 2000 : Sen. 2,455,613 : Cost 59.15820694 : Time 411.91s : 13085.63 words/s : L.r. 3.7500e-05
[2019-05-10 23:17:15] Ep. 1 : Up. 2500 : Sen. 3,068,664 : Cost 56.84601212 : Time 416.62s : 13148.21 words/s : L.r. 4.6875e-05
[2019-05-10 23:24:13] Ep. 1 : Up. 3000 : Sen. 3,680,913 : Cost 54.62938309 : Time 418.32s : 13080.27 words/s : L.r. 5.6250e-05
[2019-05-10 23:31:11] Ep. 1 : Up. 3500 : Sen. 4,293,584 : Cost 53.12337875 : Time 418.00s : 13139.02 words/s : L.r. 6.5625e-05
[2019-05-10 23:38:06] Ep. 1 : Up. 4000 : Sen. 4,908,121 : Cost 50.62285995 : Time 414.35s : 13168.61 words/s : L.r. 7.5000e-05
[2019-05-10 23:45:00] Ep. 1 : Up. 4500 : Sen. 5,519,712 : Cost 49.61720276 : Time 414.68s : 13218.43 words/s : L.r. 8.4375e-05
[2019-05-10 23:51:55] Ep. 1 : Up. 5000 : Sen. 6,132,772 : Cost 47.45753098 : Time 414.92s : 13140.19 words/s : L.r. 9.3750e-05
[2019-05-10 23:51:55] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-10 23:51:59] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter5000.npz
[2019-05-10 23:52:03] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-10 23:52:08] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-10 23:52:20] [valid] Ep. 1 : Up. 5000 : cross-entropy : 46.1629 : new best
[2019-05-10 23:52:25] [valid] Ep. 1 : Up. 5000 : perplexity : 123.348 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-10 23:58:38] [valid] Ep. 1 : Up. 5000 : translation : 3.87 : new best
[2019-05-11 00:05:32] Ep. 1 : Up. 5500 : Sen. 6,747,157 : Cost 45.63175201 : Time 816.64s : 6672.59 words/s : L.r. 1.0313e-04
[2019-05-11 00:12:27] Ep. 1 : Up. 6000 : Sen. 7,357,675 : Cost 44.44194412 : Time 414.97s : 13198.61 words/s : L.r. 1.1250e-04
[2019-05-11 00:19:22] Ep. 1 : Up. 6500 : Sen. 7,968,340 : Cost 42.46087646 : Time 415.18s : 13151.79 words/s : L.r. 1.2188e-04
[2019-05-11 00:26:19] Ep. 1 : Up. 7000 : Sen. 8,580,316 : Cost 40.41870117 : Time 417.01s : 13093.11 words/s : L.r. 1.3125e-04
[2019-05-11 00:33:14] Ep. 1 : Up. 7500 : Sen. 9,193,498 : Cost 38.72741699 : Time 415.28s : 13129.77 words/s : L.r. 1.4063e-04
[2019-05-11 00:40:08] Ep. 1 : Up. 8000 : Sen. 9,804,445 : Cost 37.27429199 : Time 413.49s : 13167.33 words/s : L.r. 1.5000e-04
[2019-05-11 00:47:03] Ep. 1 : Up. 8500 : Sen. 10,414,496 : Cost 36.30254745 : Time 415.22s : 13184.10 words/s : L.r. 1.5938e-04
[2019-05-11 00:54:02] Ep. 1 : Up. 9000 : Sen. 11,036,690 : Cost 34.94785690 : Time 419.33s : 13192.91 words/s : L.r. 1.6875e-04
[2019-05-11 01:00:54] Ep. 1 : Up. 9500 : Sen. 11,646,472 : Cost 33.85472488 : Time 411.83s : 13135.18 words/s : L.r. 1.7813e-04
[2019-05-11 01:07:48] Ep. 1 : Up. 10000 : Sen. 12,256,273 : Cost 33.62591553 : Time 413.38s : 13183.56 words/s : L.r. 1.8750e-04
[2019-05-11 01:07:48] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 01:07:52] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter10000.npz
[2019-05-11 01:07:56] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 01:08:02] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 01:08:15] [valid] Ep. 1 : Up. 10000 : cross-entropy : 27.8976 : new best
[2019-05-11 01:08:19] [valid] Ep. 1 : Up. 10000 : perplexity : 18.354 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 01:10:37] [valid] Ep. 1 : Up. 10000 : translation : 17.35 : new best
[2019-05-11 01:17:34] Ep. 1 : Up. 10500 : Sen. 12,871,256 : Cost 33.04523468 : Time 586.06s : 9412.09 words/s : L.r. 1.9688e-04
[2019-05-11 01:24:28] Ep. 1 : Up. 11000 : Sen. 13,486,114 : Cost 32.21024704 : Time 414.35s : 13175.33 words/s : L.r. 2.0625e-04
[2019-05-11 01:31:21] Ep. 1 : Up. 11500 : Sen. 14,098,299 : Cost 31.91324806 : Time 413.23s : 13207.18 words/s : L.r. 2.1563e-04
[2019-05-11 01:38:14] Ep. 1 : Up. 12000 : Sen. 14,714,368 : Cost 31.52664948 : Time 413.20s : 13305.14 words/s : L.r. 2.2500e-04
[2019-05-11 01:45:07] Ep. 1 : Up. 12500 : Sen. 15,323,885 : Cost 31.44360542 : Time 412.92s : 13255.71 words/s : L.r. 2.3438e-04
[2019-05-11 01:52:02] Ep. 1 : Up. 13000 : Sen. 15,938,103 : Cost 30.79264069 : Time 414.41s : 13158.05 words/s : L.r. 2.4375e-04
[2019-05-11 01:58:57] Ep. 1 : Up. 13500 : Sen. 16,560,188 : Cost 30.27799797 : Time 414.87s : 13228.60 words/s : L.r. 2.5313e-04
[2019-05-11 02:05:50] Ep. 1 : Up. 14000 : Sen. 17,170,166 : Cost 30.91272545 : Time 413.65s : 13310.74 words/s : L.r. 2.6250e-04
[2019-05-11 02:12:42] Ep. 1 : Up. 14500 : Sen. 17,781,790 : Cost 30.32088661 : Time 412.03s : 13263.79 words/s : L.r. 2.7188e-04
[2019-05-11 02:19:36] Ep. 1 : Up. 15000 : Sen. 18,393,137 : Cost 30.04374695 : Time 414.05s : 13159.67 words/s : L.r. 2.8125e-04
[2019-05-11 02:19:36] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 02:19:40] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter15000.npz
[2019-05-11 02:19:45] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 02:19:49] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 02:20:02] [valid] Ep. 1 : Up. 15000 : cross-entropy : 22.5439 : new best
[2019-05-11 02:20:07] [valid] Ep. 1 : Up. 15000 : perplexity : 10.5006 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 02:22:04] [valid] Ep. 1 : Up. 15000 : translation : 21.73 : new best
[2019-05-11 02:29:00] Ep. 1 : Up. 15500 : Sen. 19,014,058 : Cost 29.71535492 : Time 563.94s : 9764.96 words/s : L.r. 2.9063e-04
[2019-05-11 02:35:56] Ep. 1 : Up. 16000 : Sen. 19,630,195 : Cost 29.97468376 : Time 416.23s : 13270.00 words/s : L.r. 3.0000e-04
[2019-05-11 02:42:48] Ep. 1 : Up. 16500 : Sen. 20,240,023 : Cost 29.57108498 : Time 411.84s : 13189.08 words/s : L.r. 2.9542e-04
[2019-05-11 02:49:40] Ep. 1 : Up. 17000 : Sen. 20,851,459 : Cost 29.41579247 : Time 411.89s : 13234.39 words/s : L.r. 2.9104e-04
[2019-05-11 02:56:33] Ep. 1 : Up. 17500 : Sen. 21,466,610 : Cost 29.23725128 : Time 412.95s : 13276.59 words/s : L.r. 2.8685e-04
[2019-05-11 03:03:23] Ep. 1 : Up. 18000 : Sen. 22,075,999 : Cost 29.18383789 : Time 409.34s : 13329.41 words/s : L.r. 2.8284e-04
[2019-05-11 03:10:10] Ep. 1 : Up. 18500 : Sen. 22,683,067 : Cost 28.95912743 : Time 407.22s : 13313.90 words/s : L.r. 2.7899e-04
[2019-05-11 03:17:03] Ep. 1 : Up. 19000 : Sen. 23,299,088 : Cost 28.62614822 : Time 412.78s : 13255.01 words/s : L.r. 2.7530e-04
[2019-05-11 03:23:52] Ep. 1 : Up. 19500 : Sen. 23,904,457 : Cost 28.96749306 : Time 409.00s : 13311.67 words/s : L.r. 2.7175e-04
[2019-05-11 03:30:43] Ep. 1 : Up. 20000 : Sen. 24,521,184 : Cost 28.35098839 : Time 411.70s : 13297.32 words/s : L.r. 2.6833e-04
[2019-05-11 03:30:43] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 03:30:48] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter20000.npz
[2019-05-11 03:30:51] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 03:30:56] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 03:31:08] [valid] Ep. 1 : Up. 20000 : cross-entropy : 20.6548 : new best
[2019-05-11 03:31:13] [valid] Ep. 1 : Up. 20000 : perplexity : 8.62265 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 03:33:10] [valid] Ep. 1 : Up. 20000 : translation : 23.19 : new best
[2019-05-11 03:40:02] Ep. 1 : Up. 20500 : Sen. 25,135,625 : Cost 28.25477219 : Time 558.76s : 9748.54 words/s : L.r. 2.6504e-04
[2019-05-11 03:46:56] Ep. 1 : Up. 21000 : Sen. 25,753,028 : Cost 28.32202339 : Time 413.62s : 13322.64 words/s : L.r. 2.6186e-04
[2019-05-11 03:53:42] Ep. 1 : Up. 21500 : Sen. 26,352,137 : Cost 28.67207146 : Time 406.62s : 13308.17 words/s : L.r. 2.5880e-04
[2019-05-11 04:00:32] Ep. 1 : Up. 22000 : Sen. 26,968,432 : Cost 28.00080109 : Time 410.11s : 13366.29 words/s : L.r. 2.5584e-04
[2019-05-11 04:07:18] Ep. 1 : Up. 22500 : Sen. 27,574,702 : Cost 27.93481064 : Time 405.48s : 13302.05 words/s : L.r. 2.5298e-04
[2019-05-11 04:14:08] Ep. 1 : Up. 23000 : Sen. 28,190,317 : Cost 27.97520828 : Time 409.79s : 13407.96 words/s : L.r. 2.5022e-04
[2019-05-11 04:20:56] Ep. 1 : Up. 23500 : Sen. 28,802,293 : Cost 27.83732414 : Time 408.87s : 13326.72 words/s : L.r. 2.4754e-04
[2019-05-11 04:27:45] Ep. 1 : Up. 24000 : Sen. 29,409,223 : Cost 28.02553749 : Time 408.56s : 13324.42 words/s : L.r. 2.4495e-04
[2019-05-11 04:34:35] Ep. 1 : Up. 24500 : Sen. 30,025,960 : Cost 27.70533943 : Time 410.27s : 13410.52 words/s : L.r. 2.4244e-04
[2019-05-11 04:41:26] Ep. 1 : Up. 25000 : Sen. 30,641,344 : Cost 27.52407265 : Time 410.98s : 13323.52 words/s : L.r. 2.4000e-04
[2019-05-11 04:41:26] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 04:41:31] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter25000.npz
[2019-05-11 04:41:35] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 04:41:39] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 04:41:52] [valid] Ep. 1 : Up. 25000 : cross-entropy : 19.6198 : new best
[2019-05-11 04:41:56] [valid] Ep. 1 : Up. 25000 : perplexity : 7.74028 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 04:43:51] [valid] Ep. 1 : Up. 25000 : translation : 24.12 : new best
[2019-05-11 04:50:41] Ep. 1 : Up. 25500 : Sen. 31,252,007 : Cost 27.74342918 : Time 554.70s : 9861.35 words/s : L.r. 2.3764e-04
[2019-05-11 04:57:32] Ep. 1 : Up. 26000 : Sen. 31,867,011 : Cost 27.33041382 : Time 411.03s : 13271.57 words/s : L.r. 2.3534e-04
[2019-05-11 05:04:23] Ep. 1 : Up. 26500 : Sen. 32,474,967 : Cost 27.78099060 : Time 411.22s : 13312.65 words/s : L.r. 2.3311e-04
[2019-05-11 05:11:14] Ep. 1 : Up. 27000 : Sen. 33,092,639 : Cost 27.16898537 : Time 411.08s : 13315.78 words/s : L.r. 2.3094e-04
[2019-05-11 05:18:05] Ep. 1 : Up. 27500 : Sen. 33,704,506 : Cost 27.29104614 : Time 411.15s : 13252.26 words/s : L.r. 2.2883e-04
[2019-05-11 05:24:59] Ep. 1 : Up. 28000 : Sen. 34,324,841 : Cost 27.16629219 : Time 413.83s : 13327.01 words/s : L.r. 2.2678e-04
[2019-05-11 05:31:49] Ep. 1 : Up. 28500 : Sen. 34,932,308 : Cost 27.51864624 : Time 409.59s : 13336.18 words/s : L.r. 2.2478e-04
[2019-05-11 05:38:42] Ep. 1 : Up. 29000 : Sen. 35,549,478 : Cost 27.10499191 : Time 412.80s : 13301.02 words/s : L.r. 2.2283e-04
[2019-05-11 05:45:32] Ep. 1 : Up. 29500 : Sen. 36,160,000 : Cost 27.14470482 : Time 410.12s : 13279.45 words/s : L.r. 2.2094e-04
[2019-05-11 05:52:23] Ep. 1 : Up. 30000 : Sen. 36,768,721 : Cost 27.40393066 : Time 411.57s : 13311.84 words/s : L.r. 2.1909e-04
[2019-05-11 05:52:23] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 05:52:28] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter30000.npz
[2019-05-11 05:52:32] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 05:52:36] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 05:52:48] [valid] Ep. 1 : Up. 30000 : cross-entropy : 18.983 : new best
[2019-05-11 05:52:53] [valid] Ep. 1 : Up. 30000 : perplexity : 7.24288 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 05:54:47] [valid] Ep. 1 : Up. 30000 : translation : 24.82 : new best
[2019-05-11 06:01:40] Ep. 1 : Up. 30500 : Sen. 37,391,224 : Cost 26.58898735 : Time 556.69s : 9837.28 words/s : L.r. 2.1729e-04
[2019-05-11 06:08:35] Ep. 1 : Up. 31000 : Sen. 38,008,379 : Cost 27.13607216 : Time 415.17s : 13312.79 words/s : L.r. 2.1553e-04
[2019-05-11 06:15:28] Ep. 1 : Up. 31500 : Sen. 38,618,068 : Cost 26.85262871 : Time 412.40s : 13140.34 words/s : L.r. 2.1381e-04
[2019-05-11 06:22:23] Ep. 1 : Up. 32000 : Sen. 39,231,504 : Cost 27.08256531 : Time 415.63s : 13223.56 words/s : L.r. 2.1213e-04
[2019-05-11 06:29:15] Ep. 1 : Up. 32500 : Sen. 39,842,148 : Cost 26.90157509 : Time 411.55s : 13247.25 words/s : L.r. 2.1049e-04
[2019-05-11 06:36:11] Ep. 1 : Up. 33000 : Sen. 40,457,049 : Cost 26.80935478 : Time 416.66s : 13149.61 words/s : L.r. 2.0889e-04
[2019-05-11 06:43:07] Ep. 1 : Up. 33500 : Sen. 41,076,040 : Cost 26.69788933 : Time 415.65s : 13231.43 words/s : L.r. 2.0733e-04
[2019-05-11 06:49:57] Ep. 1 : Up. 34000 : Sen. 41,679,781 : Cost 26.96227074 : Time 410.30s : 13195.33 words/s : L.r. 2.0580e-04
[2019-05-11 06:56:51] Ep. 1 : Up. 34500 : Sen. 42,291,528 : Cost 26.74900246 : Time 413.81s : 13184.90 words/s : L.r. 2.0430e-04
[2019-05-11 06:57:08] Seen 42313370 samples
[2019-05-11 06:57:08] Starting epoch 2
[2019-05-11 06:57:08] [data] Shuffling files
[2019-05-11 06:57:24] [data] Done reading 42314920 sentences
[2019-05-11 07:01:10] [data] Done shuffling 42314920 sentences to temp files
[2019-05-11 07:07:59] Ep. 2 : Up. 35000 : Sen. 594,469 : Cost 26.70125961 : Time 667.73s : 8264.68 words/s : L.r. 2.0284e-04
[2019-05-11 07:07:59] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 07:08:04] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter35000.npz
[2019-05-11 07:08:08] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 07:08:12] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 07:08:25] [valid] Ep. 2 : Up. 35000 : cross-entropy : 18.5655 : new best
[2019-05-11 07:08:29] [valid] Ep. 2 : Up. 35000 : perplexity : 6.93425 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 07:10:22] [valid] Ep. 2 : Up. 35000 : translation : 25.23 : new best
[2019-05-11 07:17:17] Ep. 2 : Up. 35500 : Sen. 1,206,299 : Cost 26.47944069 : Time 557.57s : 9751.61 words/s : L.r. 2.0140e-04
[2019-05-11 07:24:13] Ep. 2 : Up. 36000 : Sen. 1,816,138 : Cost 26.73569870 : Time 416.56s : 13119.81 words/s : L.r. 2.0000e-04
[2019-05-11 07:31:12] Ep. 2 : Up. 36500 : Sen. 2,432,552 : Cost 26.42016983 : Time 418.50s : 13092.75 words/s : L.r. 1.9863e-04
[2019-05-11 07:38:08] Ep. 2 : Up. 37000 : Sen. 3,046,363 : Cost 26.49853897 : Time 416.20s : 13144.58 words/s : L.r. 1.9728e-04
[2019-05-11 07:45:03] Ep. 2 : Up. 37500 : Sen. 3,653,556 : Cost 26.61950493 : Time 414.82s : 13107.22 words/s : L.r. 1.9596e-04
[2019-05-11 07:51:59] Ep. 2 : Up. 38000 : Sen. 4,271,099 : Cost 26.24076080 : Time 416.24s : 13151.62 words/s : L.r. 1.9467e-04
[2019-05-11 07:58:53] Ep. 2 : Up. 38500 : Sen. 4,881,062 : Cost 26.36878967 : Time 414.56s : 13091.73 words/s : L.r. 1.9340e-04
[2019-05-11 08:05:52] Ep. 2 : Up. 39000 : Sen. 5,497,666 : Cost 26.47644997 : Time 418.96s : 13153.40 words/s : L.r. 1.9215e-04
[2019-05-11 08:12:49] Ep. 2 : Up. 39500 : Sen. 6,110,525 : Cost 26.48986053 : Time 417.01s : 13142.46 words/s : L.r. 1.9093e-04
[2019-05-11 08:19:46] Ep. 2 : Up. 40000 : Sen. 6,720,992 : Cost 26.30824852 : Time 416.88s : 13042.95 words/s : L.r. 1.8974e-04
[2019-05-11 08:19:46] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 08:19:51] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter40000.npz
[2019-05-11 08:19:55] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 08:19:59] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 08:20:12] [valid] Ep. 2 : Up. 40000 : cross-entropy : 18.2663 : new best
[2019-05-11 08:20:16] [valid] Ep. 2 : Up. 40000 : perplexity : 6.72113 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 08:22:11] [valid] Ep. 2 : Up. 40000 : translation : 25.52 : new best
[2019-05-11 08:29:09] Ep. 2 : Up. 40500 : Sen. 7,335,468 : Cost 26.42525864 : Time 562.97s : 9755.91 words/s : L.r. 1.8856e-04
[2019-05-11 08:36:02] Ep. 2 : Up. 41000 : Sen. 7,939,517 : Cost 26.33116150 : Time 412.88s : 13046.15 words/s : L.r. 1.8741e-04
[2019-05-11 08:43:02] Ep. 2 : Up. 41500 : Sen. 8,554,643 : Cost 26.41294861 : Time 419.71s : 13128.48 words/s : L.r. 1.8628e-04
[2019-05-11 08:50:03] Ep. 2 : Up. 42000 : Sen. 9,177,606 : Cost 26.00519371 : Time 420.74s : 13090.84 words/s : L.r. 1.8516e-04
[2019-05-11 08:56:58] Ep. 2 : Up. 42500 : Sen. 9,787,425 : Cost 26.18374443 : Time 415.19s : 13066.09 words/s : L.r. 1.8407e-04
[2019-05-11 09:03:56] Ep. 2 : Up. 43000 : Sen. 10,394,379 : Cost 26.38731384 : Time 417.89s : 13022.36 words/s : L.r. 1.8300e-04
[2019-05-11 09:10:54] Ep. 2 : Up. 43500 : Sen. 11,006,268 : Cost 26.24677086 : Time 418.57s : 13054.86 words/s : L.r. 1.8194e-04
[2019-05-11 09:17:53] Ep. 2 : Up. 44000 : Sen. 11,620,255 : Cost 26.04872704 : Time 419.12s : 13005.40 words/s : L.r. 1.8091e-04
[2019-05-11 09:24:53] Ep. 2 : Up. 44500 : Sen. 12,234,243 : Cost 26.28710938 : Time 419.20s : 13102.96 words/s : L.r. 1.7989e-04
[2019-05-11 09:31:51] Ep. 2 : Up. 45000 : Sen. 12,843,905 : Cost 26.30892754 : Time 418.17s : 13072.66 words/s : L.r. 1.7889e-04
[2019-05-11 09:31:51] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 09:31:55] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter45000.npz
[2019-05-11 09:31:59] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 09:32:03] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 09:32:16] [valid] Ep. 2 : Up. 45000 : cross-entropy : 18.0285 : new best
[2019-05-11 09:32:20] [valid] Ep. 2 : Up. 45000 : perplexity : 6.55647 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 09:34:17] [valid] Ep. 2 : Up. 45000 : translation : 25.78 : new best
[2019-05-11 09:41:15] Ep. 2 : Up. 45500 : Sen. 13,457,836 : Cost 26.05195427 : Time 564.40s : 9668.59 words/s : L.r. 1.7790e-04
[2019-05-11 09:48:16] Ep. 2 : Up. 46000 : Sen. 14,070,996 : Cost 26.37224960 : Time 420.47s : 13105.74 words/s : L.r. 1.7693e-04
[2019-05-11 09:55:13] Ep. 2 : Up. 46500 : Sen. 14,683,359 : Cost 26.03765488 : Time 417.16s : 13056.29 words/s : L.r. 1.7598e-04
[2019-05-11 10:02:12] Ep. 2 : Up. 47000 : Sen. 15,296,000 : Cost 26.14868736 : Time 419.25s : 13060.06 words/s : L.r. 1.7504e-04
[2019-05-11 10:09:14] Ep. 2 : Up. 47500 : Sen. 15,914,377 : Cost 25.94841385 : Time 422.40s : 13004.47 words/s : L.r. 1.7411e-04
[2019-05-11 10:16:14] Ep. 2 : Up. 48000 : Sen. 16,524,917 : Cost 26.19355583 : Time 419.95s : 13017.01 words/s : L.r. 1.7321e-04
[2019-05-11 10:23:12] Ep. 2 : Up. 48500 : Sen. 17,134,321 : Cost 26.18598557 : Time 417.98s : 13065.13 words/s : L.r. 1.7231e-04
[2019-05-11 10:30:13] Ep. 2 : Up. 49000 : Sen. 17,752,786 : Cost 25.96663857 : Time 421.04s : 13080.35 words/s : L.r. 1.7143e-04
[2019-05-11 10:37:14] Ep. 2 : Up. 49500 : Sen. 18,367,272 : Cost 25.85807419 : Time 420.74s : 12974.55 words/s : L.r. 1.7056e-04
[2019-05-11 10:44:14] Ep. 2 : Up. 50000 : Sen. 18,974,270 : Cost 25.99589157 : Time 419.63s : 12914.09 words/s : L.r. 1.6971e-04
[2019-05-11 10:44:14] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 10:44:18] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter50000.npz
[2019-05-11 10:44:22] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 10:44:27] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 10:44:40] [valid] Ep. 2 : Up. 50000 : cross-entropy : 17.837 : new best
[2019-05-11 10:44:44] [valid] Ep. 2 : Up. 50000 : perplexity : 6.42687 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 10:46:40] [valid] Ep. 2 : Up. 50000 : translation : 25.86 : new best
[2019-05-11 10:53:39] Ep. 2 : Up. 50500 : Sen. 19,587,368 : Cost 25.82021713 : Time 565.64s : 9618.81 words/s : L.r. 1.6886e-04
[2019-05-11 11:00:39] Ep. 2 : Up. 51000 : Sen. 20,195,830 : Cost 26.19082451 : Time 419.39s : 13034.92 words/s : L.r. 1.6803e-04
[2019-05-11 11:07:39] Ep. 2 : Up. 51500 : Sen. 20,818,641 : Cost 25.72496986 : Time 420.18s : 13147.92 words/s : L.r. 1.6722e-04
[2019-05-11 11:14:35] Ep. 2 : Up. 52000 : Sen. 21,424,825 : Cost 26.00946999 : Time 415.63s : 13042.09 words/s : L.r. 1.6641e-04
[2019-05-11 11:21:36] Ep. 2 : Up. 52500 : Sen. 22,042,641 : Cost 26.00344086 : Time 421.52s : 13119.19 words/s : L.r. 1.6562e-04
[2019-05-11 11:28:35] Ep. 2 : Up. 53000 : Sen. 22,656,728 : Cost 25.87795448 : Time 419.35s : 13047.97 words/s : L.r. 1.6483e-04
[2019-05-11 11:35:35] Ep. 2 : Up. 53500 : Sen. 23,271,688 : Cost 25.84225655 : Time 419.13s : 13077.81 words/s : L.r. 1.6406e-04
[2019-05-11 11:42:35] Ep. 2 : Up. 54000 : Sen. 23,887,773 : Cost 25.90033913 : Time 420.17s : 13097.88 words/s : L.r. 1.6330e-04
[2019-05-11 11:49:34] Ep. 2 : Up. 54500 : Sen. 24,502,292 : Cost 25.84402466 : Time 418.92s : 13075.73 words/s : L.r. 1.6255e-04
[2019-05-11 11:56:29] Ep. 2 : Up. 55000 : Sen. 25,112,648 : Cost 25.85815048 : Time 415.59s : 13107.65 words/s : L.r. 1.6181e-04
[2019-05-11 11:56:29] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz.orig.npz
[2019-05-11 11:56:33] Saving model weights and runtime parameters to model/model.src1tgt0.concat.iter55000.npz
[2019-05-11 11:56:37] Saving model weights and runtime parameters to model/model.src1tgt0.concat.npz
[2019-05-11 11:56:41] Saving Adam parameters to model/model.src1tgt0.concat.npz.optimizer.npz
[2019-05-11 11:56:53] [valid] Ep. 2 : Up. 55000 : cross-entropy : 17.6864 : new best
[2019-05-11 11:56:57] [valid] Ep. 2 : Up. 55000 : perplexity : 6.32667 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-11 11:58:55] [valid] Ep. 2 : Up. 55000 : translation : 26.07 : new best
[2019-05-11 12:05:54] Ep. 2 : Up. 55500 : Sen. 25,726,400 : Cost 25.82794762 : Time 564.53s : 9692.03 words/s : L.r. 1.6108e-04
[2019-05-11 12:09:57] Error: CUDA error 77 'an illegal memory access was encountered' - /home/big_maggie/usr/marian_spider/marian_1.7.6/marian-dev/src/tensors/gpu/algorithm.cu:55: cudaStreamSynchronize(0)
[2019-05-11 12:09:57] Error: CUDA error 77 'an illegal memory access was encountered' - /home/big_maggie/usr/marian_spider/marian_1.7.6/marian-dev/src/tensors/gpu/cuda_helpers.h:45: cudaMemcpy(dest, start, (end - start) * sizeof(T), cudaMemcpyDefault)
[2019-05-11 12:09:57] Error: Aborted from void marian::gpu::fill(marian::Ptr<marian::Backend>, T*, T*, T) [with T = float; marian::Ptr<marian::Backend> = std::shared_ptr<marian::Backend>] in /home/big_maggie/usr/marian_spider/marian_1.7.6/marian-dev/src/tensors/gpu/algorithm.cu:55
Aborted from void CudaCopy(const T*, const T*, T*) [with T = const float*] in /home/big_maggie/usr/marian_spider/marian_1.7.6/marian-dev/src/tensors/gpu/cuda_helpers.h:45

[CALL STACK]
[0xa7019c]                                                            
[0xa6f36e]                                                            
[0x5cd5c3]                                                            
[0x5f6a75]                                                            
[0x5ebca1]                                                            
[0x652cef]                                                            
[0x4e857d]                                                            
[0x752c40]                                                            
[0x7f465d912230]                                                       + 0xb5230
[0x7f465e094dc5]                                                       + 0x7dc5
[0x7f465d06c76d]    clone                                              + 0x6d

train_trans.sh: line 27: 16665 Aborted                 (core dumped) $marian_home/marian --model model/model.src1tgt0.concat.npz --type transformer --train-sets corp/opensub.cs-en.docs.train.en.bpe corp/opensub.cs-en.docs.train.cs.bpe --max-length 110 --vocabs corp/vocab.encz.opensub.yml corp/vocab.encz.opensub.yml --mini-batch-fit -w 9000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.cs-en.docs.dev.en.bpe corp/opensub.cs-en.docs.dev.cs.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans2.log --valid-log model/valid_trans2.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 1 --sync-sgd --seed 1111 --no-nccl --exponential-smoothing --no-restore-corpus
