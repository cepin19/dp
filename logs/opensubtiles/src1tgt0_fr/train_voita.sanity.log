[2019-05-03 02:04:17] [marian] Marian v1.7.8 7824caf 2019-05-02 19:47:16 +0200
[2019-05-03 02:04:17] [marian] Running on bakchus.lingea.cz as process 19614 with command line:
[2019-05-03 02:04:17] [marian] /home/large/data/models/marian/marian-doc/marian_voita/doc-marian/build/marian --model model/model.voita.sanity.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.fr.bpe corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9300 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.fr.bpe corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --context-enc-depth 4 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
[2019-05-03 02:04:17] [config] after-batches: 0
[2019-05-03 02:04:17] [config] after-epochs: 0
[2019-05-03 02:04:17] [config] allow-unk: false
[2019-05-03 02:04:17] [config] beam-size: 6
[2019-05-03 02:04:17] [config] bert-class-symbol: "[CLS]"
[2019-05-03 02:04:17] [config] bert-mask-symbol: "[MASK]"
[2019-05-03 02:04:17] [config] bert-masking-fraction: 0.15
[2019-05-03 02:04:17] [config] bert-sep-symbol: "[SEP]"
[2019-05-03 02:04:17] [config] bert-train-type-embeddings: true
[2019-05-03 02:04:17] [config] bert-type-vocab-size: 2
[2019-05-03 02:04:17] [config] best-deep: false
[2019-05-03 02:04:17] [config] clip-gemm: 0
[2019-05-03 02:04:17] [config] clip-norm: 5
[2019-05-03 02:04:17] [config] context-enc-depth: 4
[2019-05-03 02:04:17] [config] cost-type: ce-mean
[2019-05-03 02:04:17] [config] cpu-threads: 0
[2019-05-03 02:04:17] [config] data-weighting: ""
[2019-05-03 02:04:17] [config] data-weighting-type: sentence
[2019-05-03 02:04:17] [config] dec-cell: gru
[2019-05-03 02:04:17] [config] dec-cell-base-depth: 2
[2019-05-03 02:04:17] [config] dec-cell-high-depth: 1
[2019-05-03 02:04:17] [config] dec-depth: 6
[2019-05-03 02:04:17] [config] devices:
[2019-05-03 02:04:17] [config]   - 0
[2019-05-03 02:04:17] [config] dim-emb: 512
[2019-05-03 02:04:17] [config] dim-rnn: 1024
[2019-05-03 02:04:17] [config] dim-vocabs:
[2019-05-03 02:04:17] [config]   - 30000
[2019-05-03 02:04:17] [config]   - 30000
[2019-05-03 02:04:17] [config] disp-first: 0
[2019-05-03 02:04:17] [config] disp-freq: 500
[2019-05-03 02:04:17] [config] disp-label-counts: false
[2019-05-03 02:04:17] [config] dropout-rnn: 0
[2019-05-03 02:04:17] [config] dropout-src: 0
[2019-05-03 02:04:17] [config] dropout-trg: 0
[2019-05-03 02:04:17] [config] dump-config: ""
[2019-05-03 02:04:17] [config] early-stopping: 10
[2019-05-03 02:04:17] [config] embedding-fix-src: false
[2019-05-03 02:04:17] [config] embedding-fix-trg: false
[2019-05-03 02:04:17] [config] embedding-normalization: false
[2019-05-03 02:04:17] [config] embedding-vectors:
[2019-05-03 02:04:17] [config]   []
[2019-05-03 02:04:17] [config] enc-cell: gru
[2019-05-03 02:04:17] [config] enc-cell-depth: 1
[2019-05-03 02:04:17] [config] enc-depth: 6
[2019-05-03 02:04:17] [config] enc-type: bidirectional
[2019-05-03 02:04:17] [config] exponential-smoothing: 0.0001
[2019-05-03 02:04:17] [config] freeze: false
[2019-05-03 02:04:17] [config] grad-dropping-momentum: 0
[2019-05-03 02:04:17] [config] grad-dropping-rate: 0
[2019-05-03 02:04:17] [config] grad-dropping-warmup: 100
[2019-05-03 02:04:17] [config] guided-alignment: none
[2019-05-03 02:04:17] [config] guided-alignment-cost: mse
[2019-05-03 02:04:17] [config] guided-alignment-weight: 0.1
[2019-05-03 02:04:17] [config] hier-att: false
[2019-05-03 02:04:17] [config] ignore-model-config: false
[2019-05-03 02:04:17] [config] input-types:
[2019-05-03 02:04:17] [config]   []
[2019-05-03 02:04:17] [config] interpolate-env-vars: false
[2019-05-03 02:04:17] [config] keep-best: false
[2019-05-03 02:04:17] [config] label-smoothing: 0.1
[2019-05-03 02:04:17] [config] layer-normalization: false
[2019-05-03 02:04:17] [config] learn-rate: 0.0002
[2019-05-03 02:04:17] [config] log: model/train_trans.gate.log
[2019-05-03 02:04:17] [config] log-level: info
[2019-05-03 02:04:17] [config] log-time-zone: ""
[2019-05-03 02:04:17] [config] lr-decay: 0
[2019-05-03 02:04:17] [config] lr-decay-freq: 50000
[2019-05-03 02:04:17] [config] lr-decay-inv-sqrt:
[2019-05-03 02:04:17] [config]   - 16000
[2019-05-03 02:04:17] [config] lr-decay-repeat-warmup: false
[2019-05-03 02:04:17] [config] lr-decay-reset-optimizer: false
[2019-05-03 02:04:17] [config] lr-decay-start:
[2019-05-03 02:04:17] [config]   - 10
[2019-05-03 02:04:17] [config]   - 1
[2019-05-03 02:04:17] [config] lr-decay-strategy: epoch+stalled
[2019-05-03 02:04:17] [config] lr-report: true
[2019-05-03 02:04:17] [config] lr-warmup: 16000
[2019-05-03 02:04:17] [config] lr-warmup-at-reload: false
[2019-05-03 02:04:17] [config] lr-warmup-cycle: false
[2019-05-03 02:04:17] [config] lr-warmup-start-rate: 0
[2019-05-03 02:04:17] [config] max-length: 55
[2019-05-03 02:04:17] [config] max-length-crop: false
[2019-05-03 02:04:17] [config] max-length-factor: 3
[2019-05-03 02:04:17] [config] maxi-batch: 1000
[2019-05-03 02:04:17] [config] maxi-batch-sort: trg
[2019-05-03 02:04:17] [config] mini-batch: 1000
[2019-05-03 02:04:17] [config] mini-batch-fit: true
[2019-05-03 02:04:17] [config] mini-batch-fit-step: 10
[2019-05-03 02:04:17] [config] mini-batch-overstuff: 1
[2019-05-03 02:04:17] [config] mini-batch-track-lr: false
[2019-05-03 02:04:17] [config] mini-batch-understuff: 1
[2019-05-03 02:04:17] [config] mini-batch-warmup: 0
[2019-05-03 02:04:17] [config] mini-batch-words: 0
[2019-05-03 02:04:17] [config] mini-batch-words-ref: 0
[2019-05-03 02:04:17] [config] model: model/model.voita.sanity.npz
[2019-05-03 02:04:17] [config] multi-loss-type: sum
[2019-05-03 02:04:17] [config] multi-node: false
[2019-05-03 02:04:17] [config] multi-node-overlap: true
[2019-05-03 02:04:17] [config] n-best: false
[2019-05-03 02:04:17] [config] no-nccl: false
[2019-05-03 02:04:17] [config] no-reload: false
[2019-05-03 02:04:17] [config] no-restore-corpus: true
[2019-05-03 02:04:17] [config] no-shuffle: false
[2019-05-03 02:04:17] [config] normalize: 0.6
[2019-05-03 02:04:17] [config] num-devices: 0
[2019-05-03 02:04:17] [config] optimizer: adam
[2019-05-03 02:04:17] [config] optimizer-delay: 4
[2019-05-03 02:04:17] [config] optimizer-params:
[2019-05-03 02:04:17] [config]   - 0.9
[2019-05-03 02:04:17] [config]   - 0.98
[2019-05-03 02:04:17] [config]   - 1e-09
[2019-05-03 02:04:17] [config] overwrite: false
[2019-05-03 02:04:17] [config] pretrained-model: ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 02:04:17] [config] quiet: false
[2019-05-03 02:04:17] [config] quiet-translation: true
[2019-05-03 02:04:17] [config] relative-paths: false
[2019-05-03 02:04:17] [config] right-left: false
[2019-05-03 02:04:17] [config] save-freq: 5000
[2019-05-03 02:04:17] [config] seed: 1111
[2019-05-03 02:04:17] [config] shuffle-in-ram: false
[2019-05-03 02:04:17] [config] skip: false
[2019-05-03 02:04:17] [config] sqlite: ""
[2019-05-03 02:04:17] [config] sqlite-drop: false
[2019-05-03 02:04:17] [config] sync-sgd: true
[2019-05-03 02:04:17] [config] tempdir: /tmp
[2019-05-03 02:04:17] [config] tied-embeddings: false
[2019-05-03 02:04:17] [config] tied-embeddings-all: true
[2019-05-03 02:04:17] [config] tied-embeddings-src: false
[2019-05-03 02:04:17] [config] train-sets:
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.train.fr.bpe
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.train.fr.bpe
[2019-05-03 02:04:17] [config] transformer-aan-activation: swish
[2019-05-03 02:04:17] [config] transformer-aan-depth: 2
[2019-05-03 02:04:17] [config] transformer-aan-nogate: false
[2019-05-03 02:04:17] [config] transformer-decoder-autoreg: self-attention
[2019-05-03 02:04:17] [config] transformer-dim-aan: 2048
[2019-05-03 02:04:17] [config] transformer-dim-ffn: 2048
[2019-05-03 02:04:17] [config] transformer-dropout: 0.1
[2019-05-03 02:04:17] [config] transformer-dropout-attention: 0
[2019-05-03 02:04:17] [config] transformer-dropout-ffn: 0
[2019-05-03 02:04:17] [config] transformer-ffn-activation: swish
[2019-05-03 02:04:17] [config] transformer-ffn-depth: 2
[2019-05-03 02:04:17] [config] transformer-guided-alignment-layer: last
[2019-05-03 02:04:17] [config] transformer-heads: 8
[2019-05-03 02:04:17] [config] transformer-no-projection: false
[2019-05-03 02:04:17] [config] transformer-postprocess: dan
[2019-05-03 02:04:17] [config] transformer-postprocess-emb: d
[2019-05-03 02:04:17] [config] transformer-preprocess: ""
[2019-05-03 02:04:17] [config] transformer-tied-layers:
[2019-05-03 02:04:17] [config]   []
[2019-05-03 02:04:17] [config] transformer-train-position-embeddings: false
[2019-05-03 02:04:17] [config] type: transformer-context
[2019-05-03 02:04:17] [config] ulr: false
[2019-05-03 02:04:17] [config] ulr-dim-emb: 0
[2019-05-03 02:04:17] [config] ulr-dropout: 0
[2019-05-03 02:04:17] [config] ulr-keys-vectors: ""
[2019-05-03 02:04:17] [config] ulr-query-vectors: ""
[2019-05-03 02:04:17] [config] ulr-softmax-temperature: 1
[2019-05-03 02:04:17] [config] ulr-trainable-transformation: false
[2019-05-03 02:04:17] [config] valid-freq: 5000
[2019-05-03 02:04:17] [config] valid-log: model/valid_trans.gate.log
[2019-05-03 02:04:17] [config] valid-max-length: 1000
[2019-05-03 02:04:17] [config] valid-metrics:
[2019-05-03 02:04:17] [config]   - cross-entropy
[2019-05-03 02:04:17] [config]   - perplexity
[2019-05-03 02:04:17] [config]   - translation
[2019-05-03 02:04:17] [config] valid-mini-batch: 64
[2019-05-03 02:04:17] [config] valid-script-path: ./val.sh
[2019-05-03 02:04:17] [config] valid-sets:
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.dev.fr.bpe
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src
[2019-05-03 02:04:17] [config]   - corp/opensub.en-fr.docs.dev.fr.bpe
[2019-05-03 02:04:17] [config] valid-translation-output: data/valid.bpe.en.output
[2019-05-03 02:04:17] [config] vocabs:
[2019-05-03 02:04:17] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:17] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:17] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:17] [config] word-penalty: 0
[2019-05-03 02:04:17] [config] workspace: 9300
[2019-05-03 02:04:17] [config] Model is being created with Marian v1.7.8 7824caf 2019-05-02 19:47:16 +0200
[2019-05-03 02:04:17] Using synchronous training
[2019-05-03 02:04:17] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:18] [data] Setting vocabulary size for input 0 to 30000
[2019-05-03 02:04:18] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:18] [data] Setting vocabulary size for input 1 to 30000
[2019-05-03 02:04:18] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 02:04:18] [data] Setting vocabulary size for input 2 to 30000
[2019-05-03 02:04:18] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-03 02:04:18] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-03 02:04:21] [memory] Extending reserved space to 9344 MB (device gpu0)
[2019-05-03 02:04:22] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-03 02:04:22] [comm] NCCLCommunicator constructed successfully.
[2019-05-03 02:04:22] [training] Using 1 GPUs
[2019-05-03 02:04:22] [memory] Reserving 235 MB, device gpu0
[2019-05-03 02:04:22] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-05-03 02:04:22] [memory] Reserving 235 MB, device gpu0
[2019-05-03 02:04:34] [batching] Done. Typical MB size is 21840 target words
[2019-05-03 02:04:34] [memory] Extending reserved space to 9344 MB (device gpu0)
[2019-05-03 02:04:34] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-03 02:04:34] [comm] NCCLCommunicator constructed successfully.
[2019-05-03 02:04:34] [training] Using 1 GPUs
[2019-05-03 02:04:34] [training] Initializing model weights with the pre-trained model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 02:04:34] Loading model from ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 02:04:36] Training started
[2019-05-03 02:04:36] [data] Shuffling data
[2019-05-03 02:05:07] [data] Done reading 41736982 sentences
[2019-05-03 02:08:32] [data] Done shuffling 41736982 sentences to temp files
[2019-05-03 02:09:08] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-05-03 02:09:08] [memory] Reserving 247 MB, device gpu0
[2019-05-03 02:09:09] [memory] Reserving 247 MB, device gpu0
[2019-05-03 02:09:09] [memory] Reserving 247 MB, device gpu0
[2019-05-03 02:09:11] [memory] Reserving 494 MB, device gpu0
[2019-05-03 02:21:03] Ep. 1 : Up. 500 : Sen. 750,599 : Cost 76.23646545 : Time 1004.39s : 6926.30 words/s : L.r. 6.2500e-06
[2019-05-03 02:32:53] Ep. 1 : Up. 1000 : Sen. 1,466,494 : Cost 48.80900574 : Time 710.10s : 9618.62 words/s : L.r. 1.2500e-05
[2019-05-03 02:44:56] Ep. 1 : Up. 1500 : Sen. 2,200,413 : Cost 33.65763092 : Time 723.77s : 9756.59 words/s : L.r. 1.8750e-05
[2019-05-03 02:57:02] Ep. 1 : Up. 2000 : Sen. 2,941,338 : Cost 23.19271660 : Time 725.40s : 9633.49 words/s : L.r. 2.5000e-05
[2019-05-03 03:08:57] Ep. 1 : Up. 2500 : Sen. 3,681,785 : Cost 20.01018524 : Time 715.27s : 9643.49 words/s : L.r. 3.1250e-05
[2019-05-03 03:21:23] Ep. 1 : Up. 3000 : Sen. 4,422,883 : Cost 18.18232346 : Time 745.47s : 9488.48 words/s : L.r. 3.7500e-05
[2019-05-03 03:33:24] Ep. 1 : Up. 3500 : Sen. 5,160,033 : Cost 16.21049881 : Time 721.55s : 9638.38 words/s : L.r. 4.3750e-05
[2019-05-03 03:45:27] Ep. 1 : Up. 4000 : Sen. 5,892,253 : Cost 15.15246010 : Time 722.72s : 9635.00 words/s : L.r. 5.0000e-05
[2019-05-03 03:57:23] Ep. 1 : Up. 4500 : Sen. 6,620,317 : Cost 14.21759510 : Time 715.94s : 9581.06 words/s : L.r. 5.6250e-05
[2019-05-03 04:09:27] Ep. 1 : Up. 5000 : Sen. 7,358,673 : Cost 13.83490372 : Time 723.95s : 9614.64 words/s : L.r. 6.2500e-05
[2019-05-03 04:09:27] Saving model weights and runtime parameters to model/model.voita.sanity.npz.orig.npz
[2019-05-03 04:09:35] Saving model weights and runtime parameters to model/model.voita.sanity.iter5000.npz
[2019-05-03 04:09:38] Saving model weights and runtime parameters to model/model.voita.sanity.npz
[2019-05-03 04:09:43] Saving Adam parameters to model/model.voita.sanity.npz.optimizer.npz
[2019-05-03 04:10:20] [valid] Ep. 1 : Up. 5000 : cross-entropy : 2.20439 : new best
[2019-05-03 04:10:24] [valid] Ep. 1 : Up. 5000 : perplexity : 1.18653 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 04:11:38] [valid] Ep. 1 : Up. 5000 : translation : 93.48 : new best
[2019-05-03 04:23:47] Ep. 1 : Up. 5500 : Sen. 8,094,460 : Cost 13.85728931 : Time 860.44s : 8160.12 words/s : L.r. 6.8750e-05
[2019-05-03 04:35:48] Ep. 1 : Up. 6000 : Sen. 8,842,947 : Cost 13.30071735 : Time 720.42s : 9612.15 words/s : L.r. 7.5000e-05
[2019-05-03 04:47:53] Ep. 1 : Up. 6500 : Sen. 9,552,807 : Cost 14.07668304 : Time 725.06s : 9627.07 words/s : L.r. 8.1250e-05
[2019-05-03 04:59:57] Ep. 1 : Up. 7000 : Sen. 10,297,051 : Cost 13.34710789 : Time 723.83s : 9639.11 words/s : L.r. 8.7500e-05
[2019-05-03 05:12:02] Ep. 1 : Up. 7500 : Sen. 11,039,556 : Cost 13.30093384 : Time 725.09s : 9604.68 words/s : L.r. 9.3750e-05
[2019-05-03 05:24:07] Ep. 1 : Up. 8000 : Sen. 11,773,461 : Cost 13.48396015 : Time 725.08s : 9622.73 words/s : L.r. 1.0000e-04
[2019-05-03 05:36:11] Ep. 1 : Up. 8500 : Sen. 12,506,585 : Cost 13.45301628 : Time 724.24s : 9623.39 words/s : L.r. 1.0625e-04
[2019-05-03 05:48:16] Ep. 1 : Up. 9000 : Sen. 13,238,157 : Cost 13.47695923 : Time 725.34s : 9648.20 words/s : L.r. 1.1250e-04
[2019-05-03 06:00:12] Ep. 1 : Up. 9500 : Sen. 13,981,492 : Cost 13.03005600 : Time 716.06s : 9598.12 words/s : L.r. 1.1875e-04
[2019-05-03 06:12:18] Ep. 1 : Up. 10000 : Sen. 14,712,829 : Cost 13.47061062 : Time 725.44s : 9646.63 words/s : L.r. 1.2500e-04
[2019-05-03 06:12:18] Saving model weights and runtime parameters to model/model.voita.sanity.npz.orig.npz
[2019-05-03 06:12:22] Saving model weights and runtime parameters to model/model.voita.sanity.iter10000.npz
[2019-05-03 06:12:26] Saving model weights and runtime parameters to model/model.voita.sanity.npz
[2019-05-03 06:12:31] Saving Adam parameters to model/model.voita.sanity.npz.optimizer.npz
[2019-05-03 06:12:44] [valid] Ep. 1 : Up. 10000 : cross-entropy : 1.41717 : new best
[2019-05-03 06:12:49] [valid] Ep. 1 : Up. 10000 : perplexity : 1.11623 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 06:13:58] [valid] Ep. 1 : Up. 10000 : translation : 97.12 : new best
[2019-05-03 06:26:00] Ep. 1 : Up. 10500 : Sen. 15,454,246 : Cost 13.14488506 : Time 822.11s : 8420.47 words/s : L.r. 1.3125e-04
[2019-05-03 06:38:04] Ep. 1 : Up. 11000 : Sen. 16,192,620 : Cost 13.29705048 : Time 724.43s : 9657.67 words/s : L.r. 1.3750e-04
[2019-05-03 06:50:05] Ep. 1 : Up. 11500 : Sen. 16,925,843 : Cost 13.28802299 : Time 721.00s : 9597.49 words/s : L.r. 1.4375e-04
[2019-05-03 07:02:11] Ep. 1 : Up. 12000 : Sen. 17,671,342 : Cost 13.16589832 : Time 725.82s : 9639.63 words/s : L.r. 1.5000e-04
[2019-05-03 07:14:22] Ep. 1 : Up. 12500 : Sen. 18,403,562 : Cost 13.52140236 : Time 730.40s : 9630.83 words/s : L.r. 1.5625e-04
[2019-05-03 07:26:19] Ep. 1 : Up. 13000 : Sen. 19,150,766 : Cost 12.86988163 : Time 717.23s : 9557.78 words/s : L.r. 1.6250e-04
[2019-05-03 07:38:19] Ep. 1 : Up. 13500 : Sen. 19,872,875 : Cost 13.49041462 : Time 719.69s : 9637.47 words/s : L.r. 1.6875e-04
[2019-05-03 07:50:21] Ep. 1 : Up. 14000 : Sen. 20,634,670 : Cost 12.71075249 : Time 722.81s : 9580.86 words/s : L.r. 1.7500e-04
[2019-05-03 08:02:32] Ep. 1 : Up. 14500 : Sen. 21,336,304 : Cost 14.10883141 : Time 730.23s : 9655.33 words/s : L.r. 1.8125e-04
[2019-05-03 08:14:35] Ep. 1 : Up. 15000 : Sen. 22,080,968 : Cost 13.12242699 : Time 723.48s : 9606.72 words/s : L.r. 1.8750e-04
[2019-05-03 08:14:35] Saving model weights and runtime parameters to model/model.voita.sanity.npz.orig.npz
[2019-05-03 08:14:40] Saving model weights and runtime parameters to model/model.voita.sanity.iter15000.npz
[2019-05-03 08:14:43] Saving model weights and runtime parameters to model/model.voita.sanity.npz
[2019-05-03 08:14:48] Saving Adam parameters to model/model.voita.sanity.npz.optimizer.npz
[2019-05-03 08:15:01] [valid] Ep. 1 : Up. 15000 : cross-entropy : 1.40102 : new best
[2019-05-03 08:15:06] [valid] Ep. 1 : Up. 15000 : perplexity : 1.11483 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 08:16:15] [valid] Ep. 1 : Up. 15000 : translation : 97.44 : new best
[2019-05-03 08:28:17] Ep. 1 : Up. 15500 : Sen. 22,805,444 : Cost 13.44722271 : Time 822.30s : 8443.69 words/s : L.r. 1.9375e-04
[2019-05-03 08:40:25] Ep. 1 : Up. 16000 : Sen. 23,548,296 : Cost 13.22424698 : Time 727.53s : 9608.34 words/s : L.r. 2.0000e-04
[2019-05-03 08:52:31] Ep. 1 : Up. 16500 : Sen. 24,283,846 : Cost 13.28023815 : Time 726.48s : 9615.58 words/s : L.r. 1.9695e-04
[2019-05-03 09:04:30] Ep. 1 : Up. 17000 : Sen. 25,028,349 : Cost 12.98681927 : Time 718.62s : 9620.75 words/s : L.r. 1.9403e-04
[2019-05-03 09:16:29] Ep. 1 : Up. 17500 : Sen. 25,767,866 : Cost 13.09215641 : Time 718.58s : 9626.22 words/s : L.r. 1.9124e-04
[2019-05-03 09:28:28] Ep. 1 : Up. 18000 : Sen. 26,489,226 : Cost 13.29401684 : Time 719.66s : 9560.27 words/s : L.r. 1.8856e-04
[2019-05-03 09:40:33] Ep. 1 : Up. 18500 : Sen. 27,227,750 : Cost 13.18547535 : Time 724.93s : 9622.21 words/s : L.r. 1.8600e-04
[2019-05-03 09:52:47] Ep. 1 : Up. 19000 : Sen. 27,965,792 : Cost 13.38236809 : Time 733.92s : 9653.04 words/s : L.r. 1.8353e-04
[2019-05-03 10:04:52] Ep. 1 : Up. 19500 : Sen. 28,697,566 : Cost 13.24794579 : Time 724.39s : 9616.89 words/s : L.r. 1.8116e-04
[2019-05-03 10:16:56] Ep. 1 : Up. 20000 : Sen. 29,444,443 : Cost 12.99075603 : Time 724.66s : 9598.32 words/s : L.r. 1.7889e-04
[2019-05-03 10:16:56] Saving model weights and runtime parameters to model/model.voita.sanity.npz.orig.npz
[2019-05-03 10:17:00] Saving model weights and runtime parameters to model/model.voita.sanity.iter20000.npz
[2019-05-03 10:17:04] Saving model weights and runtime parameters to model/model.voita.sanity.npz
[2019-05-03 10:17:09] Saving Adam parameters to model/model.voita.sanity.npz.optimizer.npz
[2019-05-03 10:17:21] [valid] Ep. 1 : Up. 20000 : cross-entropy : 1.44136 : stalled 1 times (last best: 1.40102)
[2019-05-03 10:17:26] [valid] Ep. 1 : Up. 20000 : perplexity : 1.11832 : stalled 1 times (last best: 1.11483)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 10:18:35] [valid] Ep. 1 : Up. 20000 : translation : 97.53 : new best
[2019-05-03 10:30:36] Ep. 1 : Up. 20500 : Sen. 30,180,210 : Cost 13.04907036 : Time 819.43s : 8431.91 words/s : L.r. 1.7669e-04
[2019-05-03 10:42:40] Ep. 1 : Up. 21000 : Sen. 30,920,958 : Cost 13.09422779 : Time 724.73s : 9644.76 words/s : L.r. 1.7457e-04
[2019-05-03 10:54:48] Ep. 1 : Up. 21500 : Sen. 31,654,683 : Cost 13.33710289 : Time 727.54s : 9675.57 words/s : L.r. 1.7253e-04
[2019-05-03 11:06:55] Ep. 1 : Up. 22000 : Sen. 32,393,343 : Cost 13.17086792 : Time 727.42s : 9646.94 words/s : L.r. 1.7056e-04
[2019-05-03 11:18:43] Ep. 1 : Up. 22500 : Sen. 33,118,837 : Cost 12.92353439 : Time 707.93s : 9542.47 words/s : L.r. 1.6865e-04
[2019-05-03 11:30:50] Ep. 1 : Up. 23000 : Sen. 33,857,709 : Cost 13.13528824 : Time 726.46s : 9635.18 words/s : L.r. 1.6681e-04
train_voita.sanity.sh: line 30: 19614 Terminated              $marian_home/marian --model model/model.voita.sanity.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.fr.bpe corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9300 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.fr.bpe corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --context-enc-depth 4 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
