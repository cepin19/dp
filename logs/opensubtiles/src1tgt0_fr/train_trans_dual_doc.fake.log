[2019-02-21 23:29:54] [marian] Marian v1.7.8 63e1cfe4 2019-02-11 21:04:00 -0800
[2019-02-21 23:29:54] [marian] Running on spider3.lingea.cz as process 26269 with command line:
[2019-02-21 23:29:54] [marian] /home/large/data/models/marian/marian-doc/marian-dev/build//marian --model model/model.src1tgt0.dual.doc_fake.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.newvocab.log --valid-log model/valid_trans.newvocab.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 1 --sync-sgd --seed 1111 --no-nccl --exponential-smoothing --no-restore-corpus
[2019-02-21 23:29:54] [config] after-batches: 0
[2019-02-21 23:29:54] [config] after-epochs: 0
[2019-02-21 23:29:54] [config] allow-unk: false
[2019-02-21 23:29:54] [config] beam-size: 6
[2019-02-21 23:29:54] [config] bert-class-symbol: "[CLS]"
[2019-02-21 23:29:54] [config] bert-mask-symbol: "[MASK]"
[2019-02-21 23:29:54] [config] bert-masking-fraction: 0.15
[2019-02-21 23:29:54] [config] bert-sep-symbol: "[SEP]"
[2019-02-21 23:29:54] [config] bert-train-type-embeddings: true
[2019-02-21 23:29:54] [config] bert-type-vocab-size: 2
[2019-02-21 23:29:54] [config] best-deep: false
[2019-02-21 23:29:54] [config] clip-gemm: 0
[2019-02-21 23:29:54] [config] clip-norm: 5
[2019-02-21 23:29:54] [config] cost-type: ce-mean
[2019-02-21 23:29:54] [config] cpu-threads: 0
[2019-02-21 23:29:54] [config] data-weighting: ""
[2019-02-21 23:29:54] [config] data-weighting-type: sentence
[2019-02-21 23:29:54] [config] dec-cell: gru
[2019-02-21 23:29:54] [config] dec-cell-base-depth: 2
[2019-02-21 23:29:54] [config] dec-cell-high-depth: 1
[2019-02-21 23:29:54] [config] dec-depth: 6
[2019-02-21 23:29:54] [config] devices:
[2019-02-21 23:29:54] [config]   - 0
[2019-02-21 23:29:54] [config]   - 1
[2019-02-21 23:29:54] [config] dim-emb: 512
[2019-02-21 23:29:54] [config] dim-rnn: 1024
[2019-02-21 23:29:54] [config] dim-vocabs:
[2019-02-21 23:29:54] [config]   - 30000
[2019-02-21 23:29:54] [config]   - 30000
[2019-02-21 23:29:54] [config] disp-first: 0
[2019-02-21 23:29:54] [config] disp-freq: 500
[2019-02-21 23:29:54] [config] disp-label-counts: false
[2019-02-21 23:29:54] [config] dropout-rnn: 0
[2019-02-21 23:29:54] [config] dropout-src: 0
[2019-02-21 23:29:54] [config] dropout-trg: 0
[2019-02-21 23:29:54] [config] dump-config: ""
[2019-02-21 23:29:54] [config] early-stopping: 10
[2019-02-21 23:29:54] [config] embedding-fix-src: false
[2019-02-21 23:29:54] [config] embedding-fix-trg: false
[2019-02-21 23:29:54] [config] embedding-normalization: false
[2019-02-21 23:29:54] [config] embedding-vectors:
[2019-02-21 23:29:54] [config]   []
[2019-02-21 23:29:54] [config] enc-cell: gru
[2019-02-21 23:29:54] [config] enc-cell-depth: 1
[2019-02-21 23:29:54] [config] enc-depth: 6
[2019-02-21 23:29:54] [config] enc-type: bidirectional
[2019-02-21 23:29:54] [config] exponential-smoothing: 0.0001
[2019-02-21 23:29:54] [config] grad-dropping-momentum: 0
[2019-02-21 23:29:54] [config] grad-dropping-rate: 0
[2019-02-21 23:29:54] [config] grad-dropping-warmup: 100
[2019-02-21 23:29:54] [config] guided-alignment: none
[2019-02-21 23:29:54] [config] guided-alignment-cost: mse
[2019-02-21 23:29:54] [config] guided-alignment-weight: 0.1
[2019-02-21 23:29:54] [config] ignore-model-config: false
[2019-02-21 23:29:54] [config] input-types:
[2019-02-21 23:29:54] [config]   []
[2019-02-21 23:29:54] [config] interpolate-env-vars: false
[2019-02-21 23:29:54] [config] keep-best: false
[2019-02-21 23:29:54] [config] label-smoothing: 0.1
[2019-02-21 23:29:54] [config] layer-normalization: false
[2019-02-21 23:29:54] [config] learn-rate: 0.0001
[2019-02-21 23:29:54] [config] log: model/train_trans.newvocab.log
[2019-02-21 23:29:54] [config] log-level: info
[2019-02-21 23:29:54] [config] log-time-zone: ""
[2019-02-21 23:29:54] [config] lr-decay: 0
[2019-02-21 23:29:54] [config] lr-decay-freq: 50000
[2019-02-21 23:29:54] [config] lr-decay-inv-sqrt:
[2019-02-21 23:29:54] [config]   - 16000
[2019-02-21 23:29:54] [config] lr-decay-repeat-warmup: false
[2019-02-21 23:29:54] [config] lr-decay-reset-optimizer: false
[2019-02-21 23:29:54] [config] lr-decay-start:
[2019-02-21 23:29:54] [config]   - 10
[2019-02-21 23:29:54] [config]   - 1
[2019-02-21 23:29:54] [config] lr-decay-strategy: epoch+stalled
[2019-02-21 23:29:54] [config] lr-report: true
[2019-02-21 23:29:54] [config] lr-warmup: 16000
[2019-02-21 23:29:54] [config] lr-warmup-at-reload: false
[2019-02-21 23:29:54] [config] lr-warmup-cycle: false
[2019-02-21 23:29:54] [config] lr-warmup-start-rate: 0
[2019-02-21 23:29:54] [config] max-length: 55
[2019-02-21 23:29:54] [config] max-length-crop: false
[2019-02-21 23:29:54] [config] max-length-factor: 3
[2019-02-21 23:29:54] [config] maxi-batch: 1000
[2019-02-21 23:29:54] [config] maxi-batch-sort: trg
[2019-02-21 23:29:54] [config] mini-batch: 1000
[2019-02-21 23:29:54] [config] mini-batch-fit: true
[2019-02-21 23:29:54] [config] mini-batch-fit-step: 10
[2019-02-21 23:29:54] [config] mini-batch-overstuff: 1
[2019-02-21 23:29:54] [config] mini-batch-track-lr: false
[2019-02-21 23:29:54] [config] mini-batch-understuff: 1
[2019-02-21 23:29:54] [config] mini-batch-warmup: 0
[2019-02-21 23:29:54] [config] mini-batch-words: 0
[2019-02-21 23:29:54] [config] mini-batch-words-ref: 0
[2019-02-21 23:29:54] [config] model: model/model.src1tgt0.dual.doc_fake.npz
[2019-02-21 23:29:54] [config] multi-loss-type: sum
[2019-02-21 23:29:54] [config] multi-node: false
[2019-02-21 23:29:54] [config] multi-node-overlap: true
[2019-02-21 23:29:54] [config] n-best: false
[2019-02-21 23:29:54] [config] no-nccl: true
[2019-02-21 23:29:54] [config] no-reload: false
[2019-02-21 23:29:54] [config] no-restore-corpus: true
[2019-02-21 23:29:54] [config] no-shuffle: false
[2019-02-21 23:29:54] [config] normalize: 0.6
[2019-02-21 23:29:54] [config] num-devices: 0
[2019-02-21 23:29:54] [config] optimizer: adam
[2019-02-21 23:29:54] [config] optimizer-delay: 4
[2019-02-21 23:29:54] [config] optimizer-params:
[2019-02-21 23:29:54] [config]   - 0.9
[2019-02-21 23:29:54] [config]   - 0.98
[2019-02-21 23:29:54] [config]   - 1e-09
[2019-02-21 23:29:54] [config] overwrite: false
[2019-02-21 23:29:54] [config] pretrained-model: ""
[2019-02-21 23:29:54] [config] quiet: false
[2019-02-21 23:29:54] [config] quiet-translation: true
[2019-02-21 23:29:54] [config] relative-paths: false
[2019-02-21 23:29:54] [config] right-left: false
[2019-02-21 23:29:54] [config] save-freq: 5000
[2019-02-21 23:29:54] [config] seed: 1111
[2019-02-21 23:29:54] [config] shuffle-in-ram: false
[2019-02-21 23:29:54] [config] skip: false
[2019-02-21 23:29:54] [config] sqlite: ""
[2019-02-21 23:29:54] [config] sqlite-drop: false
[2019-02-21 23:29:54] [config] sync-sgd: true
[2019-02-21 23:29:54] [config] tempdir: /tmp
[2019-02-21 23:29:54] [config] tied-embeddings: false
[2019-02-21 23:29:54] [config] tied-embeddings-all: true
[2019-02-21 23:29:54] [config] tied-embeddings-src: false
[2019-02-21 23:29:54] [config] train-sets:
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.train.fr.bpe
[2019-02-21 23:29:54] [config] transformer-aan-activation: swish
[2019-02-21 23:29:54] [config] transformer-aan-depth: 2
[2019-02-21 23:29:54] [config] transformer-aan-nogate: false
[2019-02-21 23:29:54] [config] transformer-decoder-autoreg: self-attention
[2019-02-21 23:29:54] [config] transformer-dim-aan: 2048
[2019-02-21 23:29:54] [config] transformer-dim-ffn: 2048
[2019-02-21 23:29:54] [config] transformer-dropout: 0.1
[2019-02-21 23:29:54] [config] transformer-dropout-attention: 0
[2019-02-21 23:29:54] [config] transformer-dropout-ffn: 0
[2019-02-21 23:29:54] [config] transformer-ffn-activation: swish
[2019-02-21 23:29:54] [config] transformer-ffn-depth: 2
[2019-02-21 23:29:54] [config] transformer-guided-alignment-layer: last
[2019-02-21 23:29:54] [config] transformer-heads: 8
[2019-02-21 23:29:54] [config] transformer-no-projection: false
[2019-02-21 23:29:54] [config] transformer-postprocess: dan
[2019-02-21 23:29:54] [config] transformer-postprocess-emb: d
[2019-02-21 23:29:54] [config] transformer-preprocess: ""
[2019-02-21 23:29:54] [config] transformer-tied-layers:
[2019-02-21 23:29:54] [config]   []
[2019-02-21 23:29:54] [config] transformer-train-position-embeddings: false
[2019-02-21 23:29:54] [config] type: transformer-context
[2019-02-21 23:29:54] [config] ulr: false
[2019-02-21 23:29:54] [config] ulr-dim-emb: 0
[2019-02-21 23:29:54] [config] ulr-dropout: 0
[2019-02-21 23:29:54] [config] ulr-keys-vectors: ""
[2019-02-21 23:29:54] [config] ulr-query-vectors: ""
[2019-02-21 23:29:54] [config] ulr-softmax-temperature: 1
[2019-02-21 23:29:54] [config] ulr-trainable-transformation: false
[2019-02-21 23:29:54] [config] valid-freq: 5000
[2019-02-21 23:29:54] [config] valid-log: model/valid_trans.newvocab.log
[2019-02-21 23:29:54] [config] valid-max-length: 1000
[2019-02-21 23:29:54] [config] valid-metrics:
[2019-02-21 23:29:54] [config]   - cross-entropy
[2019-02-21 23:29:54] [config]   - perplexity
[2019-02-21 23:29:54] [config]   - translation
[2019-02-21 23:29:54] [config] valid-mini-batch: 64
[2019-02-21 23:29:54] [config] valid-script-path: ./val.sh
[2019-02-21 23:29:54] [config] valid-sets:
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src
[2019-02-21 23:29:54] [config]   - corp/opensub.en-fr.docs.dev.fr.bpe
[2019-02-21 23:29:54] [config] valid-translation-output: data/valid.bpe.en.output
[2019-02-21 23:29:54] [config] vocabs:
[2019-02-21 23:29:54] [config]   - corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:54] [config]   - corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:54] [config]   - corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:54] [config] word-penalty: 0
[2019-02-21 23:29:54] [config] workspace: 9000
[2019-02-21 23:29:54] [config] Model is being created with Marian v1.7.8 63e1cfe4 2019-02-11 21:04:00 -0800
[2019-02-21 23:29:54] Using synchronous training
[2019-02-21 23:29:54] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:55] [data] Setting vocabulary size for input 0 to 30000
[2019-02-21 23:29:55] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:55] [data] Setting vocabulary size for input 1 to 30000
[2019-02-21 23:29:55] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-02-21 23:29:55] [data] Setting vocabulary size for input 2 to 30000
[2019-02-21 23:29:55] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-02-21 23:29:55] [batching] Collecting statistics for batch fitting with step size 10
[2019-02-21 23:29:58] [memory] Extending reserved space to 9088 MB (device gpu0)
[2019-02-21 23:29:59] [memory] Extending reserved space to 9088 MB (device gpu1)
[2019-02-21 23:29:59] [comm] NCCL communicator overridden
[2019-02-21 23:29:59] [training] Using 2 GPUs
[2019-02-21 23:29:59] [memory] Reserving 345 MB, device gpu0
[2019-02-21 23:29:59] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-02-21 23:29:59] [memory] Reserving 345 MB, device gpu0
[2019-02-21 23:30:09] [batching] Done. Typical MB size is 40786 target words
[2019-02-21 23:30:09] [memory] Extending reserved space to 9088 MB (device gpu0)
[2019-02-21 23:30:10] [memory] Extending reserved space to 9088 MB (device gpu1)
[2019-02-21 23:30:10] [comm] NCCL communicator overridden
[2019-02-21 23:30:10] [training] Using 2 GPUs
[2019-02-21 23:30:10] Training started
[2019-02-21 23:30:10] [data] Shuffling data
[2019-02-21 23:30:31] [data] Done reading 41736982 sentences
[2019-02-21 23:35:45] [data] Done shuffling 41736982 sentences to temp files
[2019-02-21 23:36:28] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-02-21 23:36:28] [memory] Reserving 345 MB, device gpu1
[2019-02-21 23:36:28] [memory] Reserving 345 MB, device gpu0
[2019-02-21 23:36:28] [memory] Reserving 345 MB, device gpu0
[2019-02-21 23:36:28] [memory] Reserving 345 MB, device gpu1
[2019-02-21 23:36:28] [memory] Reserving 172 MB, device gpu0
[2019-02-21 23:36:28] [memory] Reserving 172 MB, device gpu1
[2019-02-21 23:36:29] [memory] Reserving 172 MB, device gpu0
[2019-02-21 23:36:29] [memory] Reserving 172 MB, device gpu1
[2019-02-21 23:36:29] [memory] Reserving 345 MB, device gpu0
[2019-02-21 23:36:29] [memory] Reserving 345 MB, device gpu1
[2019-02-21 23:49:58] Ep. 1 : Up. 500 : Sen. 1,340,919 : Cost 88.86091614 : Time 1202.60s : 10474.19 words/s : L.r. 3.1250e-06
[2019-02-22 00:03:32] Ep. 1 : Up. 1000 : Sen. 2,645,129 : Cost 76.70446777 : Time 813.61s : 15456.80 words/s : L.r. 6.2500e-06
[2019-02-22 00:17:04] Ep. 1 : Up. 1500 : Sen. 3,964,048 : Cost 67.07802582 : Time 812.03s : 15265.98 words/s : L.r. 9.3750e-06
[2019-02-22 00:30:37] Ep. 1 : Up. 2000 : Sen. 5,286,966 : Cost 65.88231659 : Time 812.90s : 15223.06 words/s : L.r. 1.2500e-05
[2019-02-22 00:44:18] Ep. 1 : Up. 2500 : Sen. 6,614,125 : Cost 66.76422119 : Time 821.28s : 15509.67 words/s : L.r. 1.5625e-05
[2019-02-22 00:57:59] Ep. 1 : Up. 3000 : Sen. 7,950,341 : Cost 65.34323120 : Time 821.26s : 15215.63 words/s : L.r. 1.8750e-05
[2019-02-22 01:11:33] Ep. 1 : Up. 3500 : Sen. 9,267,796 : Cost 65.21444702 : Time 813.60s : 15329.07 words/s : L.r. 2.1875e-05
[2019-02-22 01:25:17] Ep. 1 : Up. 4000 : Sen. 10,610,702 : Cost 64.55865479 : Time 824.71s : 15263.40 words/s : L.r. 2.5000e-05
[2019-02-22 01:38:49] Ep. 1 : Up. 4500 : Sen. 11,903,841 : Cost 65.49296570 : Time 811.57s : 15230.26 words/s : L.r. 2.8125e-05
[2019-02-22 01:52:27] Ep. 1 : Up. 5000 : Sen. 13,230,462 : Cost 64.41054535 : Time 818.25s : 15261.48 words/s : L.r. 3.1250e-05
[2019-02-22 01:52:27] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 01:52:34] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter5000.npz
[2019-02-22 01:52:39] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 01:52:46] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 01:53:00] [valid] Ep. 1 : Up. 5000 : cross-entropy : 78.3706 : new best
[2019-02-22 01:53:03] [valid] Ep. 1 : Up. 5000 : perplexity : 688.424 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 01:55:57] [valid] Ep. 1 : Up. 5000 : translation : 0 : new best
[2019-02-22 02:09:36] Ep. 1 : Up. 5500 : Sen. 14,552,544 : Cost 65.26328278 : Time 1028.95s : 12213.15 words/s : L.r. 3.4375e-05
[2019-02-22 02:23:13] Ep. 1 : Up. 6000 : Sen. 15,887,874 : Cost 64.31919098 : Time 816.92s : 15185.77 words/s : L.r. 3.7500e-05
[2019-02-22 02:36:53] Ep. 1 : Up. 6500 : Sen. 17,195,547 : Cost 67.28153992 : Time 819.87s : 15421.45 words/s : L.r. 4.0625e-05
[2019-02-22 02:50:26] Ep. 1 : Up. 7000 : Sen. 18,510,495 : Cost 66.05397797 : Time 813.43s : 15201.83 words/s : L.r. 4.3750e-05
[2019-02-22 03:04:06] Ep. 1 : Up. 7500 : Sen. 19,837,209 : Cost 66.83434296 : Time 819.42s : 15428.91 words/s : L.r. 4.6875e-05
[2019-02-22 03:17:39] Ep. 1 : Up. 8000 : Sen. 21,153,968 : Cost 67.28162384 : Time 813.57s : 15387.42 words/s : L.r. 5.0000e-05
[2019-02-22 03:31:17] Ep. 1 : Up. 8500 : Sen. 22,499,122 : Cost 66.61742401 : Time 817.90s : 15356.71 words/s : L.r. 5.3125e-05
[2019-02-22 03:44:51] Ep. 1 : Up. 9000 : Sen. 23,828,408 : Cost 67.18177795 : Time 813.78s : 15386.22 words/s : L.r. 5.6250e-05
[2019-02-22 03:58:25] Ep. 1 : Up. 9500 : Sen. 25,152,567 : Cost 68.43448639 : Time 814.26s : 15288.90 words/s : L.r. 5.9375e-05
[2019-02-22 04:11:54] Ep. 1 : Up. 10000 : Sen. 26,440,528 : Cost 70.70625305 : Time 809.05s : 15321.67 words/s : L.r. 6.2500e-05
[2019-02-22 04:11:54] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 04:12:01] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter10000.npz
[2019-02-22 04:12:06] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 04:12:13] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 04:12:29] [valid] Ep. 1 : Up. 10000 : cross-entropy : 80.9082 : stalled 1 times (last best: 78.3706)
[2019-02-22 04:12:32] [valid] Ep. 1 : Up. 10000 : perplexity : 850.633 : stalled 1 times (last best: 688.424)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 04:13:45] [valid] Ep. 1 : Up. 10000 : translation : 0 : stalled 1 times (last best: 0)
[2019-02-22 04:27:22] Ep. 1 : Up. 10500 : Sen. 27,791,638 : Cost 68.39234161 : Time 927.34s : 13658.09 words/s : L.r. 6.5625e-05
[2019-02-22 04:40:53] Ep. 1 : Up. 11000 : Sen. 29,110,775 : Cost 70.20433807 : Time 810.81s : 15420.14 words/s : L.r. 6.8750e-05
[2019-02-22 04:54:25] Ep. 1 : Up. 11500 : Sen. 30,430,251 : Cost 70.29154205 : Time 812.13s : 15519.52 words/s : L.r. 7.1875e-05
[2019-02-22 05:07:54] Ep. 1 : Up. 12000 : Sen. 31,730,673 : Cost 71.79067993 : Time 809.46s : 15238.01 words/s : L.r. 7.5000e-05
[2019-02-22 05:21:30] Ep. 1 : Up. 12500 : Sen. 33,089,034 : Cost 69.29093170 : Time 816.34s : 15442.44 words/s : L.r. 7.8125e-05
[2019-02-22 05:35:02] Ep. 1 : Up. 13000 : Sen. 34,415,578 : Cost 71.23200226 : Time 811.57s : 15322.60 words/s : L.r. 8.1250e-05
[2019-02-22 05:48:36] Ep. 1 : Up. 13500 : Sen. 35,745,738 : Cost 71.69704437 : Time 814.05s : 15496.96 words/s : L.r. 8.4375e-05
[2019-02-22 06:02:10] Ep. 1 : Up. 14000 : Sen. 37,061,337 : Cost 72.61906433 : Time 813.88s : 15430.18 words/s : L.r. 8.7500e-05
[2019-02-22 06:15:40] Ep. 1 : Up. 14500 : Sen. 38,368,401 : Cost 72.33937073 : Time 809.71s : 15440.20 words/s : L.r. 9.0625e-05
[2019-02-22 06:29:15] Ep. 1 : Up. 15000 : Sen. 39,699,374 : Cost 71.47860718 : Time 815.25s : 15512.04 words/s : L.r. 9.3750e-05
[2019-02-22 06:29:15] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 06:29:21] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter15000.npz
[2019-02-22 06:29:27] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 06:29:33] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 06:29:47] [valid] Ep. 1 : Up. 15000 : cross-entropy : 174.067 : stalled 2 times (last best: 78.3706)
[2019-02-22 06:29:50] [valid] Ep. 1 : Up. 15000 : perplexity : 2.00957e+06 : stalled 2 times (last best: 688.424)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 06:36:44] [valid] Ep. 1 : Up. 15000 : translation : 0 : stalled 2 times (last best: 0)
[2019-02-22 06:50:10] Ep. 1 : Up. 15500 : Sen. 41,026,399 : Cost 69.75721741 : Time 1254.62s : 9810.33 words/s : L.r. 9.6875e-05
[2019-02-22 06:57:21] Seen 41719119 samples
[2019-02-22 06:57:21] Starting epoch 2
[2019-02-22 06:57:21] [data] Shuffling data
[2019-02-22 06:57:41] [data] Done reading 41736982 sentences
[2019-02-22 07:02:57] [data] Done shuffling 41736982 sentences to temp files
[2019-02-22 07:09:54] Ep. 2 : Up. 16000 : Sen. 646,660 : Cost 69.09620667 : Time 1184.71s : 10462.50 words/s : L.r. 1.0000e-04
[2019-02-22 07:23:21] Ep. 2 : Up. 16500 : Sen. 1,936,120 : Cost 72.06856537 : Time 807.21s : 15539.53 words/s : L.r. 9.8473e-05
[2019-02-22 07:36:54] Ep. 2 : Up. 17000 : Sen. 3,282,310 : Cost 67.67906952 : Time 812.42s : 15436.46 words/s : L.r. 9.7014e-05
[2019-02-22 07:50:31] Ep. 2 : Up. 17500 : Sen. 4,617,650 : Cost 71.09931946 : Time 817.12s : 15433.52 words/s : L.r. 9.5618e-05
[2019-02-22 08:03:55] Ep. 2 : Up. 18000 : Sen. 5,916,625 : Cost 69.32640839 : Time 803.52s : 15426.79 words/s : L.r. 9.4281e-05
[2019-02-22 08:17:25] Ep. 2 : Up. 18500 : Sen. 7,221,229 : Cost 69.21516418 : Time 810.04s : 15418.09 words/s : L.r. 9.2998e-05
[2019-02-22 08:30:55] Ep. 2 : Up. 19000 : Sen. 8,548,261 : Cost 67.92281342 : Time 810.22s : 15531.76 words/s : L.r. 9.1766e-05
[2019-02-22 08:44:29] Ep. 2 : Up. 19500 : Sen. 9,886,456 : Cost 69.49797821 : Time 814.23s : 15399.46 words/s : L.r. 9.0582e-05
[2019-02-22 08:57:58] Ep. 2 : Up. 20000 : Sen. 11,226,031 : Cost 69.91049194 : Time 809.42s : 15561.95 words/s : L.r. 8.9443e-05
[2019-02-22 08:57:58] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 08:58:05] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter20000.npz
[2019-02-22 08:58:10] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 08:58:17] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 08:58:32] [valid] Ep. 2 : Up. 20000 : cross-entropy : 181.82 : stalled 3 times (last best: 78.3706)
[2019-02-22 08:58:35] [valid] Ep. 2 : Up. 20000 : perplexity : 3.83544e+06 : stalled 3 times (last best: 688.424)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 09:05:40] [valid] Ep. 2 : Up. 20000 : translation : 0 : stalled 3 times (last best: 0)
[2019-02-22 09:19:08] Ep. 2 : Up. 20500 : Sen. 12,540,804 : Cost 72.76381683 : Time 1269.98s : 9796.71 words/s : L.r. 8.8345e-05
[2019-02-22 09:32:40] Ep. 2 : Up. 21000 : Sen. 13,852,027 : Cost 75.69768524 : Time 811.96s : 15527.71 words/s : L.r. 8.7287e-05
[2019-02-22 09:46:09] Ep. 2 : Up. 21500 : Sen. 15,180,995 : Cost 74.56590271 : Time 808.49s : 15438.76 words/s : L.r. 8.6266e-05
[2019-02-22 09:59:30] Ep. 2 : Up. 22000 : Sen. 16,509,489 : Cost 77.82263184 : Time 801.23s : 15408.09 words/s : L.r. 8.5280e-05
[2019-02-22 10:13:04] Ep. 2 : Up. 22500 : Sen. 17,835,050 : Cost 81.03592682 : Time 813.75s : 15481.66 words/s : L.r. 8.4327e-05
[2019-02-22 10:26:30] Ep. 2 : Up. 23000 : Sen. 19,140,791 : Cost 84.17553711 : Time 806.34s : 15466.41 words/s : L.r. 8.3406e-05
[2019-02-22 10:40:00] Ep. 2 : Up. 23500 : Sen. 20,464,544 : Cost 79.46004486 : Time 810.09s : 15536.66 words/s : L.r. 8.2514e-05
[2019-02-22 10:53:25] Ep. 2 : Up. 24000 : Sen. 21,781,740 : Cost 80.73438263 : Time 804.78s : 15384.53 words/s : L.r. 8.1650e-05
[2019-02-22 11:06:51] Ep. 2 : Up. 24500 : Sen. 23,122,432 : Cost 85.12096405 : Time 806.15s : 15590.41 words/s : L.r. 8.0812e-05
[2019-02-22 11:20:19] Ep. 2 : Up. 25000 : Sen. 24,432,092 : Cost 91.27355194 : Time 807.80s : 15357.92 words/s : L.r. 8.0000e-05
[2019-02-22 11:20:19] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 11:20:25] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter25000.npz
[2019-02-22 11:20:31] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 11:20:37] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 11:20:52] [valid] Ep. 2 : Up. 25000 : cross-entropy : 101.371 : stalled 4 times (last best: 78.3706)
[2019-02-22 11:20:55] [valid] Ep. 2 : Up. 25000 : perplexity : 4684.95 : stalled 4 times (last best: 688.424)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 11:23:10] [valid] Ep. 2 : Up. 25000 : translation : 0 : stalled 4 times (last best: 0)
[2019-02-22 11:36:31] Ep. 2 : Up. 25500 : Sen. 25,762,776 : Cost 83.88352966 : Time 972.37s : 12976.51 words/s : L.r. 7.9212e-05
[2019-02-22 11:50:11] Ep. 2 : Up. 26000 : Sen. 27,095,783 : Cost 82.72453308 : Time 819.24s : 15426.53 words/s : L.r. 7.8446e-05
[2019-02-22 12:03:32] Ep. 2 : Up. 26500 : Sen. 28,399,472 : Cost 86.77754974 : Time 801.27s : 15510.46 words/s : L.r. 7.7703e-05
[2019-02-22 12:16:57] Ep. 2 : Up. 27000 : Sen. 29,738,480 : Cost 87.25877380 : Time 804.87s : 15428.80 words/s : L.r. 7.6980e-05
[2019-02-22 12:30:24] Ep. 2 : Up. 27500 : Sen. 31,048,864 : Cost 88.28286743 : Time 807.01s : 15571.59 words/s : L.r. 7.6277e-05
[2019-02-22 12:43:55] Ep. 2 : Up. 28000 : Sen. 32,377,196 : Cost 82.33834839 : Time 810.98s : 15494.08 words/s : L.r. 7.5593e-05
[2019-02-22 12:57:20] Ep. 2 : Up. 28500 : Sen. 33,690,413 : Cost 83.42617798 : Time 804.73s : 15367.70 words/s : L.r. 7.4927e-05
[2019-02-22 13:10:45] Ep. 2 : Up. 29000 : Sen. 35,022,424 : Cost 81.67429352 : Time 805.69s : 15584.56 words/s : L.r. 7.4278e-05
[2019-02-22 13:24:14] Ep. 2 : Up. 29500 : Sen. 36,344,632 : Cost 82.09367371 : Time 809.31s : 15461.11 words/s : L.r. 7.3646e-05
[2019-02-22 13:37:41] Ep. 2 : Up. 30000 : Sen. 37,658,634 : Cost 81.31269836 : Time 806.31s : 15556.66 words/s : L.r. 7.3030e-05
[2019-02-22 13:37:41] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz.orig.npz
[2019-02-22 13:37:47] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.iter30000.npz
[2019-02-22 13:37:52] Saving model weights and runtime parameters to model/model.src1tgt0.dual.doc_fake.npz
[2019-02-22 13:37:58] Saving Adam parameters to model/model.src1tgt0.dual.doc_fake.npz.optimizer.npz
[2019-02-22 13:38:13] [valid] Ep. 2 : Up. 30000 : cross-entropy : 107.758 : stalled 5 times (last best: 78.3706)
[2019-02-22 13:38:16] [valid] Ep. 2 : Up. 30000 : perplexity : 7979.49 : stalled 5 times (last best: 688.424)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-02-22 13:38:37] [valid] Ep. 2 : Up. 30000 : translation : 0 : stalled 5 times (last best: 0)
[2019-02-22 13:52:07] Ep. 2 : Up. 30500 : Sen. 38,996,128 : Cost 80.46369934 : Time 866.22s : 14494.04 words/s : L.r. 7.2429e-05
[2019-02-22 14:05:49] Ep. 2 : Up. 31000 : Sen. 40,316,146 : Cost 88.88282013 : Time 821.62s : 15288.34 words/s : L.r. 7.1842e-05
train_trans_dual_doc.sh: line 28: 26269 Terminated              $marian_home/marian --model model/model.src1tgt0.dual.doc_fake.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9000 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.newvocab.log --valid-log model/valid_trans.newvocab.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 1 --sync-sgd --seed 1111 --no-nccl --exponential-smoothing --no-restore-corpus
