[2019-05-18 03:08:59] [marian] Marian v1.7.8 e6a4b7b 2019-05-14 03:10:41 +0200
[2019-05-18 03:08:59] [marian] Running on metis.lingea.cz as process 10052 with command line:
[2019-05-18 03:08:59] [marian] /home/large/data/models/marian/marian-doc/doc-marian3_metis_prev_gate/build/marian --model model/model.src1tgt0.voita.last.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-voita --train-sets corp/opensub.en-fr.docs.train.en.bpe.src_prev corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 8200 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src_prev corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 8 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
[2019-05-18 03:08:59] [config] after-batches: 0
[2019-05-18 03:08:59] [config] after-epochs: 0
[2019-05-18 03:08:59] [config] allow-unk: false
[2019-05-18 03:08:59] [config] beam-size: 6
[2019-05-18 03:08:59] [config] bert-class-symbol: "[CLS]"
[2019-05-18 03:08:59] [config] bert-mask-symbol: "[MASK]"
[2019-05-18 03:08:59] [config] bert-masking-fraction: 0.15
[2019-05-18 03:08:59] [config] bert-sep-symbol: "[SEP]"
[2019-05-18 03:08:59] [config] bert-train-type-embeddings: true
[2019-05-18 03:08:59] [config] bert-type-vocab-size: 2
[2019-05-18 03:08:59] [config] best-deep: false
[2019-05-18 03:08:59] [config] clip-gemm: 0
[2019-05-18 03:08:59] [config] clip-norm: 5
[2019-05-18 03:08:59] [config] context-enc-depth: 1
[2019-05-18 03:08:59] [config] context-gate: false
[2019-05-18 03:08:59] [config] cost-type: ce-mean
[2019-05-18 03:08:59] [config] cpu-threads: 0
[2019-05-18 03:08:59] [config] data-weighting: ""
[2019-05-18 03:08:59] [config] data-weighting-type: sentence
[2019-05-18 03:08:59] [config] dec-cell: gru
[2019-05-18 03:08:59] [config] dec-cell-base-depth: 2
[2019-05-18 03:08:59] [config] dec-cell-high-depth: 1
[2019-05-18 03:08:59] [config] dec-depth: 6
[2019-05-18 03:08:59] [config] devices:
[2019-05-18 03:08:59] [config]   - 0
[2019-05-18 03:08:59] [config] dim-emb: 512
[2019-05-18 03:08:59] [config] dim-rnn: 1024
[2019-05-18 03:08:59] [config] dim-vocabs:
[2019-05-18 03:08:59] [config]   - 30000
[2019-05-18 03:08:59] [config]   - 30000
[2019-05-18 03:08:59] [config] disp-first: 0
[2019-05-18 03:08:59] [config] disp-freq: 500
[2019-05-18 03:08:59] [config] disp-label-counts: false
[2019-05-18 03:08:59] [config] dropout-rnn: 0
[2019-05-18 03:08:59] [config] dropout-src: 0
[2019-05-18 03:08:59] [config] dropout-trg: 0
[2019-05-18 03:08:59] [config] dump-config: ""
[2019-05-18 03:08:59] [config] early-stopping: 10
[2019-05-18 03:08:59] [config] embedding-fix-src: false
[2019-05-18 03:08:59] [config] embedding-fix-trg: false
[2019-05-18 03:08:59] [config] embedding-normalization: false
[2019-05-18 03:08:59] [config] embedding-vectors:
[2019-05-18 03:08:59] [config]   []
[2019-05-18 03:08:59] [config] enc-cell: gru
[2019-05-18 03:08:59] [config] enc-cell-depth: 1
[2019-05-18 03:08:59] [config] enc-depth: 6
[2019-05-18 03:08:59] [config] enc-type: bidirectional
[2019-05-18 03:08:59] [config] exponential-smoothing: 0.0001
[2019-05-18 03:08:59] [config] freeze: false
[2019-05-18 03:08:59] [config] grad-dropping-momentum: 0
[2019-05-18 03:08:59] [config] grad-dropping-rate: 0
[2019-05-18 03:08:59] [config] grad-dropping-warmup: 100
[2019-05-18 03:08:59] [config] guided-alignment: none
[2019-05-18 03:08:59] [config] guided-alignment-cost: mse
[2019-05-18 03:08:59] [config] guided-alignment-weight: 0.1
[2019-05-18 03:08:59] [config] ignore-model-config: false
[2019-05-18 03:08:59] [config] input-types:
[2019-05-18 03:08:59] [config]   []
[2019-05-18 03:08:59] [config] interpolate-env-vars: false
[2019-05-18 03:08:59] [config] keep-best: false
[2019-05-18 03:08:59] [config] label-smoothing: 0.1
[2019-05-18 03:08:59] [config] layer-normalization: false
[2019-05-18 03:08:59] [config] learn-rate: 0.0002
[2019-05-18 03:08:59] [config] log: model/train_trans.gate.log
[2019-05-18 03:08:59] [config] log-level: info
[2019-05-18 03:08:59] [config] log-time-zone: ""
[2019-05-18 03:08:59] [config] lr-decay: 0
[2019-05-18 03:08:59] [config] lr-decay-freq: 50000
[2019-05-18 03:08:59] [config] lr-decay-inv-sqrt:
[2019-05-18 03:08:59] [config]   - 16000
[2019-05-18 03:08:59] [config] lr-decay-repeat-warmup: false
[2019-05-18 03:08:59] [config] lr-decay-reset-optimizer: false
[2019-05-18 03:08:59] [config] lr-decay-start:
[2019-05-18 03:08:59] [config]   - 10
[2019-05-18 03:08:59] [config]   - 1
[2019-05-18 03:08:59] [config] lr-decay-strategy: epoch+stalled
[2019-05-18 03:08:59] [config] lr-report: true
[2019-05-18 03:08:59] [config] lr-warmup: 16000
[2019-05-18 03:08:59] [config] lr-warmup-at-reload: false
[2019-05-18 03:08:59] [config] lr-warmup-cycle: false
[2019-05-18 03:08:59] [config] lr-warmup-start-rate: 0
[2019-05-18 03:08:59] [config] max-length: 55
[2019-05-18 03:08:59] [config] max-length-crop: false
[2019-05-18 03:08:59] [config] max-length-factor: 3
[2019-05-18 03:08:59] [config] maxi-batch: 1000
[2019-05-18 03:08:59] [config] maxi-batch-sort: trg
[2019-05-18 03:08:59] [config] mini-batch: 1000
[2019-05-18 03:08:59] [config] mini-batch-fit: true
[2019-05-18 03:08:59] [config] mini-batch-fit-step: 10
[2019-05-18 03:08:59] [config] mini-batch-overstuff: 1
[2019-05-18 03:08:59] [config] mini-batch-track-lr: false
[2019-05-18 03:08:59] [config] mini-batch-understuff: 1
[2019-05-18 03:08:59] [config] mini-batch-warmup: 0
[2019-05-18 03:08:59] [config] mini-batch-words: 0
[2019-05-18 03:08:59] [config] mini-batch-words-ref: 0
[2019-05-18 03:08:59] [config] model: model/model.src1tgt0.voita.last.npz
[2019-05-18 03:08:59] [config] multi-loss-type: sum
[2019-05-18 03:08:59] [config] multi-node: false
[2019-05-18 03:08:59] [config] multi-node-overlap: true
[2019-05-18 03:08:59] [config] n-best: false
[2019-05-18 03:08:59] [config] no-nccl: false
[2019-05-18 03:08:59] [config] no-reload: false
[2019-05-18 03:08:59] [config] no-restore-corpus: true
[2019-05-18 03:08:59] [config] no-shuffle: false
[2019-05-18 03:08:59] [config] normalize: 0.6
[2019-05-18 03:08:59] [config] num-devices: 0
[2019-05-18 03:08:59] [config] optimizer: adam
[2019-05-18 03:08:59] [config] optimizer-delay: 8
[2019-05-18 03:08:59] [config] optimizer-params:
[2019-05-18 03:08:59] [config]   - 0.9
[2019-05-18 03:08:59] [config]   - 0.98
[2019-05-18 03:08:59] [config]   - 1e-09
[2019-05-18 03:08:59] [config] overwrite: false
[2019-05-18 03:08:59] [config] pretrained-model: ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-18 03:08:59] [config] quiet: false
[2019-05-18 03:08:59] [config] quiet-translation: true
[2019-05-18 03:08:59] [config] relative-paths: false
[2019-05-18 03:08:59] [config] right-left: false
[2019-05-18 03:08:59] [config] save-freq: 5000
[2019-05-18 03:08:59] [config] seed: 1111
[2019-05-18 03:08:59] [config] shuffle-in-ram: false
[2019-05-18 03:08:59] [config] skip: false
[2019-05-18 03:08:59] [config] sqlite: ""
[2019-05-18 03:08:59] [config] sqlite-drop: false
[2019-05-18 03:08:59] [config] sync-sgd: true
[2019-05-18 03:08:59] [config] tempdir: /tmp
[2019-05-18 03:08:59] [config] tied-embeddings: false
[2019-05-18 03:08:59] [config] tied-embeddings-all: true
[2019-05-18 03:08:59] [config] tied-embeddings-src: false
[2019-05-18 03:08:59] [config] train-sets:
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src_prev
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.train.fr.bpe
[2019-05-18 03:08:59] [config] transformer-aan-activation: swish
[2019-05-18 03:08:59] [config] transformer-aan-depth: 2
[2019-05-18 03:08:59] [config] transformer-aan-nogate: false
[2019-05-18 03:08:59] [config] transformer-decoder-autoreg: self-attention
[2019-05-18 03:08:59] [config] transformer-dim-aan: 2048
[2019-05-18 03:08:59] [config] transformer-dim-ffn: 2048
[2019-05-18 03:08:59] [config] transformer-dropout: 0.1
[2019-05-18 03:08:59] [config] transformer-dropout-attention: 0
[2019-05-18 03:08:59] [config] transformer-dropout-ffn: 0
[2019-05-18 03:08:59] [config] transformer-ffn-activation: swish
[2019-05-18 03:08:59] [config] transformer-ffn-depth: 2
[2019-05-18 03:08:59] [config] transformer-guided-alignment-layer: last
[2019-05-18 03:08:59] [config] transformer-heads: 8
[2019-05-18 03:08:59] [config] transformer-no-projection: false
[2019-05-18 03:08:59] [config] transformer-postprocess: dan
[2019-05-18 03:08:59] [config] transformer-postprocess-emb: d
[2019-05-18 03:08:59] [config] transformer-preprocess: ""
[2019-05-18 03:08:59] [config] transformer-tied-layers:
[2019-05-18 03:08:59] [config]   []
[2019-05-18 03:08:59] [config] transformer-train-position-embeddings: false
[2019-05-18 03:08:59] [config] type: transformer-voita
[2019-05-18 03:08:59] [config] ulr: false
[2019-05-18 03:08:59] [config] ulr-dim-emb: 0
[2019-05-18 03:08:59] [config] ulr-dropout: 0
[2019-05-18 03:08:59] [config] ulr-keys-vectors: ""
[2019-05-18 03:08:59] [config] ulr-query-vectors: ""
[2019-05-18 03:08:59] [config] ulr-softmax-temperature: 1
[2019-05-18 03:08:59] [config] ulr-trainable-transformation: false
[2019-05-18 03:08:59] [config] valid-freq: 5000
[2019-05-18 03:08:59] [config] valid-log: model/valid_trans.gate.log
[2019-05-18 03:08:59] [config] valid-max-length: 1000
[2019-05-18 03:08:59] [config] valid-metrics:
[2019-05-18 03:08:59] [config]   - cross-entropy
[2019-05-18 03:08:59] [config]   - perplexity
[2019-05-18 03:08:59] [config]   - translation
[2019-05-18 03:08:59] [config] valid-mini-batch: 64
[2019-05-18 03:08:59] [config] valid-script-path: ./val.sh
[2019-05-18 03:08:59] [config] valid-sets:
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src_prev
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src
[2019-05-18 03:08:59] [config]   - corp/opensub.en-fr.docs.dev.fr.bpe
[2019-05-18 03:08:59] [config] valid-translation-output: data/valid.bpe.en.output
[2019-05-18 03:08:59] [config] vocabs:
[2019-05-18 03:08:59] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [config] word-penalty: 0
[2019-05-18 03:08:59] [config] workspace: 8200
[2019-05-18 03:08:59] [config] Model is being created with Marian v1.7.8 e6a4b7b 2019-05-14 03:10:41 +0200
[2019-05-18 03:08:59] Using synchronous training
[2019-05-18 03:08:59] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [data] Setting vocabulary size for input 0 to 30000
[2019-05-18 03:08:59] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [data] Setting vocabulary size for input 1 to 30000
[2019-05-18 03:08:59] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-18 03:08:59] [data] Setting vocabulary size for input 2 to 30000
[2019-05-18 03:08:59] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-18 03:08:59] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-18 03:09:00] [memory] Extending reserved space to 8320 MB (device gpu0)
[2019-05-18 03:09:00] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-18 03:09:00] [comm] NCCLCommunicator constructed successfully.
[2019-05-18 03:09:00] [training] Using 1 GPUs
[2019-05-18 03:09:00] [memory] Reserving 237 MB, device gpu0
[2019-05-18 03:09:00] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-05-18 03:09:00] [memory] Reserving 237 MB, device gpu0
[2019-05-18 03:09:09] [batching] Done. Typical MB size is 38400 target words
[2019-05-18 03:09:09] [memory] Extending reserved space to 8320 MB (device gpu0)
[2019-05-18 03:09:09] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-18 03:09:09] [comm] NCCLCommunicator constructed successfully.
[2019-05-18 03:09:09] [training] Using 1 GPUs
[2019-05-18 03:09:09] [training] Initializing model weights with the pre-trained model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-18 03:09:09] Loading model from ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-18 03:09:10] Training started
[2019-05-18 03:09:10] [data] Shuffling data
tcmalloc: large alloc 1073741824 bytes == 0x112ee2000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7ff65f7fa000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7ff5deffa000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x112ee2000 @ 
[2019-05-18 03:09:22] [data] Done reading 41736982 sentences
[2019-05-18 03:18:31] [data] Done shuffling 41736982 sentences to temp files
[2019-05-18 03:18:52] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-05-18 03:18:52] [memory] Reserving 249 MB, device gpu0
[2019-05-18 03:18:52] [memory] Reserving 249 MB, device gpu0
[2019-05-18 03:18:52] [memory] Reserving 249 MB, device gpu0
[2019-05-18 03:18:53] [memory] Reserving 498 MB, device gpu0
[2019-05-18 03:32:54] Ep. 1 : Up. 500 : Sen. 728,131 : Cost 61.18646622 : Time 1434.54s : 4727.29 words/s : L.r. 6.2500e-06
[2019-05-18 03:47:13] Ep. 1 : Up. 1000 : Sen. 1,454,583 : Cost 25.55908775 : Time 859.30s : 8067.59 words/s : L.r. 1.2500e-05
[2019-05-18 04:01:36] Ep. 1 : Up. 1500 : Sen. 2,180,315 : Cost 24.38453865 : Time 862.92s : 8085.98 words/s : L.r. 1.8750e-05
[2019-05-18 04:15:48] Ep. 1 : Up. 2000 : Sen. 2,914,219 : Cost 23.29083824 : Time 851.65s : 8026.22 words/s : L.r. 2.5000e-05
[2019-05-18 04:30:15] Ep. 1 : Up. 2500 : Sen. 3,656,146 : Cost 23.66177177 : Time 867.64s : 8092.79 words/s : L.r. 3.1250e-05
[2019-05-18 04:44:14] Ep. 1 : Up. 3000 : Sen. 4,352,875 : Cost 24.17093658 : Time 838.75s : 8034.21 words/s : L.r. 3.7500e-05
[2019-05-18 04:58:28] Ep. 1 : Up. 3500 : Sen. 5,082,333 : Cost 23.47230911 : Time 854.37s : 7984.71 words/s : L.r. 4.3750e-05
[2019-05-18 05:12:53] Ep. 1 : Up. 4000 : Sen. 5,827,877 : Cost 23.21104050 : Time 864.84s : 8107.20 words/s : L.r. 5.0000e-05
[2019-05-18 05:27:16] Ep. 1 : Up. 4500 : Sen. 6,566,983 : Cost 23.23229218 : Time 862.71s : 7955.31 words/s : L.r. 5.6250e-05
[2019-05-18 05:40:56] Ep. 1 : Up. 5000 : Sen. 7,247,099 : Cost 23.96964836 : Time 820.19s : 7961.28 words/s : L.r. 6.2500e-05
[2019-05-18 05:40:56] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz.orig.npz
[2019-05-18 05:41:01] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.iter5000.npz
[2019-05-18 05:41:06] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz
[2019-05-18 05:41:11] Saving Adam parameters to model/model.src1tgt0.voita.last.npz.optimizer.npz
[2019-05-18 05:41:25] [valid] Ep. 1 : Up. 5000 : cross-entropy : 19.1688 : new best
[2019-05-18 05:41:31] [valid] Ep. 1 : Up. 5000 : perplexity : 4.42494 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-18 05:42:45] [valid] Ep. 1 : Up. 5000 : translation : 34.26 : new best
[2019-05-18 05:57:22] Ep. 1 : Up. 5500 : Sen. 7,999,256 : Cost 23.48717880 : Time 985.54s : 7228.16 words/s : L.r. 6.8750e-05
[2019-05-18 06:11:44] Ep. 1 : Up. 6000 : Sen. 8,740,740 : Cost 22.91141319 : Time 861.83s : 7952.94 words/s : L.r. 7.5000e-05
[2019-05-18 06:25:59] Ep. 1 : Up. 6500 : Sen. 9,457,067 : Cost 24.32664108 : Time 855.19s : 8153.50 words/s : L.r. 8.1250e-05
[2019-05-18 06:40:04] Ep. 1 : Up. 7000 : Sen. 10,166,596 : Cost 23.72683907 : Time 845.30s : 7956.66 words/s : L.r. 8.7500e-05
[2019-05-18 06:54:18] Ep. 1 : Up. 7500 : Sen. 10,899,215 : Cost 23.25160599 : Time 853.80s : 8026.75 words/s : L.r. 9.3750e-05
[2019-05-18 07:08:57] Ep. 1 : Up. 8000 : Sen. 11,652,036 : Cost 23.52293015 : Time 879.04s : 8090.98 words/s : L.r. 1.0000e-04
[2019-05-18 07:22:52] Ep. 1 : Up. 8500 : Sen. 12,361,229 : Cost 23.36886215 : Time 835.38s : 7920.01 words/s : L.r. 1.0625e-04
[2019-05-18 07:37:18] Ep. 1 : Up. 9000 : Sen. 13,086,236 : Cost 24.07758522 : Time 865.88s : 8076.06 words/s : L.r. 1.1250e-04
[2019-05-18 07:51:37] Ep. 1 : Up. 9500 : Sen. 13,817,734 : Cost 23.44743729 : Time 858.53s : 7969.63 words/s : L.r. 1.1875e-04
[2019-05-18 08:05:50] Ep. 1 : Up. 10000 : Sen. 14,541,826 : Cost 23.57412338 : Time 853.57s : 7990.23 words/s : L.r. 1.2500e-04
[2019-05-18 08:05:50] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz.orig.npz
[2019-05-18 08:05:55] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.iter10000.npz
[2019-05-18 08:06:00] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz
[2019-05-18 08:06:05] Saving Adam parameters to model/model.src1tgt0.voita.last.npz.optimizer.npz
[2019-05-18 08:06:20] [valid] Ep. 1 : Up. 10000 : cross-entropy : 19.0571 : new best
[2019-05-18 08:06:25] [valid] Ep. 1 : Up. 10000 : perplexity : 4.38679 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-18 08:07:43] [valid] Ep. 1 : Up. 10000 : translation : 34.44 : new best
[2019-05-18 08:22:11] Ep. 1 : Up. 10500 : Sen. 15,287,914 : Cost 23.61922264 : Time 980.81s : 7153.35 words/s : L.r. 1.3125e-04
[2019-05-18 08:36:22] Ep. 1 : Up. 11000 : Sen. 15,993,288 : Cost 24.24405861 : Time 850.81s : 7990.56 words/s : L.r. 1.3750e-04
[2019-05-18 08:50:31] Ep. 1 : Up. 11500 : Sen. 16,717,063 : Cost 23.59321404 : Time 849.56s : 8023.80 words/s : L.r. 1.4375e-04
[2019-05-18 09:04:47] Ep. 1 : Up. 12000 : Sen. 17,425,099 : Cost 24.90196609 : Time 855.69s : 8127.40 words/s : L.r. 1.5000e-04
[2019-05-18 09:19:05] Ep. 1 : Up. 12500 : Sen. 18,178,290 : Cost 22.86269569 : Time 857.48s : 7983.11 words/s : L.r. 1.5625e-04
[2019-05-18 09:33:25] Ep. 1 : Up. 13000 : Sen. 18,917,557 : Cost 23.55284309 : Time 860.48s : 8019.13 words/s : L.r. 1.6250e-04
[2019-05-18 09:47:58] Ep. 1 : Up. 13500 : Sen. 19,652,750 : Cost 24.30315399 : Time 872.99s : 8062.02 words/s : L.r. 1.6875e-04
[2019-05-18 10:02:38] Ep. 1 : Up. 14000 : Sen. 20,410,226 : Cost 23.61323166 : Time 880.05s : 8119.88 words/s : L.r. 1.7500e-04
[2019-05-18 10:16:37] Ep. 1 : Up. 14500 : Sen. 21,110,278 : Cost 24.30209541 : Time 839.14s : 7929.95 words/s : L.r. 1.8125e-04
[2019-05-18 10:31:00] Ep. 1 : Up. 15000 : Sen. 21,844,135 : Cost 24.02780151 : Time 863.21s : 8080.01 words/s : L.r. 1.8750e-04
[2019-05-18 10:31:00] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz.orig.npz
[2019-05-18 10:31:05] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.iter15000.npz
[2019-05-18 10:31:10] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz
[2019-05-18 10:31:15] Saving Adam parameters to model/model.src1tgt0.voita.last.npz.optimizer.npz
[2019-05-18 10:31:29] [valid] Ep. 1 : Up. 15000 : cross-entropy : 19.0868 : stalled 1 times (last best: 19.0571)
[2019-05-18 10:31:35] [valid] Ep. 1 : Up. 15000 : perplexity : 4.39689 : stalled 1 times (last best: 4.38679)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-18 10:32:51] [valid] Ep. 1 : Up. 15000 : translation : 34.36 : stalled 1 times (last best: 34.44)
[2019-05-18 10:46:43] Ep. 1 : Up. 15500 : Sen. 22,550,651 : Cost 23.83411217 : Time 942.60s : 7001.94 words/s : L.r. 1.9375e-04
[2019-05-18 11:01:11] Ep. 1 : Up. 16000 : Sen. 23,290,297 : Cost 23.80464935 : Time 868.15s : 8009.55 words/s : L.r. 2.0000e-04
[2019-05-18 11:15:35] Ep. 1 : Up. 16500 : Sen. 24,019,016 : Cost 24.41475487 : Time 863.83s : 8079.29 words/s : L.r. 1.9695e-04
[2019-05-18 11:29:58] Ep. 1 : Up. 17000 : Sen. 24,752,222 : Cost 24.24205017 : Time 863.26s : 8088.82 words/s : L.r. 1.9403e-04
[2019-05-18 11:44:07] Ep. 1 : Up. 17500 : Sen. 25,461,717 : Cost 24.48389816 : Time 848.26s : 8033.40 words/s : L.r. 1.9124e-04
[2019-05-18 11:58:27] Ep. 1 : Up. 18000 : Sen. 26,195,086 : Cost 23.55852509 : Time 860.69s : 7894.51 words/s : L.r. 1.8856e-04
[2019-05-18 12:12:58] Ep. 1 : Up. 18500 : Sen. 26,942,048 : Cost 23.73425865 : Time 870.65s : 8031.26 words/s : L.r. 1.8600e-04
[2019-05-18 12:28:15] Ep. 1 : Up. 19000 : Sen. 27,682,696 : Cost 23.84279251 : Time 916.84s : 7629.45 words/s : L.r. 1.8353e-04
[2019-05-18 12:42:23] Ep. 1 : Up. 19500 : Sen. 28,387,368 : Cost 24.57111931 : Time 848.55s : 7971.57 words/s : L.r. 1.8116e-04
[2019-05-18 12:56:52] Ep. 1 : Up. 20000 : Sen. 29,128,242 : Cost 23.86643028 : Time 868.62s : 8047.42 words/s : L.r. 1.7889e-04
[2019-05-18 12:56:52] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz.orig.npz
[2019-05-18 12:56:57] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.iter20000.npz
[2019-05-18 12:57:01] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz
[2019-05-18 12:57:06] Saving Adam parameters to model/model.src1tgt0.voita.last.npz.optimizer.npz
[2019-05-18 12:57:21] [valid] Ep. 1 : Up. 20000 : cross-entropy : 19.2146 : stalled 2 times (last best: 19.0571)
[2019-05-18 12:57:26] [valid] Ep. 1 : Up. 20000 : perplexity : 4.4407 : stalled 2 times (last best: 4.38679)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-18 12:58:42] [valid] Ep. 1 : Up. 20000 : translation : 34.23 : stalled 2 times (last best: 34.44)
[2019-05-18 13:12:55] Ep. 1 : Up. 20500 : Sen. 29,831,173 : Cost 24.39585114 : Time 963.39s : 6990.58 words/s : L.r. 1.7669e-04
[2019-05-18 13:27:40] Ep. 1 : Up. 21000 : Sen. 30,580,407 : Cost 23.86199570 : Time 884.53s : 7994.89 words/s : L.r. 1.7457e-04
[2019-05-18 13:41:58] Ep. 1 : Up. 21500 : Sen. 31,321,936 : Cost 23.18933487 : Time 857.84s : 7928.03 words/s : L.r. 1.7253e-04
[2019-05-18 13:56:23] Ep. 1 : Up. 22000 : Sen. 32,048,200 : Cost 24.52274132 : Time 865.69s : 8076.90 words/s : L.r. 1.7056e-04
[2019-05-18 14:10:36] Ep. 1 : Up. 22500 : Sen. 32,762,364 : Cost 24.03090668 : Time 852.72s : 7944.25 words/s : L.r. 1.6865e-04
[2019-05-18 14:25:23] Ep. 1 : Up. 23000 : Sen. 33,520,661 : Cost 23.75639915 : Time 886.92s : 8057.01 words/s : L.r. 1.6681e-04
[2019-05-18 14:39:31] Ep. 1 : Up. 23500 : Sen. 34,247,539 : Cost 23.35289764 : Time 847.41s : 7927.93 words/s : L.r. 1.6503e-04
[2019-05-18 14:54:10] Ep. 1 : Up. 24000 : Sen. 34,979,245 : Cost 24.56421661 : Time 879.70s : 8057.42 words/s : L.r. 1.6330e-04
[2019-05-18 15:08:47] Ep. 1 : Up. 24500 : Sen. 35,731,417 : Cost 23.28423500 : Time 877.22s : 7956.43 words/s : L.r. 1.6162e-04
[2019-05-18 15:23:12] Ep. 1 : Up. 25000 : Sen. 36,466,163 : Cost 23.67081070 : Time 864.93s : 7970.52 words/s : L.r. 1.6000e-04
[2019-05-18 15:23:12] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz.orig.npz
[2019-05-18 15:23:17] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.iter25000.npz
[2019-05-18 15:23:22] Saving model weights and runtime parameters to model/model.src1tgt0.voita.last.npz
[2019-05-18 15:23:27] Saving Adam parameters to model/model.src1tgt0.voita.last.npz.optimizer.npz
[2019-05-18 15:23:42] [valid] Ep. 1 : Up. 25000 : cross-entropy : 19.2471 : stalled 3 times (last best: 19.0571)
[2019-05-18 15:23:47] [valid] Ep. 1 : Up. 25000 : perplexity : 4.45191 : stalled 3 times (last best: 4.38679)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-18 15:25:06] [valid] Ep. 1 : Up. 25000 : translation : 33.99 : stalled 3 times (last best: 34.44)
[2019-05-18 15:39:35] Ep. 1 : Up. 25500 : Sen. 37,167,554 : Cost 25.03623772 : Time 982.69s : 6989.90 words/s : L.r. 1.5842e-04
[2019-05-18 15:53:47] Ep. 1 : Up. 26000 : Sen. 37,877,885 : Cost 23.83462143 : Time 851.53s : 7850.92 words/s : L.r. 1.5689e-04
[2019-05-18 16:08:31] Ep. 1 : Up. 26500 : Sen. 38,628,452 : Cost 23.43563271 : Time 884.71s : 7935.20 words/s : L.r. 1.5541e-04
[2019-05-18 16:22:47] Ep. 1 : Up. 27000 : Sen. 39,340,104 : Cost 24.41851997 : Time 855.25s : 7986.07 words/s : L.r. 1.5396e-04
[2019-05-18 16:37:13] Ep. 1 : Up. 27500 : Sen. 40,072,634 : Cost 23.51725197 : Time 866.76s : 7872.70 words/s : L.r. 1.5255e-04
[2019-05-18 16:51:40] Ep. 1 : Up. 28000 : Sen. 40,788,983 : Cost 24.23480988 : Time 866.67s : 7941.44 words/s : L.r. 1.5119e-04
[2019-05-18 17:05:28] Ep. 1 : Up. 28500 : Sen. 41,472,226 : Cost 23.82297134 : Time 828.27s : 7784.94 words/s : L.r. 1.4985e-04
[2019-05-18 17:10:01] Seen 41709169 samples
[2019-05-18 17:10:01] Starting epoch 2
[2019-05-18 17:10:01] [data] Shuffling data
tcmalloc: large alloc 2147483648 bytes == 0x1126a6000 @ 
tcmalloc: large alloc 2147483648 bytes == 0x7ff4fa000000 @ 
[2019-05-18 17:10:15] [data] Done reading 41736982 sentences
train_doc_new_spider.newest.voita.sh: line 30: 10052 Terminated              $marian_home/marian --model model/model.src1tgt0.voita.last.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-voita --train-sets corp/opensub.en-fr.docs.train.en.bpe.src_prev corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 8200 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src_prev corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 8 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
