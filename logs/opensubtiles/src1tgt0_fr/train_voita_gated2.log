[2019-05-03 16:28:32] [marian] Marian v1.7.8 1fcb013 2019-05-03 03:15:04 +0200
[2019-05-03 16:28:32] [marian] Running on bakchus.lingea.cz as process 22451 with command line:
[2019-05-03 16:28:32] [marian] /home/large/data/models/marian/marian-doc/marian_voita/doc-marian/build/marian --model model/model.voita.gate2.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.en.bpe.src_prev corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9300 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src_prev corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
[2019-05-03 16:28:32] [config] after-batches: 0
[2019-05-03 16:28:32] [config] after-epochs: 0
[2019-05-03 16:28:32] [config] allow-unk: false
[2019-05-03 16:28:32] [config] beam-size: 6
[2019-05-03 16:28:32] [config] bert-class-symbol: "[CLS]"
[2019-05-03 16:28:32] [config] bert-mask-symbol: "[MASK]"
[2019-05-03 16:28:32] [config] bert-masking-fraction: 0.15
[2019-05-03 16:28:32] [config] bert-sep-symbol: "[SEP]"
[2019-05-03 16:28:32] [config] bert-train-type-embeddings: true
[2019-05-03 16:28:32] [config] bert-type-vocab-size: 2
[2019-05-03 16:28:32] [config] best-deep: false
[2019-05-03 16:28:32] [config] clip-gemm: 0
[2019-05-03 16:28:32] [config] clip-norm: 5
[2019-05-03 16:28:32] [config] context-enc-depth: 1
[2019-05-03 16:28:32] [config] cost-type: ce-mean
[2019-05-03 16:28:32] [config] cpu-threads: 0
[2019-05-03 16:28:32] [config] data-weighting: ""
[2019-05-03 16:28:32] [config] data-weighting-type: sentence
[2019-05-03 16:28:32] [config] dec-cell: gru
[2019-05-03 16:28:32] [config] dec-cell-base-depth: 2
[2019-05-03 16:28:32] [config] dec-cell-high-depth: 1
[2019-05-03 16:28:32] [config] dec-depth: 6
[2019-05-03 16:28:32] [config] devices:
[2019-05-03 16:28:32] [config]   - 0
[2019-05-03 16:28:32] [config] dim-emb: 512
[2019-05-03 16:28:32] [config] dim-rnn: 1024
[2019-05-03 16:28:32] [config] dim-vocabs:
[2019-05-03 16:28:32] [config]   - 30000
[2019-05-03 16:28:32] [config]   - 30000
[2019-05-03 16:28:32] [config] disp-first: 0
[2019-05-03 16:28:32] [config] disp-freq: 500
[2019-05-03 16:28:32] [config] disp-label-counts: false
[2019-05-03 16:28:32] [config] dropout-rnn: 0
[2019-05-03 16:28:32] [config] dropout-src: 0
[2019-05-03 16:28:32] [config] dropout-trg: 0
[2019-05-03 16:28:32] [config] dump-config: ""
[2019-05-03 16:28:32] [config] early-stopping: 10
[2019-05-03 16:28:32] [config] embedding-fix-src: false
[2019-05-03 16:28:32] [config] embedding-fix-trg: false
[2019-05-03 16:28:32] [config] embedding-normalization: false
[2019-05-03 16:28:32] [config] embedding-vectors:
[2019-05-03 16:28:32] [config]   []
[2019-05-03 16:28:32] [config] enc-cell: gru
[2019-05-03 16:28:32] [config] enc-cell-depth: 1
[2019-05-03 16:28:32] [config] enc-depth: 6
[2019-05-03 16:28:32] [config] enc-type: bidirectional
[2019-05-03 16:28:32] [config] exponential-smoothing: 0.0001
[2019-05-03 16:28:32] [config] freeze: false
[2019-05-03 16:28:32] [config] grad-dropping-momentum: 0
[2019-05-03 16:28:32] [config] grad-dropping-rate: 0
[2019-05-03 16:28:32] [config] grad-dropping-warmup: 100
[2019-05-03 16:28:32] [config] guided-alignment: none
[2019-05-03 16:28:32] [config] guided-alignment-cost: mse
[2019-05-03 16:28:32] [config] guided-alignment-weight: 0.1
[2019-05-03 16:28:32] [config] hier-att: false
[2019-05-03 16:28:32] [config] ignore-model-config: false
[2019-05-03 16:28:32] [config] input-types:
[2019-05-03 16:28:32] [config]   []
[2019-05-03 16:28:32] [config] interpolate-env-vars: false
[2019-05-03 16:28:32] [config] keep-best: false
[2019-05-03 16:28:32] [config] label-smoothing: 0.1
[2019-05-03 16:28:32] [config] layer-normalization: false
[2019-05-03 16:28:32] [config] learn-rate: 0.0002
[2019-05-03 16:28:32] [config] log: model/train_trans.gate.log
[2019-05-03 16:28:32] [config] log-level: info
[2019-05-03 16:28:32] [config] log-time-zone: ""
[2019-05-03 16:28:32] [config] lr-decay: 0
[2019-05-03 16:28:32] [config] lr-decay-freq: 50000
[2019-05-03 16:28:32] [config] lr-decay-inv-sqrt:
[2019-05-03 16:28:32] [config]   - 16000
[2019-05-03 16:28:32] [config] lr-decay-repeat-warmup: false
[2019-05-03 16:28:32] [config] lr-decay-reset-optimizer: false
[2019-05-03 16:28:32] [config] lr-decay-start:
[2019-05-03 16:28:32] [config]   - 10
[2019-05-03 16:28:32] [config]   - 1
[2019-05-03 16:28:32] [config] lr-decay-strategy: epoch+stalled
[2019-05-03 16:28:32] [config] lr-report: true
[2019-05-03 16:28:32] [config] lr-warmup: 16000
[2019-05-03 16:28:32] [config] lr-warmup-at-reload: false
[2019-05-03 16:28:32] [config] lr-warmup-cycle: false
[2019-05-03 16:28:32] [config] lr-warmup-start-rate: 0
[2019-05-03 16:28:32] [config] max-length: 55
[2019-05-03 16:28:32] [config] max-length-crop: false
[2019-05-03 16:28:32] [config] max-length-factor: 3
[2019-05-03 16:28:32] [config] maxi-batch: 1000
[2019-05-03 16:28:32] [config] maxi-batch-sort: trg
[2019-05-03 16:28:32] [config] mini-batch: 1000
[2019-05-03 16:28:32] [config] mini-batch-fit: true
[2019-05-03 16:28:32] [config] mini-batch-fit-step: 10
[2019-05-03 16:28:32] [config] mini-batch-overstuff: 1
[2019-05-03 16:28:32] [config] mini-batch-track-lr: false
[2019-05-03 16:28:32] [config] mini-batch-understuff: 1
[2019-05-03 16:28:32] [config] mini-batch-warmup: 0
[2019-05-03 16:28:32] [config] mini-batch-words: 0
[2019-05-03 16:28:32] [config] mini-batch-words-ref: 0
[2019-05-03 16:28:32] [config] model: model/model.voita.gate2.npz
[2019-05-03 16:28:32] [config] multi-loss-type: sum
[2019-05-03 16:28:32] [config] multi-node: false
[2019-05-03 16:28:32] [config] multi-node-overlap: true
[2019-05-03 16:28:32] [config] n-best: false
[2019-05-03 16:28:32] [config] no-nccl: false
[2019-05-03 16:28:32] [config] no-reload: false
[2019-05-03 16:28:32] [config] no-restore-corpus: true
[2019-05-03 16:28:32] [config] no-shuffle: false
[2019-05-03 16:28:32] [config] normalize: 0.6
[2019-05-03 16:28:32] [config] num-devices: 0
[2019-05-03 16:28:32] [config] optimizer: adam
[2019-05-03 16:28:32] [config] optimizer-delay: 4
[2019-05-03 16:28:32] [config] optimizer-params:
[2019-05-03 16:28:32] [config]   - 0.9
[2019-05-03 16:28:32] [config]   - 0.98
[2019-05-03 16:28:32] [config]   - 1e-09
[2019-05-03 16:28:32] [config] overwrite: false
[2019-05-03 16:28:32] [config] pretrained-model: ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 16:28:32] [config] quiet: false
[2019-05-03 16:28:32] [config] quiet-translation: true
[2019-05-03 16:28:32] [config] relative-paths: false
[2019-05-03 16:28:32] [config] right-left: false
[2019-05-03 16:28:32] [config] save-freq: 5000
[2019-05-03 16:28:32] [config] seed: 1111
[2019-05-03 16:28:32] [config] shuffle-in-ram: false
[2019-05-03 16:28:32] [config] skip: false
[2019-05-03 16:28:32] [config] sqlite: ""
[2019-05-03 16:28:32] [config] sqlite-drop: false
[2019-05-03 16:28:32] [config] sync-sgd: true
[2019-05-03 16:28:32] [config] tempdir: /tmp
[2019-05-03 16:28:32] [config] tied-embeddings: false
[2019-05-03 16:28:32] [config] tied-embeddings-all: true
[2019-05-03 16:28:32] [config] tied-embeddings-src: false
[2019-05-03 16:28:32] [config] train-sets:
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src_prev
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.train.en.bpe.src
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.train.fr.bpe
[2019-05-03 16:28:32] [config] transformer-aan-activation: swish
[2019-05-03 16:28:32] [config] transformer-aan-depth: 2
[2019-05-03 16:28:32] [config] transformer-aan-nogate: false
[2019-05-03 16:28:32] [config] transformer-decoder-autoreg: self-attention
[2019-05-03 16:28:32] [config] transformer-dim-aan: 2048
[2019-05-03 16:28:32] [config] transformer-dim-ffn: 2048
[2019-05-03 16:28:32] [config] transformer-dropout: 0.1
[2019-05-03 16:28:32] [config] transformer-dropout-attention: 0
[2019-05-03 16:28:32] [config] transformer-dropout-ffn: 0
[2019-05-03 16:28:32] [config] transformer-ffn-activation: swish
[2019-05-03 16:28:32] [config] transformer-ffn-depth: 2
[2019-05-03 16:28:32] [config] transformer-guided-alignment-layer: last
[2019-05-03 16:28:32] [config] transformer-heads: 8
[2019-05-03 16:28:32] [config] transformer-no-projection: false
[2019-05-03 16:28:32] [config] transformer-postprocess: dan
[2019-05-03 16:28:32] [config] transformer-postprocess-emb: d
[2019-05-03 16:28:32] [config] transformer-preprocess: ""
[2019-05-03 16:28:32] [config] transformer-tied-layers:
[2019-05-03 16:28:32] [config]   []
[2019-05-03 16:28:32] [config] transformer-train-position-embeddings: false
[2019-05-03 16:28:32] [config] type: transformer-context
[2019-05-03 16:28:32] [config] ulr: false
[2019-05-03 16:28:32] [config] ulr-dim-emb: 0
[2019-05-03 16:28:32] [config] ulr-dropout: 0
[2019-05-03 16:28:32] [config] ulr-keys-vectors: ""
[2019-05-03 16:28:32] [config] ulr-query-vectors: ""
[2019-05-03 16:28:32] [config] ulr-softmax-temperature: 1
[2019-05-03 16:28:32] [config] ulr-trainable-transformation: false
[2019-05-03 16:28:32] [config] valid-freq: 5000
[2019-05-03 16:28:32] [config] valid-log: model/valid_trans.gate.log
[2019-05-03 16:28:32] [config] valid-max-length: 1000
[2019-05-03 16:28:32] [config] valid-metrics:
[2019-05-03 16:28:32] [config]   - cross-entropy
[2019-05-03 16:28:32] [config]   - perplexity
[2019-05-03 16:28:32] [config]   - translation
[2019-05-03 16:28:32] [config] valid-mini-batch: 64
[2019-05-03 16:28:32] [config] valid-script-path: ./val.sh
[2019-05-03 16:28:32] [config] valid-sets:
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src_prev
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.dev.en.bpe.src
[2019-05-03 16:28:32] [config]   - corp/opensub.en-fr.docs.dev.fr.bpe
[2019-05-03 16:28:32] [config] valid-translation-output: data/valid.bpe.en.output
[2019-05-03 16:28:32] [config] vocabs:
[2019-05-03 16:28:32] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:32] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:32] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:32] [config] word-penalty: 0
[2019-05-03 16:28:32] [config] workspace: 9300
[2019-05-03 16:28:32] [config] Model is being created with Marian v1.7.8 1fcb013 2019-05-03 03:15:04 +0200
[2019-05-03 16:28:32] Using synchronous training
[2019-05-03 16:28:32] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:33] [data] Setting vocabulary size for input 0 to 30000
[2019-05-03 16:28:33] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:33] [data] Setting vocabulary size for input 1 to 30000
[2019-05-03 16:28:33] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-03 16:28:33] [data] Setting vocabulary size for input 2 to 30000
[2019-05-03 16:28:33] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-03 16:28:33] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-03 16:28:36] [memory] Extending reserved space to 9344 MB (device gpu0)
[2019-05-03 16:28:37] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-03 16:28:37] [comm] NCCLCommunicator constructed successfully.
[2019-05-03 16:28:37] [training] Using 1 GPUs
[2019-05-03 16:28:37] [memory] Reserving 237 MB, device gpu0
[2019-05-03 16:28:37] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-05-03 16:28:37] [memory] Reserving 237 MB, device gpu0
[2019-05-03 16:28:48] [batching] Done. Typical MB size is 21840 target words
[2019-05-03 16:28:48] [memory] Extending reserved space to 9344 MB (device gpu0)
[2019-05-03 16:28:48] [comm] Using NCCL 2.4.2 for GPU communication
[2019-05-03 16:28:48] [comm] NCCLCommunicator constructed successfully.
[2019-05-03 16:28:48] [training] Using 1 GPUs
[2019-05-03 16:28:48] [training] Initializing model weights with the pre-trained model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 16:28:48] Loading model from ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz
[2019-05-03 16:28:51] Training started
[2019-05-03 16:28:51] [data] Shuffling data
[2019-05-03 16:29:34] [data] Done reading 41736982 sentences
[2019-05-03 16:33:02] [data] Done shuffling 41736982 sentences to temp files
[2019-05-03 16:33:39] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-05-03 16:33:39] [memory] Reserving 249 MB, device gpu0
[2019-05-03 16:33:39] [memory] Reserving 249 MB, device gpu0
[2019-05-03 16:33:39] [memory] Reserving 249 MB, device gpu0
[2019-05-03 16:33:41] [memory] Reserving 498 MB, device gpu0
[2019-05-03 16:42:10] Ep. 1 : Up. 500 : Sen. 481,176 : Cost 60.40675354 : Time 816.55s : 5441.67 words/s : L.r. 6.2500e-06
[2019-05-03 16:50:46] Ep. 1 : Up. 1000 : Sen. 948,383 : Cost 25.83440971 : Time 515.84s : 8708.84 words/s : L.r. 1.2500e-05
[2019-05-03 16:59:26] Ep. 1 : Up. 1500 : Sen. 1,430,450 : Cost 23.65410042 : Time 520.18s : 8661.15 words/s : L.r. 1.8750e-05
[2019-05-03 17:08:03] Ep. 1 : Up. 2000 : Sen. 1,901,686 : Cost 24.06543350 : Time 516.93s : 8702.99 words/s : L.r. 2.5000e-05
[2019-05-03 17:16:47] Ep. 1 : Up. 2500 : Sen. 2,378,288 : Cost 24.08267403 : Time 523.76s : 8675.19 words/s : L.r. 3.1250e-05
[2019-05-03 17:25:15] Ep. 1 : Up. 3000 : Sen. 2,843,733 : Cost 23.44921875 : Time 508.16s : 8643.00 words/s : L.r. 3.7500e-05
[2019-05-03 17:33:49] Ep. 1 : Up. 3500 : Sen. 3,299,767 : Cost 24.50576210 : Time 513.97s : 8657.54 words/s : L.r. 4.3750e-05
[2019-05-03 17:42:29] Ep. 1 : Up. 4000 : Sen. 3,782,479 : Cost 22.87650490 : Time 520.37s : 8518.25 words/s : L.r. 5.0000e-05
[2019-05-03 17:51:08] Ep. 1 : Up. 4500 : Sen. 4,260,777 : Cost 23.29463768 : Time 518.72s : 8634.86 words/s : L.r. 5.6250e-05
[2019-05-03 17:59:47] Ep. 1 : Up. 5000 : Sen. 4,725,790 : Cost 24.22678375 : Time 519.36s : 8638.56 words/s : L.r. 6.2500e-05
[2019-05-03 17:59:47] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-03 17:59:52] Saving model weights and runtime parameters to model/model.voita.gate2.iter5000.npz
[2019-05-03 17:59:55] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-03 18:00:00] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-03 18:00:14] [valid] Ep. 1 : Up. 5000 : cross-entropy : 19.2564 : new best
[2019-05-03 18:00:20] [valid] Ep. 1 : Up. 5000 : perplexity : 4.45512 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 18:01:25] [valid] Ep. 1 : Up. 5000 : translation : 34.18 : new best
[2019-05-03 18:09:56] Ep. 1 : Up. 5500 : Sen. 5,204,499 : Cost 22.85643768 : Time 609.29s : 7195.45 words/s : L.r. 6.8750e-05
[2019-05-03 18:18:40] Ep. 1 : Up. 6000 : Sen. 5,671,244 : Cost 24.35947037 : Time 523.44s : 8659.88 words/s : L.r. 7.5000e-05
[2019-05-03 18:27:16] Ep. 1 : Up. 6500 : Sen. 6,136,632 : Cost 23.98911476 : Time 516.24s : 8728.01 words/s : L.r. 8.1250e-05
[2019-05-03 18:36:01] Ep. 1 : Up. 7000 : Sen. 6,613,609 : Cost 23.52378273 : Time 524.59s : 8581.70 words/s : L.r. 8.7500e-05
[2019-05-03 18:44:37] Ep. 1 : Up. 7500 : Sen. 7,091,964 : Cost 23.40275955 : Time 516.04s : 8628.05 words/s : L.r. 9.3750e-05
[2019-05-03 18:53:12] Ep. 1 : Up. 8000 : Sen. 7,556,491 : Cost 23.92966461 : Time 515.45s : 8559.67 words/s : L.r. 1.0000e-04
[2019-05-03 19:01:55] Ep. 1 : Up. 8500 : Sen. 8,037,273 : Cost 23.68071747 : Time 522.41s : 8689.11 words/s : L.r. 1.0625e-04
[2019-05-03 19:10:31] Ep. 1 : Up. 9000 : Sen. 8,517,143 : Cost 23.39434814 : Time 516.93s : 8661.14 words/s : L.r. 1.1250e-04
[2019-05-03 19:19:10] Ep. 1 : Up. 9500 : Sen. 8,989,896 : Cost 23.80774879 : Time 518.58s : 8611.08 words/s : L.r. 1.1875e-04
[2019-05-03 19:27:32] Ep. 1 : Up. 10000 : Sen. 9,447,976 : Cost 23.57807541 : Time 502.04s : 8509.35 words/s : L.r. 1.2500e-04
[2019-05-03 19:27:32] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-03 19:27:36] Saving model weights and runtime parameters to model/model.voita.gate2.iter10000.npz
[2019-05-03 19:27:40] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-03 19:27:45] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-03 19:27:59] [valid] Ep. 1 : Up. 10000 : cross-entropy : 19.1768 : new best
[2019-05-03 19:28:05] [valid] Ep. 1 : Up. 10000 : perplexity : 4.42768 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 19:29:11] [valid] Ep. 1 : Up. 10000 : translation : 34.37 : new best
[2019-05-03 19:37:52] Ep. 1 : Up. 10500 : Sen. 9,922,221 : Cost 24.16888809 : Time 619.60s : 7354.46 words/s : L.r. 1.3125e-04
[2019-05-03 19:46:23] Ep. 1 : Up. 11000 : Sen. 10,390,666 : Cost 23.82427979 : Time 511.05s : 8645.32 words/s : L.r. 1.3750e-04
[2019-05-03 19:54:56] Ep. 1 : Up. 11500 : Sen. 10,865,770 : Cost 23.58833885 : Time 513.29s : 8617.51 words/s : L.r. 1.4375e-04
[2019-05-03 20:03:33] Ep. 1 : Up. 12000 : Sen. 11,332,027 : Cost 24.31422424 : Time 516.50s : 8637.35 words/s : L.r. 1.5000e-04
[2019-05-03 20:12:13] Ep. 1 : Up. 12500 : Sen. 11,800,518 : Cost 24.38229942 : Time 520.13s : 8616.03 words/s : L.r. 1.5625e-04
[2019-05-03 20:20:45] Ep. 1 : Up. 13000 : Sen. 12,266,446 : Cost 23.76572609 : Time 512.70s : 8541.28 words/s : L.r. 1.6250e-04
[2019-05-03 20:29:30] Ep. 1 : Up. 13500 : Sen. 12,745,591 : Cost 24.23264503 : Time 525.00s : 8676.45 words/s : L.r. 1.6875e-04
[2019-05-03 20:38:08] Ep. 1 : Up. 14000 : Sen. 13,214,452 : Cost 24.08012962 : Time 517.84s : 8591.22 words/s : L.r. 1.7500e-04
[2019-05-03 20:46:44] Ep. 1 : Up. 14500 : Sen. 13,687,367 : Cost 23.84576225 : Time 515.77s : 8564.90 words/s : L.r. 1.8125e-04
[2019-05-03 20:55:23] Ep. 1 : Up. 15000 : Sen. 14,146,402 : Cost 25.44166565 : Time 519.31s : 8661.68 words/s : L.r. 1.8750e-04
[2019-05-03 20:55:23] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-03 20:55:28] Saving model weights and runtime parameters to model/model.voita.gate2.iter15000.npz
[2019-05-03 20:55:31] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-03 20:55:36] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-03 20:55:50] [valid] Ep. 1 : Up. 15000 : cross-entropy : 19.2923 : stalled 1 times (last best: 19.1768)
[2019-05-03 20:55:55] [valid] Ep. 1 : Up. 15000 : perplexity : 4.46755 : stalled 1 times (last best: 4.42768)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 20:57:02] [valid] Ep. 1 : Up. 15000 : translation : 34.24 : stalled 1 times (last best: 34.37)
[2019-05-03 21:05:46] Ep. 1 : Up. 15500 : Sen. 14,629,058 : Cost 23.73078346 : Time 622.34s : 7193.15 words/s : L.r. 1.9375e-04
[2019-05-03 21:14:21] Ep. 1 : Up. 16000 : Sen. 15,105,707 : Cost 23.63944817 : Time 514.92s : 8598.88 words/s : L.r. 2.0000e-04
[2019-05-03 21:22:58] Ep. 1 : Up. 16500 : Sen. 15,567,792 : Cost 24.93876076 : Time 517.17s : 8604.09 words/s : L.r. 1.9695e-04
[2019-05-03 21:31:37] Ep. 1 : Up. 17000 : Sen. 16,049,913 : Cost 23.62596130 : Time 518.84s : 8613.87 words/s : L.r. 1.9403e-04
[2019-05-03 21:40:11] Ep. 1 : Up. 17500 : Sen. 16,521,282 : Cost 24.07596016 : Time 514.43s : 8599.53 words/s : L.r. 1.9124e-04
[2019-05-03 21:48:59] Ep. 1 : Up. 18000 : Sen. 16,996,580 : Cost 24.86319923 : Time 528.17s : 8689.51 words/s : L.r. 1.8856e-04
[2019-05-03 21:57:42] Ep. 1 : Up. 18500 : Sen. 17,476,346 : Cost 24.07506752 : Time 522.37s : 8655.70 words/s : L.r. 1.8600e-04
[2019-05-03 22:06:14] Ep. 1 : Up. 19000 : Sen. 17,938,059 : Cost 24.38772964 : Time 512.08s : 8547.21 words/s : L.r. 1.8353e-04
[2019-05-03 22:14:53] Ep. 1 : Up. 19500 : Sen. 18,416,942 : Cost 23.98899269 : Time 519.21s : 8647.80 words/s : L.r. 1.8116e-04
[2019-05-03 22:23:35] Ep. 1 : Up. 20000 : Sen. 18,896,933 : Cost 24.10837555 : Time 522.46s : 8618.72 words/s : L.r. 1.7889e-04
[2019-05-03 22:23:35] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-03 22:23:40] Saving model weights and runtime parameters to model/model.voita.gate2.iter20000.npz
[2019-05-03 22:23:43] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-03 22:23:47] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-03 22:24:01] [valid] Ep. 1 : Up. 20000 : cross-entropy : 19.4337 : stalled 2 times (last best: 19.1768)
[2019-05-03 22:24:07] [valid] Ep. 1 : Up. 20000 : perplexity : 4.51682 : stalled 2 times (last best: 4.42768)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 22:25:12] [valid] Ep. 1 : Up. 20000 : translation : 33.98 : stalled 2 times (last best: 34.37)
[2019-05-03 22:33:43] Ep. 1 : Up. 20500 : Sen. 19,358,960 : Cost 24.39184380 : Time 607.23s : 7189.59 words/s : L.r. 1.7669e-04
[2019-05-03 22:42:32] Ep. 1 : Up. 21000 : Sen. 19,847,892 : Cost 24.03927422 : Time 529.17s : 8730.70 words/s : L.r. 1.7457e-04
[2019-05-03 22:51:05] Ep. 1 : Up. 21500 : Sen. 20,322,526 : Cost 23.75325394 : Time 513.40s : 8568.06 words/s : L.r. 1.7253e-04
[2019-05-03 22:59:44] Ep. 1 : Up. 22000 : Sen. 20,788,362 : Cost 24.61240578 : Time 518.42s : 8628.99 words/s : L.r. 1.7056e-04
[2019-05-03 23:08:28] Ep. 1 : Up. 22500 : Sen. 21,267,860 : Cost 24.12382698 : Time 524.33s : 8676.16 words/s : L.r. 1.6865e-04
[2019-05-03 23:17:09] Ep. 1 : Up. 23000 : Sen. 21,744,610 : Cost 24.25462914 : Time 520.74s : 8711.97 words/s : L.r. 1.6681e-04
[2019-05-03 23:25:37] Ep. 1 : Up. 23500 : Sen. 22,203,855 : Cost 24.49332619 : Time 507.93s : 8548.90 words/s : L.r. 1.6503e-04
[2019-05-03 23:34:19] Ep. 1 : Up. 24000 : Sen. 22,677,112 : Cost 24.40227127 : Time 522.73s : 8723.70 words/s : L.r. 1.6330e-04
[2019-05-03 23:42:50] Ep. 1 : Up. 24500 : Sen. 23,144,408 : Cost 23.95988846 : Time 510.71s : 8503.94 words/s : L.r. 1.6162e-04
[2019-05-03 23:51:26] Ep. 1 : Up. 25000 : Sen. 23,627,306 : Cost 23.27757263 : Time 516.05s : 8583.37 words/s : L.r. 1.6000e-04
[2019-05-03 23:51:26] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-03 23:51:30] Saving model weights and runtime parameters to model/model.voita.gate2.iter25000.npz
[2019-05-03 23:51:34] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-03 23:51:38] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-03 23:51:52] [valid] Ep. 1 : Up. 25000 : cross-entropy : 19.4362 : stalled 3 times (last best: 19.1768)
[2019-05-03 23:51:58] [valid] Ep. 1 : Up. 25000 : perplexity : 4.51771 : stalled 3 times (last best: 4.42768)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-03 23:53:03] [valid] Ep. 1 : Up. 25000 : translation : 34.1 : stalled 3 times (last best: 34.37)
[2019-05-04 00:01:46] Ep. 1 : Up. 25500 : Sen. 24,085,909 : Cost 25.44377518 : Time 620.16s : 7321.47 words/s : L.r. 1.5842e-04
[2019-05-04 00:10:25] Ep. 1 : Up. 26000 : Sen. 24,560,271 : Cost 24.16622353 : Time 519.25s : 8648.32 words/s : L.r. 1.5689e-04
[2019-05-04 00:18:57] Ep. 1 : Up. 26500 : Sen. 25,029,620 : Cost 23.84804916 : Time 512.01s : 8586.39 words/s : L.r. 1.5541e-04
[2019-05-04 00:27:31] Ep. 1 : Up. 27000 : Sen. 25,509,345 : Cost 23.44153595 : Time 513.28s : 8578.44 words/s : L.r. 1.5396e-04
[2019-05-04 00:36:15] Ep. 1 : Up. 27500 : Sen. 25,980,875 : Cost 24.69393730 : Time 524.38s : 8717.37 words/s : L.r. 1.5255e-04
[2019-05-04 00:44:52] Ep. 1 : Up. 28000 : Sen. 26,449,004 : Cost 24.34997749 : Time 517.36s : 8595.07 words/s : L.r. 1.5119e-04
[2019-05-04 00:53:30] Ep. 1 : Up. 28500 : Sen. 26,924,091 : Cost 23.87764931 : Time 517.99s : 8664.32 words/s : L.r. 1.4985e-04
[2019-05-04 01:02:14] Ep. 1 : Up. 29000 : Sen. 27,397,779 : Cost 24.71287918 : Time 523.34s : 8737.79 words/s : L.r. 1.4856e-04
[2019-05-04 01:10:57] Ep. 1 : Up. 29500 : Sen. 27,883,100 : Cost 23.57310104 : Time 523.60s : 8585.58 words/s : L.r. 1.4729e-04
[2019-05-04 01:19:24] Ep. 1 : Up. 30000 : Sen. 28,343,547 : Cost 24.06423378 : Time 506.94s : 8542.91 words/s : L.r. 1.4606e-04
[2019-05-04 01:19:24] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-04 01:19:29] Saving model weights and runtime parameters to model/model.voita.gate2.iter30000.npz
[2019-05-04 01:19:33] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-04 01:19:37] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-04 01:19:52] [valid] Ep. 1 : Up. 30000 : cross-entropy : 19.4152 : stalled 4 times (last best: 19.1768)
[2019-05-04 01:19:57] [valid] Ep. 1 : Up. 30000 : perplexity : 4.51037 : stalled 4 times (last best: 4.42768)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-04 01:21:02] [valid] Ep. 1 : Up. 30000 : translation : 34.09 : stalled 4 times (last best: 34.37)
[2019-05-04 01:29:42] Ep. 1 : Up. 30500 : Sen. 28,822,887 : Cost 23.96303177 : Time 617.99s : 7308.90 words/s : L.r. 1.4486e-04
[2019-05-04 01:38:24] Ep. 1 : Up. 31000 : Sen. 29,295,014 : Cost 24.33976173 : Time 521.70s : 8667.29 words/s : L.r. 1.4368e-04
[2019-05-04 01:46:51] Ep. 1 : Up. 31500 : Sen. 29,757,642 : Cost 23.77844810 : Time 507.21s : 8561.20 words/s : L.r. 1.4254e-04
[2019-05-04 01:55:44] Ep. 1 : Up. 32000 : Sen. 30,237,925 : Cost 24.47703362 : Time 532.93s : 8641.73 words/s : L.r. 1.4142e-04
[2019-05-04 02:04:19] Ep. 1 : Up. 32500 : Sen. 30,703,768 : Cost 24.05561447 : Time 514.38s : 8574.67 words/s : L.r. 1.4033e-04
[2019-05-04 02:12:59] Ep. 1 : Up. 33000 : Sen. 31,202,413 : Cost 22.79827499 : Time 520.12s : 8625.81 words/s : L.r. 1.3926e-04
[2019-05-04 02:21:37] Ep. 1 : Up. 33500 : Sen. 31,667,817 : Cost 24.27744484 : Time 518.35s : 8562.78 words/s : L.r. 1.3822e-04
[2019-05-04 02:30:16] Ep. 1 : Up. 34000 : Sen. 32,139,044 : Cost 24.28555679 : Time 519.42s : 8681.08 words/s : L.r. 1.3720e-04
[2019-05-04 02:38:56] Ep. 1 : Up. 34500 : Sen. 32,610,247 : Cost 24.31268501 : Time 519.12s : 8669.62 words/s : L.r. 1.3620e-04
[2019-05-04 02:47:36] Ep. 1 : Up. 35000 : Sen. 33,075,532 : Cost 24.34895325 : Time 519.97s : 8559.57 words/s : L.r. 1.3522e-04
[2019-05-04 02:47:36] Saving model weights and runtime parameters to model/model.voita.gate2.npz.orig.npz
[2019-05-04 02:47:40] Saving model weights and runtime parameters to model/model.voita.gate2.iter35000.npz
[2019-05-04 02:47:44] Saving model weights and runtime parameters to model/model.voita.gate2.npz
[2019-05-04 02:47:48] Saving Adam parameters to model/model.voita.gate2.npz.optimizer.npz
[2019-05-04 02:48:02] [valid] Ep. 1 : Up. 35000 : cross-entropy : 19.3834 : stalled 5 times (last best: 19.1768)
[2019-05-04 02:48:08] [valid] Ep. 1 : Up. 35000 : perplexity : 4.49926 : stalled 5 times (last best: 4.42768)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-04 02:49:13] [valid] Ep. 1 : Up. 35000 : translation : 34.03 : stalled 5 times (last best: 34.37)
[2019-05-04 02:57:48] Ep. 1 : Up. 35500 : Sen. 33,546,292 : Cost 24.02626991 : Time 612.85s : 7261.51 words/s : L.r. 1.3427e-04
[2019-05-04 03:06:34] Ep. 1 : Up. 36000 : Sen. 34,026,327 : Cost 23.72774315 : Time 525.48s : 8556.21 words/s : L.r. 1.3333e-04
[2019-05-04 03:15:21] Ep. 1 : Up. 36500 : Sen. 34,500,608 : Cost 24.45672989 : Time 526.97s : 8669.97 words/s : L.r. 1.3242e-04
[2019-05-04 03:23:59] Ep. 1 : Up. 37000 : Sen. 34,982,373 : Cost 23.31794548 : Time 517.77s : 8575.42 words/s : L.r. 1.3152e-04
[2019-05-04 03:32:30] Ep. 1 : Up. 37500 : Sen. 35,464,458 : Cost 22.98470688 : Time 511.58s : 8600.35 words/s : L.r. 1.3064e-04
[2019-05-04 03:41:11] Ep. 1 : Up. 38000 : Sen. 35,924,843 : Cost 25.04530334 : Time 521.30s : 8678.68 words/s : L.r. 1.2978e-04
train_voita_gated.sh: line 30: 22451 Terminated              $marian_home/marian --model model/model.voita.gate2.npz --pretrained-model ../src0tgt0_fr/model/model.src0tgt0.newvocab.iter630000.npz --type transformer-context --train-sets corp/opensub.en-fr.docs.train.en.bpe.src_prev corp/opensub.en-fr.docs.train.en.bpe.src corp/opensub.en-fr.docs.train.fr.bpe --max-length 55 --dim-vocabs 30000 30000 --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 9300 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/opensub.en-fr.docs.dev.en.bpe.src_prev corp/opensub.en-fr.docs.dev.en.bpe.src corp/opensub.en-fr.docs.dev.fr.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 64 --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0002 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
