[2019-05-05 04:44:50] [marian] Marian v1.7.8 1fcb013 2019-05-03 03:15:04 +0200
[2019-05-05 04:44:50] [marian] Running on pcknot5 as process 9995 with command line:
[2019-05-05 04:44:50] [marian] /mnt/minerva1/nlp/projects/nmt/doc-marian-new2/doc-marian/build/marian --model model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz --pretrained-model model/model.src1tgt0.docnew.enc_depth.test_base.npz.best-perplexity.npz --type transformer-context --train-sets corp/europarl.cs-en.docs.train.en.bpe.src_prev corp/europarl.cs-en.docs.train.en.bpe.src corp/europarl.cs-en.docs.train.cz.bpe --max-length 160 --dim-vocabs 30000 30000 --embedding-fix-src --embedding-fix-trg --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 7800 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --freeze --context-enc-depth 1 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics cross-entropy perplexity translation --valid-sets corp/europarl.cs-en.docs.dev.en.bpe.src_prev corp/europarl.cs-en.docs.dev.en.bpe.src corp/europarl.cs-en.docs.dev.cz.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 16 --keep-best --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --no-nccl --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
[2019-05-05 04:44:50] [config] after-batches: 0
[2019-05-05 04:44:50] [config] after-epochs: 0
[2019-05-05 04:44:50] [config] allow-unk: false
[2019-05-05 04:44:50] [config] beam-size: 6
[2019-05-05 04:44:50] [config] bert-class-symbol: "[CLS]"
[2019-05-05 04:44:50] [config] bert-mask-symbol: "[MASK]"
[2019-05-05 04:44:50] [config] bert-masking-fraction: 0.15
[2019-05-05 04:44:50] [config] bert-sep-symbol: "[SEP]"
[2019-05-05 04:44:50] [config] bert-train-type-embeddings: true
[2019-05-05 04:44:50] [config] bert-type-vocab-size: 2
[2019-05-05 04:44:50] [config] best-deep: false
[2019-05-05 04:44:50] [config] clip-gemm: 0
[2019-05-05 04:44:50] [config] clip-norm: 5
[2019-05-05 04:44:50] [config] context-enc-depth: 1
[2019-05-05 04:44:50] [config] cost-type: ce-mean
[2019-05-05 04:44:50] [config] cpu-threads: 0
[2019-05-05 04:44:50] [config] data-weighting: ""
[2019-05-05 04:44:50] [config] data-weighting-type: sentence
[2019-05-05 04:44:50] [config] dec-cell: gru
[2019-05-05 04:44:50] [config] dec-cell-base-depth: 2
[2019-05-05 04:44:50] [config] dec-cell-high-depth: 1
[2019-05-05 04:44:50] [config] dec-depth: 6
[2019-05-05 04:44:50] [config] devices:
[2019-05-05 04:44:50] [config]   - 0
[2019-05-05 04:44:50] [config] dim-emb: 512
[2019-05-05 04:44:50] [config] dim-rnn: 1024
[2019-05-05 04:44:50] [config] dim-vocabs:
[2019-05-05 04:44:50] [config]   - 30000
[2019-05-05 04:44:50] [config]   - 30000
[2019-05-05 04:44:50] [config] disp-first: 0
[2019-05-05 04:44:50] [config] disp-freq: 1000
[2019-05-05 04:44:50] [config] disp-label-counts: false
[2019-05-05 04:44:50] [config] dropout-rnn: 0
[2019-05-05 04:44:50] [config] dropout-src: 0
[2019-05-05 04:44:50] [config] dropout-trg: 0
[2019-05-05 04:44:50] [config] dump-config: ""
[2019-05-05 04:44:50] [config] early-stopping: 10
[2019-05-05 04:44:50] [config] embedding-fix-src: true
[2019-05-05 04:44:50] [config] embedding-fix-trg: true
[2019-05-05 04:44:50] [config] embedding-normalization: false
[2019-05-05 04:44:50] [config] embedding-vectors:
[2019-05-05 04:44:50] [config]   []
[2019-05-05 04:44:50] [config] enc-cell: gru
[2019-05-05 04:44:50] [config] enc-cell-depth: 1
[2019-05-05 04:44:50] [config] enc-depth: 6
[2019-05-05 04:44:50] [config] enc-type: bidirectional
[2019-05-05 04:44:50] [config] exponential-smoothing: 0.0001
[2019-05-05 04:44:50] [config] freeze: true
[2019-05-05 04:44:50] [config] grad-dropping-momentum: 0
[2019-05-05 04:44:50] [config] grad-dropping-rate: 0
[2019-05-05 04:44:50] [config] grad-dropping-warmup: 100
[2019-05-05 04:44:50] [config] guided-alignment: none
[2019-05-05 04:44:50] [config] guided-alignment-cost: mse
[2019-05-05 04:44:50] [config] guided-alignment-weight: 0.1
[2019-05-05 04:44:50] [config] ignore-model-config: false
[2019-05-05 04:44:50] [config] input-types:
[2019-05-05 04:44:50] [config]   []
[2019-05-05 04:44:50] [config] interpolate-env-vars: false
[2019-05-05 04:44:50] [config] keep-best: true
[2019-05-05 04:44:50] [config] label-smoothing: 0.1
[2019-05-05 04:44:50] [config] layer-normalization: false
[2019-05-05 04:44:50] [config] learn-rate: 0.0001
[2019-05-05 04:44:50] [config] log: model/train_trans.gate.log
[2019-05-05 04:44:50] [config] log-level: info
[2019-05-05 04:44:50] [config] log-time-zone: ""
[2019-05-05 04:44:50] [config] lr-decay: 0
[2019-05-05 04:44:50] [config] lr-decay-freq: 50000
[2019-05-05 04:44:50] [config] lr-decay-inv-sqrt:
[2019-05-05 04:44:50] [config]   - 16000
[2019-05-05 04:44:50] [config] lr-decay-repeat-warmup: false
[2019-05-05 04:44:50] [config] lr-decay-reset-optimizer: false
[2019-05-05 04:44:50] [config] lr-decay-start:
[2019-05-05 04:44:50] [config]   - 10
[2019-05-05 04:44:50] [config]   - 1
[2019-05-05 04:44:50] [config] lr-decay-strategy: epoch+stalled
[2019-05-05 04:44:50] [config] lr-report: true
[2019-05-05 04:44:50] [config] lr-warmup: 16000
[2019-05-05 04:44:50] [config] lr-warmup-at-reload: false
[2019-05-05 04:44:50] [config] lr-warmup-cycle: false
[2019-05-05 04:44:50] [config] lr-warmup-start-rate: 0
[2019-05-05 04:44:50] [config] max-length: 160
[2019-05-05 04:44:50] [config] max-length-crop: false
[2019-05-05 04:44:50] [config] max-length-factor: 3
[2019-05-05 04:44:50] [config] maxi-batch: 1000
[2019-05-05 04:44:50] [config] maxi-batch-sort: trg
[2019-05-05 04:44:50] [config] mini-batch: 1000
[2019-05-05 04:44:50] [config] mini-batch-fit: true
[2019-05-05 04:44:50] [config] mini-batch-fit-step: 10
[2019-05-05 04:44:50] [config] mini-batch-overstuff: 1
[2019-05-05 04:44:50] [config] mini-batch-track-lr: false
[2019-05-05 04:44:50] [config] mini-batch-understuff: 1
[2019-05-05 04:44:50] [config] mini-batch-warmup: 0
[2019-05-05 04:44:50] [config] mini-batch-words: 0
[2019-05-05 04:44:50] [config] mini-batch-words-ref: 0
[2019-05-05 04:44:50] [config] model: model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 04:44:50] [config] multi-loss-type: sum
[2019-05-05 04:44:50] [config] multi-node: false
[2019-05-05 04:44:50] [config] multi-node-overlap: true
[2019-05-05 04:44:50] [config] n-best: false
[2019-05-05 04:44:50] [config] no-nccl: true
[2019-05-05 04:44:50] [config] no-reload: false
[2019-05-05 04:44:50] [config] no-restore-corpus: true
[2019-05-05 04:44:50] [config] no-shuffle: false
[2019-05-05 04:44:50] [config] normalize: 0.6
[2019-05-05 04:44:50] [config] num-devices: 0
[2019-05-05 04:44:50] [config] optimizer: adam
[2019-05-05 04:44:50] [config] optimizer-delay: 4
[2019-05-05 04:44:50] [config] optimizer-params:
[2019-05-05 04:44:50] [config]   - 0.9
[2019-05-05 04:44:50] [config]   - 0.98
[2019-05-05 04:44:50] [config]   - 1e-09
[2019-05-05 04:44:50] [config] overwrite: false
[2019-05-05 04:44:50] [config] pretrained-model: model/model.src1tgt0.docnew.enc_depth.test_base.npz.best-perplexity.npz
[2019-05-05 04:44:50] [config] quiet: false
[2019-05-05 04:44:50] [config] quiet-translation: true
[2019-05-05 04:44:50] [config] relative-paths: false
[2019-05-05 04:44:50] [config] right-left: false
[2019-05-05 04:44:50] [config] save-freq: 5000
[2019-05-05 04:44:50] [config] seed: 1111
[2019-05-05 04:44:50] [config] shuffle-in-ram: false
[2019-05-05 04:44:50] [config] skip: false
[2019-05-05 04:44:50] [config] sqlite: ""
[2019-05-05 04:44:50] [config] sqlite-drop: false
[2019-05-05 04:44:50] [config] sync-sgd: true
[2019-05-05 04:44:50] [config] tempdir: /tmp
[2019-05-05 04:44:50] [config] tied-embeddings: false
[2019-05-05 04:44:50] [config] tied-embeddings-all: true
[2019-05-05 04:44:50] [config] tied-embeddings-src: false
[2019-05-05 04:44:50] [config] train-sets:
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.train.en.bpe.src_prev
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.train.en.bpe.src
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.train.cz.bpe
[2019-05-05 04:44:50] [config] transformer-aan-activation: swish
[2019-05-05 04:44:50] [config] transformer-aan-depth: 2
[2019-05-05 04:44:50] [config] transformer-aan-nogate: false
[2019-05-05 04:44:50] [config] transformer-decoder-autoreg: self-attention
[2019-05-05 04:44:50] [config] transformer-dim-aan: 2048
[2019-05-05 04:44:50] [config] transformer-dim-ffn: 2048
[2019-05-05 04:44:50] [config] transformer-dropout: 0.1
[2019-05-05 04:44:50] [config] transformer-dropout-attention: 0
[2019-05-05 04:44:50] [config] transformer-dropout-ffn: 0
[2019-05-05 04:44:50] [config] transformer-ffn-activation: swish
[2019-05-05 04:44:50] [config] transformer-ffn-depth: 2
[2019-05-05 04:44:50] [config] transformer-guided-alignment-layer: last
[2019-05-05 04:44:50] [config] transformer-heads: 8
[2019-05-05 04:44:50] [config] transformer-no-projection: false
[2019-05-05 04:44:50] [config] transformer-postprocess: dan
[2019-05-05 04:44:50] [config] transformer-postprocess-emb: d
[2019-05-05 04:44:50] [config] transformer-preprocess: ""
[2019-05-05 04:44:50] [config] transformer-tied-layers:
[2019-05-05 04:44:50] [config]   []
[2019-05-05 04:44:50] [config] transformer-train-position-embeddings: false
[2019-05-05 04:44:50] [config] type: transformer-context
[2019-05-05 04:44:50] [config] ulr: false
[2019-05-05 04:44:50] [config] ulr-dim-emb: 0
[2019-05-05 04:44:50] [config] ulr-dropout: 0
[2019-05-05 04:44:50] [config] ulr-keys-vectors: ""
[2019-05-05 04:44:50] [config] ulr-query-vectors: ""
[2019-05-05 04:44:50] [config] ulr-softmax-temperature: 1
[2019-05-05 04:44:50] [config] ulr-trainable-transformation: false
[2019-05-05 04:44:50] [config] valid-freq: 5000
[2019-05-05 04:44:50] [config] valid-log: model/valid_trans.gate.log
[2019-05-05 04:44:50] [config] valid-max-length: 1000
[2019-05-05 04:44:50] [config] valid-metrics:
[2019-05-05 04:44:50] [config]   - cross-entropy
[2019-05-05 04:44:50] [config]   - perplexity
[2019-05-05 04:44:50] [config]   - translation
[2019-05-05 04:44:50] [config] valid-mini-batch: 16
[2019-05-05 04:44:50] [config] valid-script-path: ./val.sh
[2019-05-05 04:44:50] [config] valid-sets:
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.dev.en.bpe.src_prev
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.dev.en.bpe.src
[2019-05-05 04:44:50] [config]   - corp/europarl.cs-en.docs.dev.cz.bpe
[2019-05-05 04:44:50] [config] valid-translation-output: data/valid.bpe.en.output
[2019-05-05 04:44:50] [config] vocabs:
[2019-05-05 04:44:50] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:50] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:50] [config]   - corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:50] [config] word-penalty: 0
[2019-05-05 04:44:50] [config] workspace: 7800
[2019-05-05 04:44:50] [config] Model is being created with Marian v1.7.8 1fcb013 2019-05-03 03:15:04 +0200
[2019-05-05 04:44:50] Using synchronous training
[2019-05-05 04:44:50] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:50] [data] Setting vocabulary size for input 0 to 30000
[2019-05-05 04:44:50] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:51] [data] Setting vocabulary size for input 1 to 30000
[2019-05-05 04:44:51] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-05-05 04:44:51] [data] Setting vocabulary size for input 2 to 30000
[2019-05-05 04:44:51] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-05-05 04:44:51] [batching] Collecting statistics for batch fitting with step size 10
[2019-05-05 04:44:51] [memory] Extending reserved space to 7808 MB (device gpu0)
[2019-05-05 04:44:55] [comm] NCCL communicator overridden
[2019-05-05 04:44:55] [training] Using 1 GPUs
[2019-05-05 04:44:55] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:44:55] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-05-05 04:44:55] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:45:01] [batching] Done. Typical MB size is 10522 target words
[2019-05-05 04:45:01] [memory] Extending reserved space to 7808 MB (device gpu0)
[2019-05-05 04:45:02] [comm] NCCL communicator overridden
[2019-05-05 04:45:02] [training] Using 1 GPUs
[2019-05-05 04:45:02] [training] Initializing model weights with the pre-trained model model/model.src1tgt0.docnew.enc_depth.test_base.npz.best-perplexity.npz
[2019-05-05 04:45:02] Loading model from model/model.src1tgt0.docnew.enc_depth.test_base.npz.best-perplexity.npz
[2019-05-05 04:45:02] Training started
[2019-05-05 04:45:02] [data] Shuffling data
[2019-05-05 04:45:02] [data] Done reading 620637 sentences
[2019-05-05 04:45:05] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 04:45:29] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-05-05 04:45:29] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:45:29] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:45:29] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:45:30] [memory] Reserving 311 MB, device gpu0
[2019-05-05 04:45:30] [memory] Reserving 622 MB, device gpu0
[2019-05-05 04:51:06] Ep. 1 : Up. 1000 : Sen. 167,546 : Cost 127.62656403 : Time 375.24s : 13682.08 words/s : L.r. 6.2500e-06
[2019-05-05 04:56:46] Ep. 1 : Up. 2000 : Sen. 334,200 : Cost 76.82872009 : Time 340.05s : 15398.38 words/s : L.r. 1.2500e-05
[2019-05-05 05:02:15] Ep. 1 : Up. 3000 : Sen. 501,391 : Cost 68.08705139 : Time 329.00s : 15077.24 words/s : L.r. 1.8750e-05
[2019-05-05 05:06:15] Seen 620307 samples
[2019-05-05 05:06:15] Starting epoch 2
[2019-05-05 05:06:15] [data] Shuffling data
[2019-05-05 05:06:16] [data] Done reading 620637 sentences
[2019-05-05 05:06:18] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 05:08:16] Ep. 2 : Up. 4000 : Sen. 49,779 : Cost 68.27506256 : Time 361.29s : 14152.11 words/s : L.r. 2.5000e-05
[2019-05-05 05:13:49] Ep. 2 : Up. 5000 : Sen. 213,785 : Cost 68.81875610 : Time 332.86s : 15209.79 words/s : L.r. 3.1250e-05
[2019-05-05 05:13:49] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 05:13:54] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter5000.npz
[2019-05-05 05:13:57] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 05:14:01] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 05:14:20] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 05:14:24] [valid] Ep. 2 : Up. 5000 : cross-entropy : 44.1728 : new best
[2019-05-05 05:14:33] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 05:14:37] [valid] Ep. 2 : Up. 5000 : perplexity : 4.16598 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 05:17:55] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-translation.npz
[2019-05-05 05:17:59] [valid] Ep. 2 : Up. 5000 : translation : 29.4 : new best
[2019-05-05 05:23:34] Ep. 2 : Up. 6000 : Sen. 379,439 : Cost 68.01039886 : Time 584.49s : 8723.17 words/s : L.r. 3.7500e-05
[2019-05-05 05:29:12] Ep. 2 : Up. 7000 : Sen. 546,844 : Cost 68.23215485 : Time 338.02s : 15380.94 words/s : L.r. 4.3750e-05
[2019-05-05 05:31:35] Seen 620307 samples
[2019-05-05 05:31:35] Starting epoch 3
[2019-05-05 05:31:35] [data] Shuffling data
[2019-05-05 05:31:35] [data] Done reading 620637 sentences
[2019-05-05 05:31:37] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 05:35:09] Ep. 3 : Up. 8000 : Sen. 91,757 : Cost 66.78676605 : Time 356.87s : 14154.26 words/s : L.r. 5.0000e-05
[2019-05-05 05:40:46] Ep. 3 : Up. 9000 : Sen. 260,900 : Cost 66.62432098 : Time 337.83s : 15359.39 words/s : L.r. 5.6250e-05
[2019-05-05 05:46:22] Ep. 3 : Up. 10000 : Sen. 427,430 : Cost 66.92862701 : Time 336.08s : 15321.03 words/s : L.r. 6.2500e-05
[2019-05-05 05:46:22] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 05:46:27] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter10000.npz
[2019-05-05 05:46:31] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 05:46:35] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 05:46:52] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 05:46:56] [valid] Ep. 3 : Up. 10000 : cross-entropy : 42.9999 : new best
[2019-05-05 05:47:06] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 05:47:10] [valid] Ep. 3 : Up. 10000 : perplexity : 4.01108 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 05:50:27] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-translation.npz
[2019-05-05 05:50:31] [valid] Ep. 3 : Up. 10000 : translation : 30 : new best
[2019-05-05 05:56:05] Ep. 3 : Up. 11000 : Sen. 595,769 : Cost 64.89412689 : Time 582.73s : 8682.16 words/s : L.r. 6.8750e-05
[2019-05-05 05:56:54] Seen 620307 samples
[2019-05-05 05:56:54] Starting epoch 4
[2019-05-05 05:56:54] [data] Shuffling data
[2019-05-05 05:56:54] [data] Done reading 620637 sentences
[2019-05-05 05:56:56] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 06:02:05] Ep. 4 : Up. 12000 : Sen. 142,052 : Cost 66.00769806 : Time 359.77s : 14232.86 words/s : L.r. 7.5000e-05
[2019-05-05 06:07:38] Ep. 4 : Up. 13000 : Sen. 305,591 : Cost 66.44052124 : Time 333.27s : 15191.41 words/s : L.r. 8.1250e-05
[2019-05-05 06:13:10] Ep. 4 : Up. 14000 : Sen. 473,021 : Cost 63.85665131 : Time 331.48s : 15081.42 words/s : L.r. 8.7500e-05
[2019-05-05 06:18:03] Seen 620307 samples
[2019-05-05 06:18:03] Starting epoch 5
[2019-05-05 06:18:03] [data] Shuffling data
[2019-05-05 06:18:04] [data] Done reading 620637 sentences
[2019-05-05 06:18:06] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 06:19:14] Ep. 5 : Up. 15000 : Sen. 22,474 : Cost 65.57188416 : Time 364.20s : 14317.98 words/s : L.r. 9.3750e-05
[2019-05-05 06:19:14] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 06:19:18] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter15000.npz
[2019-05-05 06:19:22] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 06:19:27] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 06:19:44] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 06:19:49] [valid] Ep. 5 : Up. 15000 : cross-entropy : 42.4773 : new best
[2019-05-05 06:19:58] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 06:20:02] [valid] Ep. 5 : Up. 15000 : perplexity : 3.94394 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 06:23:19] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-translation.npz
[2019-05-05 06:23:23] [valid] Ep. 5 : Up. 15000 : translation : 30.1 : new best
[2019-05-05 06:28:59] Ep. 5 : Up. 16000 : Sen. 192,080 : Cost 63.98476410 : Time 584.83s : 8732.13 words/s : L.r. 1.0000e-04
[2019-05-05 06:34:36] Ep. 5 : Up. 17000 : Sen. 355,247 : Cost 67.99659729 : Time 336.83s : 15437.94 words/s : L.r. 9.7014e-05
[2019-05-05 06:40:10] Ep. 5 : Up. 18000 : Sen. 525,197 : Cost 63.36363220 : Time 334.38s : 15156.30 words/s : L.r. 9.4281e-05
[2019-05-05 06:43:22] Seen 620307 samples
[2019-05-05 06:43:22] Starting epoch 6
[2019-05-05 06:43:22] [data] Shuffling data
[2019-05-05 06:43:22] [data] Done reading 620637 sentences
[2019-05-05 06:43:25] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 06:46:09] Ep. 6 : Up. 19000 : Sen. 70,823 : Cost 65.01346588 : Time 358.86s : 14197.80 words/s : L.r. 9.1766e-05
[2019-05-05 06:51:48] Ep. 6 : Up. 20000 : Sen. 239,902 : Cost 65.35044098 : Time 338.71s : 15391.79 words/s : L.r. 8.9443e-05
[2019-05-05 06:51:48] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 06:51:52] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter20000.npz
[2019-05-05 06:51:56] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 06:52:01] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 06:52:18] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 06:52:22] [valid] Ep. 6 : Up. 20000 : cross-entropy : 42.2276 : new best
[2019-05-05 06:52:32] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 06:52:36] [valid] Ep. 6 : Up. 20000 : perplexity : 3.91226 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 06:55:53] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-translation.npz
[2019-05-05 06:55:58] [valid] Ep. 6 : Up. 20000 : translation : 30.2 : new best
[2019-05-05 07:01:29] Ep. 6 : Up. 21000 : Sen. 403,884 : Cost 64.75838470 : Time 581.70s : 8625.04 words/s : L.r. 8.7287e-05
[2019-05-05 07:07:04] Ep. 6 : Up. 22000 : Sen. 568,263 : Cost 65.53887939 : Time 335.03s : 15209.72 words/s : L.r. 8.5280e-05
[2019-05-05 07:08:43] Seen 620307 samples
[2019-05-05 07:08:43] Starting epoch 7
[2019-05-05 07:08:43] [data] Shuffling data
[2019-05-05 07:08:43] [data] Done reading 620637 sentences
[2019-05-05 07:08:45] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 07:12:56] Ep. 7 : Up. 23000 : Sen. 116,382 : Cost 61.38735199 : Time 351.74s : 13944.97 words/s : L.r. 8.3406e-05
[2019-05-05 07:18:33] Ep. 7 : Up. 24000 : Sen. 282,546 : Cost 65.56448364 : Time 336.85s : 15320.38 words/s : L.r. 8.1650e-05
[2019-05-05 07:24:18] Ep. 7 : Up. 25000 : Sen. 450,972 : Cost 66.74272919 : Time 344.85s : 15454.95 words/s : L.r. 8.0000e-05
[2019-05-05 07:24:18] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 07:24:22] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter25000.npz
[2019-05-05 07:24:26] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 07:24:30] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 07:24:48] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 07:24:52] [valid] Ep. 7 : Up. 25000 : cross-entropy : 42.1584 : new best
[2019-05-05 07:25:02] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 07:25:06] [valid] Ep. 7 : Up. 25000 : perplexity : 3.90352 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 07:28:24] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-translation.npz
[2019-05-05 07:28:28] [valid] Ep. 7 : Up. 25000 : translation : 30.3 : new best
[2019-05-05 07:33:59] Ep. 7 : Up. 26000 : Sen. 617,713 : Cost 63.58931351 : Time 581.75s : 8638.62 words/s : L.r. 7.8446e-05
[2019-05-05 07:34:04] Seen 620307 samples
[2019-05-05 07:34:04] Starting epoch 8
[2019-05-05 07:34:04] [data] Shuffling data
[2019-05-05 07:34:04] [data] Done reading 620637 sentences
[2019-05-05 07:34:06] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 07:40:01] Ep. 8 : Up. 27000 : Sen. 166,388 : Cost 64.20169067 : Time 361.63s : 14270.59 words/s : L.r. 7.6980e-05
[2019-05-05 07:45:31] Ep. 8 : Up. 28000 : Sen. 334,055 : Cost 62.23682022 : Time 329.94s : 15032.76 words/s : L.r. 7.5593e-05
[2019-05-05 07:51:09] Ep. 8 : Up. 29000 : Sen. 500,476 : Cost 65.38902283 : Time 337.69s : 15297.05 words/s : L.r. 7.4278e-05
[2019-05-05 07:55:15] Seen 620307 samples
[2019-05-05 07:55:15] Starting epoch 9
[2019-05-05 07:55:15] [data] Shuffling data
[2019-05-05 07:55:15] [data] Done reading 620637 sentences
[2019-05-05 07:55:17] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 07:57:10] Ep. 9 : Up. 30000 : Sen. 43,181 : Cost 66.50164795 : Time 361.03s : 14272.32 words/s : L.r. 7.3030e-05
[2019-05-05 07:57:10] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 07:57:14] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter30000.npz
[2019-05-05 07:57:18] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 07:57:22] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 07:57:40] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-cross-entropy.npz
[2019-05-05 07:57:44] [valid] Ep. 9 : Up. 30000 : cross-entropy : 42.1333 : new best
[2019-05-05 07:57:54] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.best-perplexity.npz
[2019-05-05 07:57:58] [valid] Ep. 9 : Up. 30000 : perplexity : 3.90036 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 08:01:16] [valid] Ep. 9 : Up. 30000 : translation : 30.2 : stalled 1 times (last best: 30.3)
[2019-05-05 08:06:50] Ep. 9 : Up. 31000 : Sen. 212,196 : Cost 63.38725662 : Time 580.66s : 8779.91 words/s : L.r. 7.1842e-05
[2019-05-05 08:12:27] Ep. 9 : Up. 32000 : Sen. 375,331 : Cost 66.18612671 : Time 336.36s : 15280.11 words/s : L.r. 7.0711e-05
[2019-05-05 08:18:03] Ep. 9 : Up. 33000 : Sen. 547,191 : Cost 62.48423767 : Time 336.08s : 15215.78 words/s : L.r. 6.9631e-05
[2019-05-05 08:20:31] Seen 620307 samples
[2019-05-05 08:20:31] Starting epoch 10
[2019-05-05 08:20:31] [data] Shuffling data
[2019-05-05 08:20:31] [data] Done reading 620637 sentences
[2019-05-05 08:20:33] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 08:24:01] Ep. 10 : Up. 34000 : Sen. 85,529 : Cost 67.18856049 : Time 358.11s : 14177.77 words/s : L.r. 6.8599e-05
[2019-05-05 08:29:34] Ep. 10 : Up. 35000 : Sen. 252,351 : Cost 63.39818954 : Time 333.16s : 15158.52 words/s : L.r. 6.7612e-05
[2019-05-05 08:29:34] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 08:29:38] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter35000.npz
[2019-05-05 08:29:42] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 08:29:47] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 08:30:04] [valid] Ep. 10 : Up. 35000 : cross-entropy : 42.1571 : stalled 1 times (last best: 42.1333)
[2019-05-05 08:30:14] [valid] Ep. 10 : Up. 35000 : perplexity : 3.90336 : stalled 1 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 08:33:31] [valid] Ep. 10 : Up. 35000 : translation : 30.2 : stalled 2 times (last best: 30.3)
[2019-05-05 08:39:04] Ep. 10 : Up. 36000 : Sen. 420,048 : Cost 63.03988266 : Time 569.74s : 8851.34 words/s : L.r. 6.6667e-05
[2019-05-05 08:44:43] Ep. 10 : Up. 37000 : Sen. 592,431 : Cost 63.48221588 : Time 339.11s : 15386.72 words/s : L.r. 6.5760e-05
[2019-05-05 08:45:39] Seen 620307 samples
[2019-05-05 08:45:39] Starting epoch 11
[2019-05-05 08:45:39] [data] Shuffling data
[2019-05-05 08:45:39] [data] Done reading 620637 sentences
[2019-05-05 08:45:41] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 08:50:42] Ep. 11 : Up. 38000 : Sen. 136,379 : Cost 65.08442688 : Time 359.30s : 14182.95 words/s : L.r. 6.4889e-05
[2019-05-05 08:56:19] Ep. 11 : Up. 39000 : Sen. 304,216 : Cost 64.28245544 : Time 336.60s : 15312.93 words/s : L.r. 6.4051e-05
[2019-05-05 09:01:57] Ep. 11 : Up. 40000 : Sen. 468,399 : Cost 65.75958252 : Time 338.11s : 15253.93 words/s : L.r. 6.3246e-05
[2019-05-05 09:01:57] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 09:02:01] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter40000.npz
[2019-05-05 09:02:05] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 09:02:09] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 09:02:27] [valid] Ep. 11 : Up. 40000 : cross-entropy : 42.1827 : stalled 2 times (last best: 42.1333)
[2019-05-05 09:02:36] [valid] Ep. 11 : Up. 40000 : perplexity : 3.90659 : stalled 2 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 09:05:54] [valid] Ep. 11 : Up. 40000 : translation : 30.2 : stalled 3 times (last best: 30.3)
[2019-05-05 09:10:47] Seen 620307 samples
[2019-05-05 09:10:47] Starting epoch 12
[2019-05-05 09:10:47] [data] Shuffling data
[2019-05-05 09:10:47] [data] Done reading 620637 sentences
[2019-05-05 09:10:49] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 09:11:54] Ep. 12 : Up. 41000 : Sen. 22,616 : Cost 60.97906494 : Time 597.30s : 8515.61 words/s : L.r. 6.2470e-05
[2019-05-05 09:17:27] Ep. 12 : Up. 42000 : Sen. 192,464 : Cost 61.94905853 : Time 332.45s : 15148.60 words/s : L.r. 6.1721e-05
[2019-05-05 09:22:59] Ep. 12 : Up. 43000 : Sen. 347,689 : Cost 68.06147003 : Time 331.93s : 15196.95 words/s : L.r. 6.0999e-05
[2019-05-05 09:28:36] Ep. 12 : Up. 44000 : Sen. 517,202 : Cost 63.67607117 : Time 337.30s : 15298.64 words/s : L.r. 6.0302e-05
[2019-05-05 09:31:58] Seen 620307 samples
[2019-05-05 09:31:58] Starting epoch 13
[2019-05-05 09:31:58] [data] Shuffling data
[2019-05-05 09:31:58] [data] Done reading 620637 sentences
[2019-05-05 09:32:00] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 09:34:44] Ep. 13 : Up. 45000 : Sen. 71,866 : Cost 63.12531662 : Time 368.20s : 14350.89 words/s : L.r. 5.9628e-05
[2019-05-05 09:34:44] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 09:34:48] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter45000.npz
[2019-05-05 09:34:52] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 09:34:57] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 09:35:14] [valid] Ep. 13 : Up. 45000 : cross-entropy : 42.2206 : stalled 3 times (last best: 42.1333)
[2019-05-05 09:35:23] [valid] Ep. 13 : Up. 45000 : perplexity : 3.91137 : stalled 3 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 09:38:41] [valid] Ep. 13 : Up. 45000 : translation : 30.1 : stalled 4 times (last best: 30.3)
[2019-05-05 09:44:18] Ep. 13 : Up. 46000 : Sen. 243,546 : Cost 62.83124542 : Time 574.13s : 9000.31 words/s : L.r. 5.8977e-05
[2019-05-05 09:49:51] Ep. 13 : Up. 47000 : Sen. 406,466 : Cost 64.98061371 : Time 333.19s : 15184.23 words/s : L.r. 5.8346e-05
[2019-05-05 09:55:23] Ep. 13 : Up. 48000 : Sen. 569,304 : Cost 64.39879608 : Time 331.29s : 15139.22 words/s : L.r. 5.7735e-05
[2019-05-05 09:57:06] Seen 620307 samples
[2019-05-05 09:57:06] Starting epoch 14
[2019-05-05 09:57:06] [data] Shuffling data
[2019-05-05 09:57:06] [data] Done reading 620637 sentences
[2019-05-05 09:57:08] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 10:01:24] Ep. 14 : Up. 49000 : Sen. 115,950 : Cost 64.00193787 : Time 361.26s : 14182.02 words/s : L.r. 5.7143e-05
[2019-05-05 10:06:57] Ep. 14 : Up. 50000 : Sen. 284,247 : Cost 62.66732788 : Time 332.76s : 15195.45 words/s : L.r. 5.6569e-05
[2019-05-05 10:06:57] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 10:07:01] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter50000.npz
[2019-05-05 10:07:05] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 10:07:09] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 10:07:27] [valid] Ep. 14 : Up. 50000 : cross-entropy : 42.267 : stalled 4 times (last best: 42.1333)
[2019-05-05 10:07:36] [valid] Ep. 14 : Up. 50000 : perplexity : 3.91723 : stalled 4 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 10:10:54] [valid] Ep. 14 : Up. 50000 : translation : 30.2 : stalled 5 times (last best: 30.3)
[2019-05-05 10:16:30] Ep. 14 : Up. 51000 : Sen. 452,843 : Cost 63.80247498 : Time 573.59s : 8974.73 words/s : L.r. 5.6011e-05
[2019-05-05 10:22:04] Ep. 14 : Up. 52000 : Sen. 616,472 : Cost 65.11272430 : Time 333.66s : 15263.67 words/s : L.r. 5.5470e-05
[2019-05-05 10:22:13] Seen 620307 samples
[2019-05-05 10:22:13] Starting epoch 15
[2019-05-05 10:22:13] [data] Shuffling data
[2019-05-05 10:22:13] [data] Done reading 620637 sentences
[2019-05-05 10:22:15] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 10:28:00] Ep. 15 : Up. 53000 : Sen. 160,095 : Cost 63.81868362 : Time 356.47s : 14071.50 words/s : L.r. 5.4944e-05
[2019-05-05 10:33:32] Ep. 15 : Up. 54000 : Sen. 319,298 : Cost 65.95953369 : Time 331.02s : 15215.51 words/s : L.r. 5.4433e-05
[2019-05-05 10:39:13] Ep. 15 : Up. 55000 : Sen. 491,558 : Cost 63.60012817 : Time 341.21s : 15382.46 words/s : L.r. 5.3936e-05
[2019-05-05 10:39:13] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 10:39:17] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter55000.npz
[2019-05-05 10:39:21] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 10:39:25] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 10:39:43] [valid] Ep. 15 : Up. 55000 : cross-entropy : 42.3094 : stalled 5 times (last best: 42.1333)
[2019-05-05 10:39:52] [valid] Ep. 15 : Up. 55000 : perplexity : 3.92261 : stalled 5 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 10:43:10] [valid] Ep. 15 : Up. 55000 : translation : 30.2 : stalled 6 times (last best: 30.3)
[2019-05-05 10:47:19] Seen 620307 samples
[2019-05-05 10:47:19] Starting epoch 16
[2019-05-05 10:47:19] [data] Shuffling data
[2019-05-05 10:47:19] [data] Done reading 620637 sentences
[2019-05-05 10:47:21] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 10:49:14] Ep. 16 : Up. 56000 : Sen. 45,532 : Cost 62.87469482 : Time 601.45s : 8740.75 words/s : L.r. 5.3452e-05
[2019-05-05 10:54:40] Ep. 16 : Up. 57000 : Sen. 204,477 : Cost 64.30073547 : Time 325.54s : 15043.43 words/s : L.r. 5.2981e-05
[2019-05-05 11:00:24] Ep. 16 : Up. 58000 : Sen. 378,857 : Cost 63.44490051 : Time 343.99s : 15443.78 words/s : L.r. 5.2523e-05
[2019-05-05 11:05:54] Ep. 16 : Up. 59000 : Sen. 541,481 : Cost 63.88314056 : Time 329.86s : 15113.74 words/s : L.r. 5.2076e-05
[2019-05-05 11:08:29] Seen 620307 samples
[2019-05-05 11:08:29] Starting epoch 17
[2019-05-05 11:08:29] [data] Shuffling data
[2019-05-05 11:08:29] [data] Done reading 620637 sentences
[2019-05-05 11:08:31] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 11:11:59] Ep. 17 : Up. 60000 : Sen. 92,377 : Cost 63.01214218 : Time 365.41s : 14193.29 words/s : L.r. 5.1640e-05
[2019-05-05 11:11:59] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 11:12:03] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter60000.npz
[2019-05-05 11:12:07] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 11:12:11] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 11:12:29] [valid] Ep. 17 : Up. 60000 : cross-entropy : 42.3551 : stalled 6 times (last best: 42.1333)
[2019-05-05 11:12:38] [valid] Ep. 17 : Up. 60000 : perplexity : 3.9284 : stalled 6 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 11:15:56] [valid] Ep. 17 : Up. 60000 : translation : 30.1 : stalled 7 times (last best: 30.3)
[2019-05-05 11:21:33] Ep. 17 : Up. 61000 : Sen. 263,122 : Cost 62.74321365 : Time 573.58s : 8978.20 words/s : L.r. 5.1215e-05
[2019-05-05 11:27:08] Ep. 17 : Up. 62000 : Sen. 428,023 : Cost 64.77113342 : Time 335.68s : 15259.67 words/s : L.r. 5.0800e-05
[2019-05-05 11:32:43] Ep. 17 : Up. 63000 : Sen. 595,596 : Cost 63.48667908 : Time 334.42s : 15270.02 words/s : L.r. 5.0395e-05
[2019-05-05 11:33:37] Seen 620307 samples
[2019-05-05 11:33:37] Starting epoch 18
[2019-05-05 11:33:37] [data] Shuffling data
[2019-05-05 11:33:37] [data] Done reading 620637 sentences
[2019-05-05 11:33:39] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 11:38:43] Ep. 18 : Up. 64000 : Sen. 142,022 : Cost 63.77975082 : Time 360.62s : 14174.57 words/s : L.r. 5.0000e-05
[2019-05-05 11:44:17] Ep. 18 : Up. 65000 : Sen. 312,283 : Cost 61.64128494 : Time 333.30s : 15169.58 words/s : L.r. 4.9614e-05
[2019-05-05 11:44:17] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.orig.npz
[2019-05-05 11:44:21] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.iter65000.npz
[2019-05-05 11:44:25] Saving model weights and runtime parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz
[2019-05-05 11:44:29] Saving Adam parameters to model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz.optimizer.npz
[2019-05-05 11:44:47] [valid] Ep. 18 : Up. 65000 : cross-entropy : 42.4013 : stalled 7 times (last best: 42.1333)
[2019-05-05 11:44:56] [valid] Ep. 18 : Up. 65000 : perplexity : 3.93427 : stalled 7 times (last best: 3.90036)
Detokenizer Version $Revision: 4134 $
Language: en
[2019-05-05 11:48:13] [valid] Ep. 18 : Up. 65000 : translation : 30.1 : stalled 8 times (last best: 30.3)
[2019-05-05 11:53:43] Ep. 18 : Up. 66000 : Sen. 475,276 : Cost 64.24282837 : Time 566.73s : 8862.94 words/s : L.r. 4.9237e-05
[2019-05-05 11:58:43] Seen 620307 samples
[2019-05-05 11:58:43] Starting epoch 19
[2019-05-05 11:58:43] [data] Shuffling data
[2019-05-05 11:58:43] [data] Done reading 620637 sentences
[2019-05-05 11:58:45] [data] Done shuffling 620637 sentences to temp files
[2019-05-05 11:59:45] Ep. 19 : Up. 67000 : Sen. 18,593 : Cost 65.50408173 : Time 361.67s : 14238.14 words/s : L.r. 4.8868e-05
[2019-05-05 12:05:16] Ep. 19 : Up. 68000 : Sen. 179,454 : Cost 64.90951538 : Time 331.11s : 15181.78 words/s : L.r. 4.8507e-05
[2019-05-05 12:10:54] Ep. 19 : Up. 69000 : Sen. 352,809 : Cost 61.72978973 : Time 337.98s : 15222.42 words/s : L.r. 4.8154e-05
train_doc_docnew_enc_depth_pretrain_freeze.last.sh: řádek 29:  9995 Ukončen (SIGTERM)      $marian_home/marian --model model/model.src1tgt0.from_new_pretrained.context-enc-depth1.last2.npz --pretrained-model model/model.src1tgt0.docnew.enc_depth.test_base.npz.best-perplexity.npz --type transformer-context --train-sets corp/europarl.cs-en.docs.train.en.bpe.src_prev corp/europarl.cs-en.docs.train.en.bpe.src corp/europarl.cs-en.docs.train.cz.bpe --max-length 160 --dim-vocabs 30000 30000 --embedding-fix-src --embedding-fix-trg --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 7800 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --freeze --context-enc-depth 1 --valid-freq 5000 --save-freq 5000 --disp-freq 1000 --valid-metrics cross-entropy perplexity translation --valid-sets corp/europarl.cs-en.docs.dev.en.bpe.src_prev corp/europarl.cs-en.docs.dev.en.bpe.src corp/europarl.cs-en.docs.dev.cz.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 16 --keep-best --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --no-nccl --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
