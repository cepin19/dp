[2019-03-31 01:22:42] [marian] Marian v1.7.8 7c8ebcb 2019-03-31 01:16:45 +0100
[2019-03-31 01:22:42] [marian] Running on pcknot5 as process 3935 with command line:
[2019-03-31 01:22:42] [marian] /mnt/minerva1/nlp/projects/nmt/doc-marian/build/marian --model model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz --pretrained-model ../src0tgt0/model/model.src0tgt0_to_tgt0.newbase.160.npz.best-perplexity.npz --type transformer-context --train-sets corp/europarl.cs-en.docs.train.en.bpe.src_prev corp/europarl.cs-en.docs.train.en.bpe.src corp/europarl.cs-en.docs.train.cz.bpe --max-length 80 --dim-vocabs 30000 30000 --embedding-fix-src --embedding-fix-trg --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 7800 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --freeze --valid-freq 3000 --save-freq 3000 --disp-freq 1000 --valid-metrics cross-entropy perplexity translation --valid-sets corp/europarl.cs-en.docs.dev.en.bpe.src_prev corp/europarl.cs-en.docs.dev.en.bpe.src corp/europarl.cs-en.docs.dev.cz.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 16 --keep-best --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --no-nccl --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
[2019-03-31 01:22:42] [config] after-batches: 0
[2019-03-31 01:22:42] [config] after-epochs: 0
[2019-03-31 01:22:42] [config] allow-unk: false
[2019-03-31 01:22:42] [config] beam-size: 6
[2019-03-31 01:22:42] [config] bert-class-symbol: "[CLS]"
[2019-03-31 01:22:42] [config] bert-mask-symbol: "[MASK]"
[2019-03-31 01:22:42] [config] bert-masking-fraction: 0.15
[2019-03-31 01:22:42] [config] bert-sep-symbol: "[SEP]"
[2019-03-31 01:22:42] [config] bert-train-type-embeddings: true
[2019-03-31 01:22:42] [config] bert-type-vocab-size: 2
[2019-03-31 01:22:42] [config] best-deep: false
[2019-03-31 01:22:42] [config] clip-gemm: 0
[2019-03-31 01:22:42] [config] clip-norm: 5
[2019-03-31 01:22:42] [config] context-enc-depth: 1
[2019-03-31 01:22:42] [config] cost-type: ce-mean
[2019-03-31 01:22:42] [config] cpu-threads: 0
[2019-03-31 01:22:42] [config] data-weighting: ""
[2019-03-31 01:22:42] [config] data-weighting-type: sentence
[2019-03-31 01:22:42] [config] dec-cell: gru
[2019-03-31 01:22:42] [config] dec-cell-base-depth: 2
[2019-03-31 01:22:42] [config] dec-cell-high-depth: 1
[2019-03-31 01:22:42] [config] dec-depth: 6
[2019-03-31 01:22:42] [config] devices:
[2019-03-31 01:22:42] [config]   - 0
[2019-03-31 01:22:42] [config] dim-emb: 512
[2019-03-31 01:22:42] [config] dim-rnn: 1024
[2019-03-31 01:22:42] [config] dim-vocabs:
[2019-03-31 01:22:42] [config]   - 30000
[2019-03-31 01:22:42] [config]   - 30000
[2019-03-31 01:22:42] [config] disp-first: 0
[2019-03-31 01:22:42] [config] disp-freq: 1000
[2019-03-31 01:22:42] [config] disp-label-counts: false
[2019-03-31 01:22:42] [config] dropout-rnn: 0
[2019-03-31 01:22:42] [config] dropout-src: 0
[2019-03-31 01:22:42] [config] dropout-trg: 0
[2019-03-31 01:22:42] [config] dump-config: ""
[2019-03-31 01:22:42] [config] early-stopping: 10
[2019-03-31 01:22:42] [config] embedding-fix-src: true
[2019-03-31 01:22:42] [config] embedding-fix-trg: true
[2019-03-31 01:22:42] [config] embedding-normalization: false
[2019-03-31 01:22:42] [config] embedding-vectors:
[2019-03-31 01:22:42] [config]   []
[2019-03-31 01:22:42] [config] enc-cell: gru
[2019-03-31 01:22:42] [config] enc-cell-depth: 1
[2019-03-31 01:22:42] [config] enc-depth: 6
[2019-03-31 01:22:42] [config] enc-type: bidirectional
[2019-03-31 01:22:42] [config] exponential-smoothing: 0.0001
[2019-03-31 01:22:42] [config] freeze: true
[2019-03-31 01:22:42] [config] grad-dropping-momentum: 0
[2019-03-31 01:22:42] [config] grad-dropping-rate: 0
[2019-03-31 01:22:42] [config] grad-dropping-warmup: 100
[2019-03-31 01:22:42] [config] guided-alignment: none
[2019-03-31 01:22:42] [config] guided-alignment-cost: mse
[2019-03-31 01:22:42] [config] guided-alignment-weight: 0.1
[2019-03-31 01:22:42] [config] hier-att: false
[2019-03-31 01:22:42] [config] ignore-model-config: false
[2019-03-31 01:22:42] [config] input-types:
[2019-03-31 01:22:42] [config]   []
[2019-03-31 01:22:42] [config] interpolate-env-vars: false
[2019-03-31 01:22:42] [config] keep-best: true
[2019-03-31 01:22:42] [config] label-smoothing: 0.1
[2019-03-31 01:22:42] [config] layer-normalization: false
[2019-03-31 01:22:42] [config] learn-rate: 0.0001
[2019-03-31 01:22:42] [config] log: model/train_trans.gate.log
[2019-03-31 01:22:42] [config] log-level: info
[2019-03-31 01:22:42] [config] log-time-zone: ""
[2019-03-31 01:22:42] [config] lr-decay: 0
[2019-03-31 01:22:42] [config] lr-decay-freq: 50000
[2019-03-31 01:22:42] [config] lr-decay-inv-sqrt:
[2019-03-31 01:22:42] [config]   - 16000
[2019-03-31 01:22:42] [config] lr-decay-repeat-warmup: false
[2019-03-31 01:22:42] [config] lr-decay-reset-optimizer: false
[2019-03-31 01:22:42] [config] lr-decay-start:
[2019-03-31 01:22:42] [config]   - 10
[2019-03-31 01:22:42] [config]   - 1
[2019-03-31 01:22:42] [config] lr-decay-strategy: epoch+stalled
[2019-03-31 01:22:42] [config] lr-report: true
[2019-03-31 01:22:42] [config] lr-warmup: 16000
[2019-03-31 01:22:42] [config] lr-warmup-at-reload: false
[2019-03-31 01:22:42] [config] lr-warmup-cycle: false
[2019-03-31 01:22:42] [config] lr-warmup-start-rate: 0
[2019-03-31 01:22:42] [config] max-length: 80
[2019-03-31 01:22:42] [config] max-length-crop: false
[2019-03-31 01:22:42] [config] max-length-factor: 3
[2019-03-31 01:22:42] [config] maxi-batch: 1000
[2019-03-31 01:22:42] [config] maxi-batch-sort: trg
[2019-03-31 01:22:42] [config] mini-batch: 1000
[2019-03-31 01:22:42] [config] mini-batch-fit: true
[2019-03-31 01:22:42] [config] mini-batch-fit-step: 10
[2019-03-31 01:22:42] [config] mini-batch-overstuff: 1
[2019-03-31 01:22:42] [config] mini-batch-track-lr: false
[2019-03-31 01:22:42] [config] mini-batch-understuff: 1
[2019-03-31 01:22:42] [config] mini-batch-warmup: 0
[2019-03-31 01:22:42] [config] mini-batch-words: 0
[2019-03-31 01:22:42] [config] mini-batch-words-ref: 0
[2019-03-31 01:22:42] [config] model: model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 01:22:42] [config] multi-loss-type: sum
[2019-03-31 01:22:42] [config] multi-node: false
[2019-03-31 01:22:42] [config] multi-node-overlap: true
[2019-03-31 01:22:42] [config] n-best: false
[2019-03-31 01:22:42] [config] no-nccl: true
[2019-03-31 01:22:42] [config] no-reload: false
[2019-03-31 01:22:42] [config] no-restore-corpus: true
[2019-03-31 01:22:42] [config] no-shuffle: false
[2019-03-31 01:22:42] [config] normalize: 0.6
[2019-03-31 01:22:42] [config] num-devices: 0
[2019-03-31 01:22:42] [config] optimizer: adam
[2019-03-31 01:22:42] [config] optimizer-delay: 4
[2019-03-31 01:22:42] [config] optimizer-params:
[2019-03-31 01:22:42] [config]   - 0.9
[2019-03-31 01:22:42] [config]   - 0.98
[2019-03-31 01:22:42] [config]   - 1e-09
[2019-03-31 01:22:42] [config] overwrite: false
[2019-03-31 01:22:42] [config] pretrained-model: ../src0tgt0/model/model.src0tgt0_to_tgt0.newbase.160.npz.best-perplexity.npz
[2019-03-31 01:22:42] [config] quiet: false
[2019-03-31 01:22:42] [config] quiet-translation: true
[2019-03-31 01:22:42] [config] relative-paths: false
[2019-03-31 01:22:42] [config] right-left: false
[2019-03-31 01:22:42] [config] save-freq: 3000
[2019-03-31 01:22:42] [config] seed: 1111
[2019-03-31 01:22:42] [config] shuffle-in-ram: false
[2019-03-31 01:22:42] [config] skip: false
[2019-03-31 01:22:42] [config] sqlite: ""
[2019-03-31 01:22:42] [config] sqlite-drop: false
[2019-03-31 01:22:42] [config] sync-sgd: true
[2019-03-31 01:22:42] [config] tempdir: /tmp
[2019-03-31 01:22:42] [config] tied-embeddings: false
[2019-03-31 01:22:42] [config] tied-embeddings-all: true
[2019-03-31 01:22:42] [config] tied-embeddings-src: false
[2019-03-31 01:22:42] [config] train-sets:
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.train.en.bpe.src_prev
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.train.en.bpe.src
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.train.cz.bpe
[2019-03-31 01:22:42] [config] transformer-aan-activation: swish
[2019-03-31 01:22:42] [config] transformer-aan-depth: 2
[2019-03-31 01:22:42] [config] transformer-aan-nogate: false
[2019-03-31 01:22:42] [config] transformer-decoder-autoreg: self-attention
[2019-03-31 01:22:42] [config] transformer-dim-aan: 2048
[2019-03-31 01:22:42] [config] transformer-dim-ffn: 2048
[2019-03-31 01:22:42] [config] transformer-dropout: 0.1
[2019-03-31 01:22:42] [config] transformer-dropout-attention: 0
[2019-03-31 01:22:42] [config] transformer-dropout-ffn: 0
[2019-03-31 01:22:42] [config] transformer-ffn-activation: swish
[2019-03-31 01:22:42] [config] transformer-ffn-depth: 2
[2019-03-31 01:22:42] [config] transformer-guided-alignment-layer: last
[2019-03-31 01:22:42] [config] transformer-heads: 8
[2019-03-31 01:22:42] [config] transformer-no-projection: false
[2019-03-31 01:22:42] [config] transformer-postprocess: dan
[2019-03-31 01:22:42] [config] transformer-postprocess-emb: d
[2019-03-31 01:22:42] [config] transformer-preprocess: ""
[2019-03-31 01:22:42] [config] transformer-tied-layers:
[2019-03-31 01:22:42] [config]   []
[2019-03-31 01:22:42] [config] transformer-train-position-embeddings: false
[2019-03-31 01:22:42] [config] type: transformer-context
[2019-03-31 01:22:42] [config] ulr: false
[2019-03-31 01:22:42] [config] ulr-dim-emb: 0
[2019-03-31 01:22:42] [config] ulr-dropout: 0
[2019-03-31 01:22:42] [config] ulr-keys-vectors: ""
[2019-03-31 01:22:42] [config] ulr-query-vectors: ""
[2019-03-31 01:22:42] [config] ulr-softmax-temperature: 1
[2019-03-31 01:22:42] [config] ulr-trainable-transformation: false
[2019-03-31 01:22:42] [config] valid-freq: 3000
[2019-03-31 01:22:42] [config] valid-log: model/valid_trans.gate.log
[2019-03-31 01:22:42] [config] valid-max-length: 1000
[2019-03-31 01:22:42] [config] valid-metrics:
[2019-03-31 01:22:42] [config]   - cross-entropy
[2019-03-31 01:22:42] [config]   - perplexity
[2019-03-31 01:22:42] [config]   - translation
[2019-03-31 01:22:42] [config] valid-mini-batch: 16
[2019-03-31 01:22:42] [config] valid-script-path: ./val.sh
[2019-03-31 01:22:42] [config] valid-sets:
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.dev.en.bpe.src_prev
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.dev.en.bpe.src
[2019-03-31 01:22:42] [config]   - corp/europarl.cs-en.docs.dev.cz.bpe
[2019-03-31 01:22:42] [config] valid-translation-output: data/valid.bpe.en.output
[2019-03-31 01:22:42] [config] vocabs:
[2019-03-31 01:22:42] [config]   - corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [config]   - corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [config]   - corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [config] word-penalty: 0
[2019-03-31 01:22:42] [config] workspace: 7800
[2019-03-31 01:22:42] [config] Model is being created with Marian v1.7.8 7c8ebcb 2019-03-31 01:16:45 +0100
[2019-03-31 01:22:42] Using synchronous training
[2019-03-31 01:22:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [data] Setting vocabulary size for input 0 to 30000
[2019-03-31 01:22:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [data] Setting vocabulary size for input 1 to 30000
[2019-03-31 01:22:42] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encz.opensub.new.yml
[2019-03-31 01:22:42] [data] Setting vocabulary size for input 2 to 30000
[2019-03-31 01:22:42] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-03-31 01:22:42] [batching] Collecting statistics for batch fitting with step size 10
[2019-03-31 01:22:42] [memory] Extending reserved space to 7808 MB (device gpu0)
[2019-03-31 01:22:43] [comm] NCCL communicator overridden
[2019-03-31 01:22:43] [training] Using 1 GPUs
[2019-03-31 01:22:43] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:22:43] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-03-31 01:22:43] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:22:48] [batching] Done. Typical MB size is 15575 target words
[2019-03-31 01:22:48] [memory] Extending reserved space to 7808 MB (device gpu0)
[2019-03-31 01:22:48] [comm] NCCL communicator overridden
[2019-03-31 01:22:48] [training] Using 1 GPUs
[2019-03-31 01:22:48] [training] Initializing model weights with the pre-trained model ../src0tgt0/model/model.src0tgt0_to_tgt0.newbase.160.npz.best-perplexity.npz
[2019-03-31 01:22:48] Loading model from ../src0tgt0/model/model.src0tgt0_to_tgt0.newbase.160.npz.best-perplexity.npz
[2019-03-31 01:22:48] Training started
[2019-03-31 01:22:48] [data] Shuffling data
[2019-03-31 01:22:49] [data] Done reading 620637 sentences
[2019-03-31 01:22:51] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 01:23:13] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-03-31 01:23:13] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:23:14] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:23:14] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:23:14] [memory] Reserving 311 MB, device gpu0
[2019-03-31 01:23:14] [memory] Reserving 622 MB, device gpu0
[2019-03-31 01:30:44] Ep. 1 : Up. 1000 : Sen. 260,147 : Cost 266.12673950 : Time 482.43s : 16021.15 words/s : L.r. 6.2500e-06
[2019-03-31 01:38:13] Ep. 1 : Up. 2000 : Sen. 522,829 : Cost 243.64949036 : Time 448.62s : 17123.32 words/s : L.r. 1.2500e-05
[2019-03-31 01:40:29] Seen 601324 samples
[2019-03-31 01:40:29] Starting epoch 2
[2019-03-31 01:40:29] [data] Shuffling data
[2019-03-31 01:40:29] [data] Done reading 620637 sentences
[2019-03-31 01:40:31] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 01:46:03] Ep. 2 : Up. 3000 : Sen. 179,252 : Cost 240.64096069 : Time 469.59s : 16099.75 words/s : L.r. 1.8750e-05
[2019-03-31 01:46:03] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 01:46:07] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter3000.npz
[2019-03-31 01:46:12] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 01:46:16] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 01:46:35] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 01:46:39] [valid] Ep. 2 : Up. 3000 : cross-entropy : 242.47 : new best
[2019-03-31 01:46:49] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 01:46:53] [valid] Ep. 2 : Up. 3000 : perplexity : 2521.73 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 01:52:29] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 01:52:33] [valid] Ep. 2 : Up. 3000 : translation : 0.5 : new best
[2019-03-31 03:00:07] Ep. 2 : Up. 4000 : Sen. 447,831 : Cost 233.63128662 : Time 844.48s : 9275.08 words/s : L.r. 2.5000e-05
[2019-03-31 03:04:40] Seen 601324 samples
[2019-03-31 03:04:40] Starting epoch 3
[2019-03-31 03:04:40] [data] Shuffling data
[2019-03-31 03:04:41] [data] Done reading 620637 sentences
[2019-03-31 03:04:43] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 03:07:59] Ep. 3 : Up. 5000 : Sen. 103,741 : Cost 231.79739380 : Time 471.44s : 16173.15 words/s : L.r. 3.1250e-05
[2019-03-31 03:15:33] Ep. 3 : Up. 6000 : Sen. 364,035 : Cost 230.27526855 : Time 454.60s : 17257.46 words/s : L.r. 3.7500e-05
[2019-03-31 03:15:33] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 03:15:38] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter6000.npz
[2019-03-31 03:15:43] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 03:15:48] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 03:16:06] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 03:16:11] [valid] Ep. 3 : Up. 6000 : cross-entropy : 223.43 : new best
[2019-03-31 03:16:21] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 03:16:25] [valid] Ep. 3 : Up. 6000 : perplexity : 1363.26 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 03:21:07] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 03:21:12] [valid] Ep. 3 : Up. 6000 : translation : 2.5 : new best
[2019-03-31 03:27:59] Seen 601324 samples
[2019-03-31 03:27:59] Starting epoch 4
[2019-03-31 03:27:59] [data] Shuffling data
[2019-03-31 03:28:00] [data] Done reading 620637 sentences
[2019-03-31 03:28:02] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 03:29:05] Ep. 4 : Up. 7000 : Sen. 22,717 : Cost 219.43435669 : Time 811.68s : 9423.30 words/s : L.r. 4.3750e-05
[2019-03-31 03:36:29] Ep. 4 : Up. 8000 : Sen. 279,911 : Cost 215.23361206 : Time 444.22s : 17084.98 words/s : L.r. 5.0000e-05
[2019-03-31 03:43:58] Ep. 4 : Up. 9000 : Sen. 542,923 : Cost 208.88227844 : Time 448.71s : 17136.24 words/s : L.r. 5.6250e-05
[2019-03-31 03:43:58] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 03:44:02] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter9000.npz
[2019-03-31 03:44:07] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 03:44:12] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 03:44:31] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 03:44:37] [valid] Ep. 4 : Up. 9000 : cross-entropy : 206.72 : new best
[2019-03-31 03:44:46] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 03:44:52] [valid] Ep. 4 : Up. 9000 : perplexity : 794.598 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 03:48:56] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 03:49:02] [valid] Ep. 4 : Up. 9000 : translation : 3.4 : new best
[2019-03-31 03:50:43] Seen 601324 samples
[2019-03-31 03:50:43] Starting epoch 5
[2019-03-31 03:50:43] [data] Shuffling data
[2019-03-31 03:50:43] [data] Done reading 620637 sentences
[2019-03-31 03:50:45] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 03:57:00] Ep. 5 : Up. 10000 : Sen. 203,384 : Cost 208.74993896 : Time 781.96s : 9963.50 words/s : L.r. 6.2500e-05
[2019-03-31 04:04:30] Ep. 5 : Up. 11000 : Sen. 464,518 : Cost 202.93646240 : Time 449.90s : 17126.90 words/s : L.r. 6.8750e-05
[2019-03-31 04:08:22] Seen 601324 samples
[2019-03-31 04:08:22] Starting epoch 6
[2019-03-31 04:08:22] [data] Shuffling data
[2019-03-31 04:08:22] [data] Done reading 620637 sentences
[2019-03-31 04:08:25] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 04:12:20] Ep. 6 : Up. 12000 : Sen. 123,379 : Cost 199.09545898 : Time 470.30s : 16287.85 words/s : L.r. 7.5000e-05
[2019-03-31 04:12:20] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 04:12:24] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter12000.npz
[2019-03-31 04:12:29] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 04:12:34] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 04:12:52] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 04:12:57] [valid] Ep. 6 : Up. 12000 : cross-entropy : 193.8 : new best
[2019-03-31 04:13:06] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 04:13:11] [valid] Ep. 6 : Up. 12000 : perplexity : 523.475 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 04:17:15] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 04:17:21] [valid] Ep. 6 : Up. 12000 : translation : 4.2 : new best
[2019-03-31 04:24:47] Ep. 6 : Up. 13000 : Sen. 383,224 : Cost 195.53715515 : Time 747.13s : 10220.35 words/s : L.r. 8.1250e-05
[2019-03-31 04:31:02] Seen 601324 samples
[2019-03-31 04:31:02] Starting epoch 7
[2019-03-31 04:31:02] [data] Shuffling data
[2019-03-31 04:31:02] [data] Done reading 620637 sentences
[2019-03-31 04:31:05] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 04:32:44] Ep. 7 : Up. 14000 : Sen. 45,013 : Cost 193.33850098 : Time 476.93s : 16269.57 words/s : L.r. 8.7500e-05
[2019-03-31 04:40:09] Ep. 7 : Up. 15000 : Sen. 304,804 : Cost 189.50547791 : Time 444.66s : 17146.10 words/s : L.r. 9.3750e-05
[2019-03-31 04:40:09] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 04:40:14] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter15000.npz
[2019-03-31 04:40:18] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 04:40:23] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 04:40:41] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 04:40:46] [valid] Ep. 7 : Up. 15000 : cross-entropy : 183.489 : new best
[2019-03-31 04:40:56] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 04:41:00] [valid] Ep. 7 : Up. 15000 : perplexity : 375.176 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 04:45:02] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 04:45:07] [valid] Ep. 7 : Up. 15000 : translation : 4.7 : new best
[2019-03-31 04:52:38] Ep. 7 : Up. 16000 : Sen. 565,316 : Cost 189.15103149 : Time 749.45s : 10297.15 words/s : L.r. 1.0000e-04
[2019-03-31 04:53:40] Seen 601324 samples
[2019-03-31 04:53:40] Starting epoch 8
[2019-03-31 04:53:40] [data] Shuffling data
[2019-03-31 04:53:40] [data] Done reading 620637 sentences
[2019-03-31 04:53:42] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 05:00:31] Ep. 8 : Up. 17000 : Sen. 224,590 : Cost 184.58497620 : Time 472.37s : 16214.01 words/s : L.r. 9.7014e-05
[2019-03-31 05:08:03] Ep. 8 : Up. 18000 : Sen. 482,663 : Cost 187.74728394 : Time 452.27s : 17206.07 words/s : L.r. 9.4281e-05
[2019-03-31 05:08:03] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 05:08:08] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter18000.npz
[2019-03-31 05:08:13] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 05:08:17] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 05:08:36] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 05:08:41] [valid] Ep. 8 : Up. 18000 : cross-entropy : 175.126 : new best
[2019-03-31 05:08:50] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 05:08:55] [valid] Ep. 8 : Up. 18000 : perplexity : 286.362 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 05:12:57] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 05:13:02] [valid] Ep. 8 : Up. 18000 : translation : 5.2 : new best
[2019-03-31 05:16:18] Seen 601324 samples
[2019-03-31 05:16:18] Starting epoch 9
[2019-03-31 05:16:18] [data] Shuffling data
[2019-03-31 05:16:18] [data] Done reading 620637 sentences
[2019-03-31 05:16:21] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 05:20:55] Ep. 9 : Up. 19000 : Sen. 145,907 : Cost 177.96992493 : Time 771.77s : 9940.55 words/s : L.r. 9.1766e-05
[2019-03-31 05:28:24] Ep. 9 : Up. 20000 : Sen. 408,180 : Cost 177.58312988 : Time 449.62s : 17049.34 words/s : L.r. 8.9443e-05
[2019-03-31 05:33:59] Seen 601324 samples
[2019-03-31 05:33:59] Starting epoch 10
[2019-03-31 05:33:59] [data] Shuffling data
[2019-03-31 05:33:59] [data] Done reading 620637 sentences
[2019-03-31 05:34:01] [data] Done shuffling 620637 sentences to temp files
[2019-03-31 05:36:17] Ep. 10 : Up. 21000 : Sen. 66,760 : Cost 178.41899109 : Time 472.90s : 16283.30 words/s : L.r. 8.7287e-05
[2019-03-31 05:36:17] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.orig.npz
[2019-03-31 05:36:23] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.iter21000.npz
[2019-03-31 05:36:28] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz
[2019-03-31 05:36:33] Saving Adam parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.optimizer.npz
[2019-03-31 05:36:54] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-cross-entropy.npz
[2019-03-31 05:36:59] [valid] Ep. 10 : Up. 21000 : cross-entropy : 168.623 : new best
[2019-03-31 05:37:09] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-perplexity.npz
[2019-03-31 05:37:14] [valid] Ep. 10 : Up. 21000 : perplexity : 232.101 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2019-03-31 05:41:14] Saving model weights and runtime parameters to model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz.best-translation.npz
[2019-03-31 05:41:20] [valid] Ep. 10 : Up. 21000 : translation : 5.7 : new best
[2019-03-31 05:49:26] Ep. 10 : Up. 22000 : Sen. 328,983 : Cost 175.67515564 : Time 789.35s : 9768.58 words/s : L.r. 8.5280e-05
train_doc_docnew_enc_depth_pretrain_freeze2.sh: řádek 29:  3935 Ukončen (SIGTERM)      $marian_home/marian --model model/model.src1tgt0.docnew.enc_depth.pretrained.frozen2.fix_emb5.npz --pretrained-model ../src0tgt0/model/model.src0tgt0_to_tgt0.newbase.160.npz.best-perplexity.npz --type transformer-context --train-sets corp/europarl.cs-en.docs.train.en.bpe.src_prev corp/europarl.cs-en.docs.train.en.bpe.src corp/europarl.cs-en.docs.train.cz.bpe --max-length 80 --dim-vocabs 30000 30000 --embedding-fix-src --embedding-fix-trg --vocabs corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml corp/vocab.encz.opensub.new.yml --mini-batch-fit -w 7800 --mini-batch 1000 --maxi-batch 1000 --early-stopping 10 --freeze --valid-freq 3000 --save-freq 3000 --disp-freq 1000 --valid-metrics cross-entropy perplexity translation --valid-sets corp/europarl.cs-en.docs.dev.en.bpe.src_prev corp/europarl.cs-en.docs.dev.en.bpe.src corp/europarl.cs-en.docs.dev.cz.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 16 --keep-best --beam-size 6 --normalize 0.6 --log model/train_trans.gate.log --valid-log model/valid_trans.gate.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0001 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 4 --devices 0 --no-nccl --sync-sgd --seed 1111 --exponential-smoothing --no-restore-corpus
