[2018-11-26 04:00:24] [config] after-batches: 0
[2018-11-26 04:00:24] [config] after-epochs: 0
[2018-11-26 04:00:24] [config] allow-unk: false
[2018-11-26 04:00:24] [config] beam-size: 6
[2018-11-26 04:00:24] [config] best-deep: false
[2018-11-26 04:00:24] [config] clip-gemm: 0
[2018-11-26 04:00:24] [config] clip-norm: 5
[2018-11-26 04:00:24] [config] cost-type: ce-mean
[2018-11-26 04:00:24] [config] cpu-threads: 0
[2018-11-26 04:00:24] [config] data-weighting-type: sentence
[2018-11-26 04:00:24] [config] dec-cell: gru
[2018-11-26 04:00:24] [config] dec-cell-base-depth: 2
[2018-11-26 04:00:24] [config] dec-cell-high-depth: 1
[2018-11-26 04:00:24] [config] dec-depth: 6
[2018-11-26 04:00:24] [config] devices:
[2018-11-26 04:00:24] [config]   - 2
[2018-11-26 04:00:24] [config] dim-emb: 512
[2018-11-26 04:00:24] [config] dim-rnn: 1024
[2018-11-26 04:00:24] [config] dim-vocabs:
[2018-11-26 04:00:24] [config]   - 0
[2018-11-26 04:00:24] [config]   - 0
[2018-11-26 04:00:24] [config] disp-freq: 500
[2018-11-26 04:00:24] [config] disp-label-counts: false
[2018-11-26 04:00:24] [config] dropout-rnn: 0
[2018-11-26 04:00:24] [config] dropout-src: 0
[2018-11-26 04:00:24] [config] dropout-trg: 0
[2018-11-26 04:00:24] [config] dump-config: false
[2018-11-26 04:00:24] [config] early-stopping: 15
[2018-11-26 04:00:24] [config] embedding-fix-src: false
[2018-11-26 04:00:24] [config] embedding-fix-trg: false
[2018-11-26 04:00:24] [config] embedding-normalization: false
[2018-11-26 04:00:24] [config] enc-cell: gru
[2018-11-26 04:00:24] [config] enc-cell-depth: 1
[2018-11-26 04:00:24] [config] enc-depth: 6
[2018-11-26 04:00:24] [config] enc-type: bidirectional
[2018-11-26 04:00:24] [config] exponential-smoothing: 0.0001
[2018-11-26 04:00:24] [config] grad-dropping-momentum: 0
[2018-11-26 04:00:24] [config] grad-dropping-rate: 0
[2018-11-26 04:00:24] [config] grad-dropping-warmup: 100
[2018-11-26 04:00:24] [config] guided-alignment-cost: ce
[2018-11-26 04:00:24] [config] guided-alignment-weight: 1
[2018-11-26 04:00:24] [config] ignore-model-config: false
[2018-11-26 04:00:24] [config] interpolate-env-vars: false
[2018-11-26 04:00:24] [config] keep-best: false
[2018-11-26 04:00:24] [config] label-smoothing: 0.1
[2018-11-26 04:00:24] [config] layer-normalization: false
[2018-11-26 04:00:24] [config] learn-rate: 0.0003
[2018-11-26 04:00:24] [config] log: model/train_trans.log
[2018-11-26 04:00:24] [config] log-level: info
[2018-11-26 04:00:24] [config] lr-decay: 0
[2018-11-26 04:00:24] [config] lr-decay-freq: 50000
[2018-11-26 04:00:24] [config] lr-decay-inv-sqrt: 16000
[2018-11-26 04:00:24] [config] lr-decay-repeat-warmup: false
[2018-11-26 04:00:24] [config] lr-decay-reset-optimizer: false
[2018-11-26 04:00:24] [config] lr-decay-start:
[2018-11-26 04:00:24] [config]   - 10
[2018-11-26 04:00:24] [config]   - 1
[2018-11-26 04:00:24] [config] lr-decay-strategy: epoch+stalled
[2018-11-26 04:00:24] [config] lr-report: true
[2018-11-26 04:00:24] [config] lr-warmup: 16000
[2018-11-26 04:00:24] [config] lr-warmup-at-reload: false
[2018-11-26 04:00:24] [config] lr-warmup-cycle: false
[2018-11-26 04:00:24] [config] lr-warmup-start-rate: 0
[2018-11-26 04:00:24] [config] max-length: 160
[2018-11-26 04:00:24] [config] max-length-crop: false
[2018-11-26 04:00:24] [config] max-length-factor: 3
[2018-11-26 04:00:24] [config] maxi-batch: 1000
[2018-11-26 04:00:24] [config] maxi-batch-sort: trg
[2018-11-26 04:00:24] [config] mini-batch: 40
[2018-11-26 04:00:24] [config] mini-batch-fit: false
[2018-11-26 04:00:24] [config] mini-batch-fit-step: 10
[2018-11-26 04:00:24] [config] mini-batch-words: 0
[2018-11-26 04:00:24] [config] model: model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz
[2018-11-26 04:00:24] [config] multi-node: false
[2018-11-26 04:00:24] [config] multi-node-overlap: true
[2018-11-26 04:00:24] [config] n-best: false
[2018-11-26 04:00:24] [config] no-nccl: false
[2018-11-26 04:00:24] [config] no-reload: false
[2018-11-26 04:00:24] [config] no-restore-corpus: false
[2018-11-26 04:00:24] [config] no-shuffle: false
[2018-11-26 04:00:24] [config] normalize: 0.6
[2018-11-26 04:00:24] [config] optimizer: adam
[2018-11-26 04:00:24] [config] optimizer-delay: 32
[2018-11-26 04:00:24] [config] optimizer-params:
[2018-11-26 04:00:24] [config]   - 0.9
[2018-11-26 04:00:24] [config]   - 0.98
[2018-11-26 04:00:24] [config]   - 1e-09
[2018-11-26 04:00:24] [config] overwrite: false
[2018-11-26 04:00:24] [config] quiet: false
[2018-11-26 04:00:24] [config] quiet-translation: true
[2018-11-26 04:00:24] [config] relative-paths: false
[2018-11-26 04:00:24] [config] right-left: false
[2018-11-26 04:00:24] [config] save-freq: 5000
[2018-11-26 04:00:24] [config] seed: 1111
[2018-11-26 04:00:24] [config] skip: false
[2018-11-26 04:00:24] [config] sqlite: ""
[2018-11-26 04:00:24] [config] sqlite-drop: false
[2018-11-26 04:00:24] [config] sync-sgd: true
[2018-11-26 04:00:24] [config] tempdir: /tmp
[2018-11-26 04:00:24] [config] tied-embeddings: false
[2018-11-26 04:00:24] [config] tied-embeddings-all: true
[2018-11-26 04:00:24] [config] tied-embeddings-src: false
[2018-11-26 04:00:24] [config] train-sets:
[2018-11-26 04:00:24] [config]   - corp/europarl.cs-en.docs.train.en.bpe
[2018-11-26 04:00:24] [config]   - corp/europarl.cs-en.docs.train.cz.bpe
[2018-11-26 04:00:24] [config] transformer-aan-activation: swish
[2018-11-26 04:00:24] [config] transformer-aan-depth: 2
[2018-11-26 04:00:24] [config] transformer-aan-nogate: false
[2018-11-26 04:00:24] [config] transformer-decoder-autoreg: self-attention
[2018-11-26 04:00:24] [config] transformer-dim-aan: 2048
[2018-11-26 04:00:24] [config] transformer-dim-ffn: 2048
[2018-11-26 04:00:24] [config] transformer-dropout: 0.1
[2018-11-26 04:00:24] [config] transformer-dropout-attention: 0
[2018-11-26 04:00:24] [config] transformer-dropout-ffn: 0
[2018-11-26 04:00:24] [config] transformer-ffn-activation: swish
[2018-11-26 04:00:24] [config] transformer-ffn-depth: 2
[2018-11-26 04:00:24] [config] transformer-guided-alignment-layer: last
[2018-11-26 04:00:24] [config] transformer-heads: 8
[2018-11-26 04:00:24] [config] transformer-no-projection: false
[2018-11-26 04:00:24] [config] transformer-postprocess: dan
[2018-11-26 04:00:24] [config] transformer-postprocess-emb: d
[2018-11-26 04:00:24] [config] transformer-preprocess: ""
[2018-11-26 04:00:24] [config] transformer-tied-layers:
[2018-11-26 04:00:24] [config]   []
[2018-11-26 04:00:24] [config] type: transformer
[2018-11-26 04:00:24] [config] valid-freq: 5000
[2018-11-26 04:00:24] [config] valid-log: model/valid_trans.log
[2018-11-26 04:00:24] [config] valid-max-length: 1000
[2018-11-26 04:00:24] [config] valid-metrics:
[2018-11-26 04:00:24] [config]   - cross-entropy
[2018-11-26 04:00:24] [config]   - perplexity
[2018-11-26 04:00:24] [config]   - translation
[2018-11-26 04:00:24] [config] valid-mini-batch: 16
[2018-11-26 04:00:24] [config] valid-script-path: ./val.sh
[2018-11-26 04:00:24] [config] valid-sets:
[2018-11-26 04:00:24] [config]   - corp/europarl.cs-en.docs.dev.en.bpe
[2018-11-26 04:00:24] [config]   - corp/europarl.cs-en.docs.dev.cz.bpe
[2018-11-26 04:00:24] [config] valid-translation-output: data/valid.bpe.en.output
[2018-11-26 04:00:24] [config] vocabs:
[2018-11-26 04:00:24] [config]   - corp/vocab.encs.europarl.yml
[2018-11-26 04:00:24] [config]   - corp/vocab.encs.europarl.yml
[2018-11-26 04:00:24] [config] word-penalty: 0
[2018-11-26 04:00:24] [config] workspace: 2048
[2018-11-26 04:00:24] [config] Model is being created with Marian v1.6.0+59e69a8
[2018-11-26 04:00:24] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.europarl.yml
[2018-11-26 04:00:24] [data] Setting vocabulary size for input 0 to 32000
[2018-11-26 04:00:24] [data] Loading vocabulary from JSON/Yaml file corp/vocab.encs.europarl.yml
[2018-11-26 04:00:24] [data] Setting vocabulary size for input 1 to 32000
[2018-11-26 04:00:24] [memory] Extending reserved space to 2048 MB (device gpu2)
[2018-11-26 04:00:24] Training started
[2018-11-26 04:00:24] [data] Shuffling files
[2018-11-26 04:00:26] [data] Done
[2018-11-26 04:00:28] [memory] Reserving 230 MB, device gpu2
tcmalloc: large alloc 2147483648 bytes == 0x25e86000 @ 
[2018-11-26 04:00:33] [memory] Reserving 230 MB, device gpu2
tcmalloc: large alloc 2281701376 bytes == 0x25e86000 @ 
tcmalloc: large alloc 2415919104 bytes == 0x25e86000 @ 
[2018-11-26 04:00:35] [memory] Reserving 461 MB, device gpu2
[2018-11-26 04:00:35] [memory] Reserving 230 MB, device gpu2
tcmalloc: large alloc 2550136832 bytes == 0x25e86000 @ 
tcmalloc: large alloc 2684354560 bytes == 0x25e86000 @ 
tcmalloc: large alloc 2818572288 bytes == 0x25e86000 @ 
tcmalloc: large alloc 2952790016 bytes == 0x25e86000 @ 
tcmalloc: large alloc 3087007744 bytes == 0x25e86000 @ 
tcmalloc: large alloc 3221225472 bytes == 0x25e86000 @ 
tcmalloc: large alloc 3355443200 bytes == 0x25e86000 @ 
tcmalloc: large alloc 3489660928 bytes == 0x25e86000 @ 
tcmalloc: large alloc 4026531840 bytes == 0x25e86000 @ 
tcmalloc: large alloc 4429185024 bytes == 0x25e86000 @ 
tcmalloc: large alloc 4966055936 bytes == 0x7fdc68000000 @ 
tcmalloc: large alloc 5637144576 bytes == 0x7fcfe8000000 @ 
tcmalloc: large alloc 6308233216 bytes == 0x7fc1d8000000 @ 
tcmalloc: large alloc 7113539584 bytes == 0x7faee8000000 @ 
[2018-11-26 04:08:46] Ep. 1 : Up. 500 : Sen. 20000 : Cost 296.03 : Time 501.92s : 1196.19 words/s : L.r. 9.3750e-06
tcmalloc: large alloc 8053063680 bytes == 0x7f95f8000000 @ 
tcmalloc: large alloc 8589934592 bytes == 0x7f8658000000 @ 
tcmalloc: large alloc 8724152320 bytes == 0x7f8248000000 @ 
tcmalloc: large alloc 8858370048 bytes == 0x7f7e28000000 @ 
tcmalloc: large alloc 8992587776 bytes == 0x7f79f8000000 @ 
tcmalloc: large alloc 9126805504 bytes == 0x7f75b8000000 @ 
tcmalloc: large alloc 9261023232 bytes == 0x7f7168000000 @ 
tcmalloc: large alloc 9395240960 bytes == 0x7f6d08000000 @ 
[2018-11-26 04:16:58] Ep. 1 : Up. 1000 : Sen. 40000 : Cost 260.96 : Time 491.76s : 1255.66 words/s : L.r. 1.8750e-05
[2018-11-26 04:18:17] Ep. 1 : Up. 1500 : Sen. 60000 : Cost 239.05 : Time 79.84s : 7594.16 words/s : L.r. 2.8125e-05
tcmalloc: large alloc 9529458688 bytes == 0x7f6898000000 @ 
tcmalloc: large alloc 9663676416 bytes == 0x7f6418000000 @ 
tcmalloc: large alloc 9797894144 bytes == 0x7f5f88000000 @ 
tcmalloc: large alloc 9932111872 bytes == 0x7f5ae8000000 @ 
[2018-11-26 04:21:59] Ep. 1 : Up. 2000 : Sen. 80000 : Cost 235.69 : Time 221.64s : 2768.20 words/s : L.r. 3.7500e-05
[2018-11-26 04:23:19] Ep. 1 : Up. 2500 : Sen. 100000 : Cost 223.71 : Time 80.17s : 7499.46 words/s : L.r. 4.6875e-05
[2018-11-26 04:24:40] Ep. 1 : Up. 3000 : Sen. 120000 : Cost 219.80 : Time 80.28s : 7619.99 words/s : L.r. 5.6250e-05
[2018-11-26 04:26:03] Ep. 1 : Up. 3500 : Sen. 140000 : Cost 221.74 : Time 83.18s : 7656.63 words/s : L.r. 6.5625e-05
[2018-11-26 04:27:21] Ep. 1 : Up. 4000 : Sen. 160000 : Cost 198.05 : Time 78.49s : 7464.36 words/s : L.r. 7.5000e-05
[2018-11-26 04:28:43] Ep. 1 : Up. 4500 : Sen. 180000 : Cost 199.26 : Time 81.35s : 7441.16 words/s : L.r. 8.4375e-05
[2018-11-26 04:30:03] Ep. 1 : Up. 5000 : Sen. 200000 : Cost 194.56 : Time 80.09s : 7559.06 words/s : L.r. 9.3750e-05
[2018-11-26 04:30:14] [valid] Ep. 1 : Up. 5000 : cross-entropy : 180.718 : new best
[2018-11-26 04:30:26] [valid] Ep. 1 : Up. 5000 : perplexity : 343.057 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2018-11-26 05:23:12] [valid] Ep. 1 : Up. 5000 : translation : 0.44 : new best
[2018-11-26 05:23:12] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz.orig.npz
[2018-11-26 05:23:14] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.iter5000.npz
[2018-11-26 05:23:15] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz
[2018-11-26 05:23:15] Saving Adam parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz.optimizer.npz
[2018-11-26 05:24:40] Ep. 1 : Up. 5500 : Sen. 220000 : Cost 192.12 : Time 3277.84s : 187.17 words/s : L.r. 1.0313e-04
[2018-11-26 05:26:02] Ep. 1 : Up. 6000 : Sen. 240000 : Cost 185.08 : Time 81.79s : 7398.46 words/s : L.r. 1.1250e-04
[2018-11-26 05:27:24] Ep. 1 : Up. 6500 : Sen. 260000 : Cost 180.57 : Time 82.12s : 7353.31 words/s : L.r. 1.2188e-04
[2018-11-26 05:28:46] Ep. 1 : Up. 7000 : Sen. 280000 : Cost 179.96 : Time 81.28s : 7562.76 words/s : L.r. 1.3125e-04
[2018-11-26 05:30:09] Ep. 1 : Up. 7500 : Sen. 300000 : Cost 178.26 : Time 83.13s : 7482.10 words/s : L.r. 1.4063e-04
[2018-11-26 05:31:28] Ep. 1 : Up. 8000 : Sen. 320000 : Cost 167.17 : Time 79.04s : 7527.06 words/s : L.r. 1.5000e-04
[2018-11-26 05:32:49] Ep. 1 : Up. 8500 : Sen. 340000 : Cost 168.46 : Time 80.97s : 7489.38 words/s : L.r. 1.5938e-04
[2018-11-26 05:34:10] Ep. 1 : Up. 9000 : Sen. 360000 : Cost 167.00 : Time 80.73s : 7586.19 words/s : L.r. 1.6875e-04
[2018-11-26 05:35:34] Ep. 1 : Up. 9500 : Sen. 380000 : Cost 172.12 : Time 84.07s : 7621.85 words/s : L.r. 1.7813e-04
[2018-11-26 05:36:51] Ep. 1 : Up. 10000 : Sen. 400000 : Cost 152.96 : Time 77.86s : 7465.72 words/s : L.r. 1.8750e-04
[2018-11-26 05:37:03] [valid] Ep. 1 : Up. 10000 : cross-entropy : 139.488 : new best
[2018-11-26 05:37:15] [valid] Ep. 1 : Up. 10000 : perplexity : 90.5596 : new best
Detokenizer Version $Revision: 4134 $
Language: en
[2018-11-26 05:52:53] [valid] Ep. 1 : Up. 10000 : translation : 2.26 : new best
[2018-11-26 05:52:53] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz.orig.npz
[2018-11-26 05:52:55] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.iter10000.npz
[2018-11-26 05:52:55] Saving model weights and runtime parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz
[2018-11-26 05:52:57] Saving Adam parameters to model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz.optimizer.npz
tcmalloc: large alloc 10066329600 bytes == 0x7f5638000000 @ 
tcmalloc: large alloc 10200547328 bytes == 0x7f5178000000 @ 
[2018-11-26 05:55:16] Error: out of memory - /home/big_maggie/usr/marian_cosmas/marian_1.6.0/marian-dev/src/tensors/gpu/device.cu:30
train_trans.sh: line 26: 97271 Aborted                 (core dumped) $marian_home/marian --model model/model.src1tgt0.trans3.fixed.batch.40.opt32.npz --type transformer --train-sets corp/europarl.cs-en.docs.train.en.bpe corp/europarl.cs-en.docs.train.cz.bpe --max-length 160 --vocabs corp/vocab.encs.europarl.yml corp/vocab.encs.europarl.yml --mini-batch 40 --maxi-batch 1000 --early-stopping 15 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics cross-entropy perplexity translation --valid-sets corp/europarl.cs-en.docs.dev.en.bpe corp/europarl.cs-en.docs.dev.cz.bpe --valid-script-path ./val.sh --valid-translation-output data/valid.bpe.en.output --quiet-translation --valid-mini-batch 16 --beam-size 6 --normalize 0.6 --log model/train_trans.log --valid-log model/valid_trans.log --enc-depth 6 --dec-depth 6 --transformer-heads 8 --transformer-postprocess-emb d --transformer-postprocess dan --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings-all --optimizer-delay 32 --devices 2 --sync-sgd --seed 1111 --exponential-smoothing
